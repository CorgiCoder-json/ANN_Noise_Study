{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmer: Jacob Maurer\n",
    "Date: 9/23/2024\n",
    "Description: This file is meant to explore the statistical properties of Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_regress_pre_1 = pd.read_csv(\"results/large_regress_weight_pre_1.csv\").to_numpy()\n",
    "large_regress_pre_2 = pd.read_csv(\"results/large_regress_weight_pre_3.csv\").to_numpy()\n",
    "large_regress_post_1 = pd.read_csv(\"results/large_regress_weight_post_1.csv\").to_numpy()\n",
    "large_regress_post_2 = pd.read_csv(\"results/large_regress_weight_post_3.csv\").to_numpy()\n",
    "medium_regress_pre_1 = pd.read_csv(\"results/medium_regress_weight_pre_1.csv\").to_numpy()\n",
    "medium_regress_pre_2 = pd.read_csv(\"results/medium_regress_weight_pre_3.csv\").to_numpy()\n",
    "medium_regress_post_1 = pd.read_csv(\"results/medium_regress_weight_post_1.csv\").to_numpy()\n",
    "medium_regress_post_2 = pd.read_csv(\"results/medium_regress_weight_post_3.csv\").to_numpy()\n",
    "small_regress_pre_1 = pd.read_csv(\"results/small_regress_weight_pre_1.csv\").to_numpy()\n",
    "small_regress_pre_2 = pd.read_csv(\"results/small_regress_weight_pre_3.csv\").to_numpy()\n",
    "small_regress_post_1 = pd.read_csv(\"results/small_regress_weight_post_1.csv\").to_numpy()\n",
    "small_regress_post_2 = pd.read_csv(\"results/small_regress_weight_post_3.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_class_pre_1 = pd.read_csv(\"results/large_class_weight_pre_1.csv\").to_numpy()\n",
    "large_class_pre_2 = pd.read_csv(\"results/large_class_weight_pre_3.csv\").to_numpy()\n",
    "large_class_post_1 = pd.read_csv(\"results/large_class_weight_post_1.csv\").to_numpy()\n",
    "large_class_post_2 = pd.read_csv(\"results/large_class_weight_post_3.csv\").to_numpy()\n",
    "medium_class_pre_1 = pd.read_csv(\"results/medium_class_weight_pre_1.csv\").to_numpy()\n",
    "medium_class_pre_2 = pd.read_csv(\"results/medium_class_weight_pre_3.csv\").to_numpy()\n",
    "medium_class_post_1 = pd.read_csv(\"results/medium_class_weight_post_1.csv\").to_numpy()\n",
    "medium_class_post_2 = pd.read_csv(\"results/medium_class_weight_post_3.csv\").to_numpy()\n",
    "small_class_pre_1 = pd.read_csv(\"results/small_class_weight_pre_1.csv\").to_numpy()\n",
    "small_class_pre_2 = pd.read_csv(\"results/small_class_weight_pre_3.csv\").to_numpy()\n",
    "small_class_post_1 = pd.read_csv(\"results/small_class_weight_post_1.csv\").to_numpy()\n",
    "small_class_post_2 = pd.read_csv(\"results/small_class_weight_post_3.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_class_pre_1 = pd.read_csv(\"results/nonMNIST_multi_weight_pre_1.csv\").to_numpy()\n",
    "large_class_pre_2 = pd.read_csv(\"results/nonMNIST_multi_weight_pre_3.csv\").to_numpy()\n",
    "large_class_pre_3 = pd.read_csv(\"results/nonMNIST_multi_weight_pre_5.csv\").to_numpy()\n",
    "large_class_post_1 = pd.read_csv(\"results/nonMNIST_multi_weight_post_1.csv\").to_numpy()\n",
    "large_class_post_2 = pd.read_csv(\"results/nonMNIST_multi_weight_post_3.csv\").to_numpy()\n",
    "large_class_post_3 = pd.read_csv(\"results/nonMNIST_multi_weight_post_5.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Networks: \n",
      "   Means pre: {'small': [-0.00013199448637596107, 0.008123179754687501], 'medium': [4.3858337258824233e-05, -0.00034533024968749987], 'large': [-3.319144255076155e-05, 0.004953763677367187]}\n",
      "   Stds pre: {'small': [0.057868721946335716, 0.051975205664446446], 'medium': [0.03336347286047307, 0.05156682217406374], 'large': [0.02575080047311073, 0.04929542238232484]}\n",
      "   Means post: {'small': [0.0047729015882125, 0.48274142890624994], 'medium': [0.009424330115804166, 0.15603295703125042], 'large': [-0.024957395315540355, -0.6394644765625003]}\n",
      "   Stda post: {'small': [0.6052243903043092, 8.345506468848821], 'medium': [1.6523561810303757, 12.58434542032743], 'large': [3.7947600559035513, 18.757646165261068]}\n",
      "Binary Class Networks: \n",
      "   Means pre: {'small': [-0.0008197078568525783, 0.0005529688823437495], 'medium': [-9.18192690006094e-05, 0.0019285825765624999], 'large': [8.303491810848717e-07, 7.102814941797258e-05]}\n",
      "   Stds pre: {'small': [0.057406923998982236, 0.049211709297439094], 'medium': [0.03331499464625309, 0.05276036463951852], 'large': [0.025849843081978286, 0.02554322142251605]}\n",
      "   Means post: {'small': [-0.0008239944278315233, 0.001414240677656253], 'medium': [-0.00011049899721700884, -0.0060843294000000004], 'large': [-0.0015452711007986812, -0.005922225026243146]}\n",
      "   Stda post: {'small': [0.05807018933736423, 0.13838287370525787], 'medium': [0.03427088041125618, 0.19029081607541165], 'large': [0.03388201791001314, 0.03329243113175986]}\n",
      "Multi Class Networks: \n",
      "   Means pre: [8.303491810848717e-07, 7.102814941797258e-05, 0.00018317452117128915]\n",
      "   Stds pre: [0.025849843081978286, 0.02554322142251605, 0.02547585858776462]\n",
      "   Means post: [-0.0015452711007986812, -0.005922225026243146, 0.00018346835564765607]\n",
      "   Stda post: [0.03388201791001314, 0.03329243113175986, 0.12315839608442512]\n"
     ]
    }
   ],
   "source": [
    "def get_means(lists):\n",
    "    means = {\"small\": [], \"medium\": [], \"large\": []}\n",
    "    means[\"small\"].extend([np.mean(lists[0][item]) for item in range(len(lists[0]))])\n",
    "    means[\"medium\"].extend([np.mean(lists[1][item]) for item in range(len(lists[1]))])\n",
    "    means[\"large\"].extend([np.mean(lists[2][item]) for item in range(len(lists[2]))])\n",
    "    return means\n",
    "\n",
    "def get_stds(lists):\n",
    "    means = {\"small\": [], \"medium\": [], \"large\": []}\n",
    "    means[\"small\"].extend([np.std(lists[0][item]) for item in range(len(lists[0]))])\n",
    "    means[\"medium\"].extend([np.std(lists[1][item]) for item in range(len(lists[1]))])\n",
    "    means[\"large\"].extend([np.std(lists[2][item]) for item in range(len(lists[2]))])\n",
    "    return means\n",
    "\n",
    "final_means_pre = get_means([[small_regress_pre_1, small_regress_pre_2], [medium_regress_pre_1, medium_regress_pre_2], [large_regress_pre_1, large_regress_pre_2]])\n",
    "final_means_post = get_means([[small_regress_post_1, small_regress_post_2], [medium_regress_post_1, medium_regress_post_2], [large_regress_post_1, large_regress_post_2]])\n",
    "final_stds_pre = get_stds([[small_regress_pre_1, small_regress_pre_2], [medium_regress_pre_1, medium_regress_pre_2], [large_regress_pre_1, large_regress_pre_2]])\n",
    "final_stds_post = get_stds([[small_regress_post_1, small_regress_post_2], [medium_regress_post_1, medium_regress_post_2], [large_regress_post_1, large_regress_post_2]])\n",
    "final_class_means_pre = get_means([[small_class_pre_1, small_class_pre_2], [medium_class_pre_1, medium_class_pre_2], [large_class_pre_1, large_class_pre_2]])\n",
    "final_class_means_post = get_means([[small_class_post_1, small_class_post_2], [medium_class_post_1, medium_class_post_2], [large_class_post_1, large_class_post_2]])\n",
    "final_class_stds_pre = get_stds([[small_class_pre_1, small_class_pre_2], [medium_class_pre_1, medium_class_pre_2], [large_class_pre_1, large_class_pre_2]])\n",
    "final_class_stds_post = get_stds([[small_class_post_1, small_class_post_2], [medium_class_post_1, medium_class_post_2], [large_class_post_1, large_class_post_2]])\n",
    "final_multi_means_pre = [np.mean(layer) for layer in [large_class_pre_1, large_class_pre_2, large_class_pre_3]]\n",
    "final_multi_means_post = [np.mean(layer) for layer in [large_class_post_1, large_class_post_2, large_class_post_3]]\n",
    "final_multi_stds_pre = [np.std(layer) for layer in [large_class_pre_1, large_class_pre_2, large_class_pre_3]]\n",
    "final_multi_stds_post = [np.std(layer) for layer in [large_class_post_1, large_class_post_2, large_class_post_3]]\n",
    "print(\"Regression Networks: \")\n",
    "print(\"   Means pre:\", final_means_pre)\n",
    "print(\"   Stds pre:\",final_stds_pre)\n",
    "print(\"   Means post:\",final_means_post)\n",
    "print(\"   Stda post:\",final_stds_post)\n",
    "print(\"Binary Class Networks: \")\n",
    "print(\"   Means pre:\", final_class_means_pre)\n",
    "print(\"   Stds pre:\",final_class_stds_pre)\n",
    "print(\"   Means post:\",final_class_means_post)\n",
    "print(\"   Stda post:\",final_class_stds_post)\n",
    "print(\"Multi Class Networks: \")\n",
    "print(\"   Means pre:\", final_multi_means_pre)\n",
    "print(\"   Stds pre:\",final_multi_stds_pre)\n",
    "print(\"   Means post:\",final_multi_means_post)\n",
    "print(\"   Stda post:\",final_multi_stds_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Hypothesis: Locality and Scale changes can determine if a network can continue to be trained\\\n",
    "Definitions:\\\n",
    "Scale - The means/centers of the network. This shows how the network better corrects weights to get close to the data\\\n",
    "Locality - The standard deviation/spread of the network. This just shows a changing of emphasis from the network to the data\\\n",
    "\\\n",
    "The idea:\\\n",
    "No change/very little change: perfect init; rarely happens, but is more common in classification network early layers\\\n",
    "Change in scale: init weights were too small; network adjusted; weights starting\\\n",
    "Change in locality: what the network should do, adjust emphasis in data;\\\n",
    "Change in scale and locality: the network had small weights and is finding emphasis in data\\\n",
    "\\\n",
    "New idea: opposing network\\\n",
    "The idea here is to multiply each weight and bias by -1, then run the feedforward and find the error if the errors are of opposite signs, it is sure that the networks converge. IF they are the same sign, start with a different set of init weights. The idea is hopefully this will reduce training time by giving more optimal weight starting params\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
