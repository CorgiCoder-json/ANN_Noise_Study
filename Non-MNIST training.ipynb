{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmer: Jacob Maurer\n",
    "date: 9/21/2024\n",
    "Description: This file is meant to make models that are trained on data other than the MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_regression, make_multilabel_classification\n",
    "import copy\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these experiments, we are going to assume about half of the features are informative, with no redundant or repeated features. The samples are going to be (5*number of features) * 10. This is going to be constant across regression and classification. The final sets will be balanced, seperate experiments will be run with unbalanced sets. The batch size will be 10.The split between train and test sets will be 80% training and 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_classification_problem = make_classification(n_samples = 5000, n_features=100, n_informative=50)\n",
    "medium_classification_problem = make_classification(n_samples = 15000, n_features=300, n_informative=150)\n",
    "large_classification_problem = make_classification(n_samples = 25000, n_features=500, n_informative = 250)\n",
    "small_regression_problem = make_regression(n_samples = 5000, n_features=100, n_informative=10)\n",
    "medium_regression_problem = make_regression(n_samples = 15000, n_features=300, n_informative=25)\n",
    "large_regression_problem = make_regression(n_samples = 25000, n_features=500, n_informative = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_c = pd.DataFrame(small_classification_problem[0])\n",
    "small_c['100'] = small_classification_problem[1]\n",
    "medium_c = pd.DataFrame(medium_classification_problem[0])\n",
    "medium_c['300'] = medium_classification_problem[1]\n",
    "large_c = pd.DataFrame(large_classification_problem[0])\n",
    "large_c['500'] = large_classification_problem[1]\n",
    "small_r = pd.DataFrame(small_regression_problem[0])\n",
    "small_r['100'] = small_regression_problem[1]\n",
    "medium_r = pd.DataFrame(medium_regression_problem[0])\n",
    "medium_r['300'] = medium_regression_problem[1]\n",
    "large_r = pd.DataFrame(large_regression_problem[0])\n",
    "large_r['500'] = large_regression_problem[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x = copy.deepcopy(x_data)\n",
    "        self.y = copy.deepcopy(y_data)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], np.float32(self.y[index])\n",
    "\n",
    "small_classification_train_data = GeneratedDataset(small_classification_problem[0][int(len(small_classification_problem[0])*.2):], small_classification_problem[1][int(len(small_classification_problem[1])*.2):])\n",
    "small_classification_test_data = GeneratedDataset(small_classification_problem[0][:int(len(small_classification_problem[0])*.2)], small_classification_problem[1][:int(len(small_classification_problem[1])*.2)])\n",
    "medium_classification_train_data = GeneratedDataset(medium_classification_problem[0][int(len(medium_classification_problem[0])*.2):], medium_classification_problem[1][int(len(medium_classification_problem[1])*.2):])\n",
    "medium_classification_test_data = GeneratedDataset(medium_classification_problem[0][:int(len(medium_classification_problem[0])*.2)], medium_classification_problem[1][:int(len(medium_classification_problem[1])*.2)])\n",
    "large_classification_train_data = GeneratedDataset(large_classification_problem[0][int(len(large_classification_problem[0])*.2):], large_classification_problem[1][int(len(large_classification_problem[1])*.2):])\n",
    "large_classification_test_data = GeneratedDataset(large_classification_problem[0][:int(len(large_classification_problem[0])*.2)], large_classification_problem[1][:int(len(large_classification_problem[1])*.2)])\n",
    "small_train_loader_class = DataLoader(small_classification_train_data, batch_size=100, shuffle=True)\n",
    "small_test_loader_class = DataLoader(small_classification_test_data, batch_size=100, shuffle=True)\n",
    "medium_train_loader_class = DataLoader(medium_classification_train_data, batch_size=100, shuffle=True)\n",
    "medium_test_loader_class = DataLoader(medium_classification_test_data, batch_size=100, shuffle=True)\n",
    "large_train_loader_class = DataLoader(large_classification_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_class = DataLoader(large_classification_test_data, batch_size=100, shuffle=True)\n",
    "small_regression_train_data = GeneratedDataset(small_regression_problem[0][int(len(small_regression_problem[0])*.2):], small_regression_problem[1][int(len(small_regression_problem[1])*.2):])\n",
    "small_regression_test_data = GeneratedDataset(small_regression_problem[0][:int(len(small_regression_problem[0])*.2)], small_regression_problem[1][:int(len(small_regression_problem[1])*.2)])\n",
    "medium_regression_train_data = GeneratedDataset(medium_regression_problem[0][int(len(medium_regression_problem[0])*.2):], medium_regression_problem[1][int(len(medium_regression_problem[1])*.2):])\n",
    "medium_regression_test_data = GeneratedDataset(medium_regression_problem[0][:int(len(medium_regression_problem[0])*.2)], medium_regression_problem[1][:int(len(medium_regression_problem[1])*.2)])\n",
    "large_regression_train_data = GeneratedDataset(large_regression_problem[0][int(len(large_regression_problem[0])*.2):], large_regression_problem[1][int(len(large_regression_problem[1])*.2):])\n",
    "large_regression_test_data = GeneratedDataset(large_regression_problem[0][:int(len(large_regression_problem[0])*.2)], large_regression_problem[1][:int(len(large_regression_problem[1])*.2)])\n",
    "small_train_loader_regress = DataLoader(small_regression_train_data, batch_size=100, shuffle=True)\n",
    "small_test_loader_regress = DataLoader(small_regression_test_data, batch_size=100, shuffle=True)\n",
    "medium_train_loader_regress = DataLoader(medium_regression_train_data, batch_size=100, shuffle=True)\n",
    "medium_test_loader_regress = DataLoader(medium_regression_test_data, batch_size=100, shuffle=True)\n",
    "large_train_loader_regress = DataLoader(large_regression_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_regress = DataLoader(large_regression_test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class MediumClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class LargeClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class SmallRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class MediumRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class LargeRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_class = SmallClassifyNetwork().to(device)\n",
    "medium_model_class = MediumClassifyNetwork().to(device)\n",
    "large_model_class = LargeClassifyNetwork().to(device)\n",
    "small_model_regress = SmallRegressNetwork().to(device)\n",
    "medium_model_regress = MediumRegressNetwork().to(device)\n",
    "large_model_regress = LargeRegressNetwork().to(device)\n",
    "\n",
    "loss_fn_class = nn.BCEWithLogitsLoss()\n",
    "optimizer_class = torch.optim.SGD(small_model_class.parameters(), lr=1e-2)\n",
    "loss_fn_regress = nn.MSELoss()\n",
    "optimizer_regress = torch.optim.SGD(small_model_regress.parameters(), lr=1e-6)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.unsqueeze(1)).item()\n",
    "            #pred = (pred > 0.5).type(torch.float)\n",
    "            #correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    #correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallRegressNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state = copy.deepcopy(small_model_regress.cpu().state_dict())\n",
    "old_state = copy.deepcopy(small_model_regress.cpu().state_dict())\n",
    "for key in new_state:\n",
    "    new_state[key] = old_state[key] * 10\n",
    "small_model_regress.load_state_dict(new_state)\n",
    "small_model_regress.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 42784.727734 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 41096.680859 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 39528.785156 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 37929.750977 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 36543.695312 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 35178.369141 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 33853.389453 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 32625.608594 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31322.098633 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 29959.408789 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 28876.011133 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 27647.608594 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 26439.180469 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 25220.247070 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 24110.718164 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 23019.854199 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 21979.196094 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 20861.908008 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 19836.740430 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 18885.590332 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 17944.762402 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 17021.672070 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 16183.844336 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 15422.420313 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14658.632617 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 13938.819434 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 13319.126953 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 12621.089160 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 12059.333301 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11573.927100 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(small_train_loader_regress, small_model_regress, loss_fn_regress, optimizer_regress)\n",
    "    test(small_test_loader_regress, small_model_regress, loss_fn_regress)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallRegressNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_pre = copy.deepcopy(small_model_regress.cpu().state_dict())\n",
    "small_model_regress.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallRegressNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_post = copy.deepcopy(small_model_regress.cpu().state_dict())\n",
    "small_model_regress.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[ 0.4601, -0.4505, -0.8708,  ...,  0.4702, -0.6070, -0.8724],\n",
       "                      [-0.5756,  0.9745, -0.2203,  ...,  0.2766,  0.5393, -0.8289],\n",
       "                      [-0.2717,  0.8914,  0.3801,  ..., -0.3161, -0.6734,  0.8168],\n",
       "                      ...,\n",
       "                      [ 0.0929,  0.7803,  0.9947,  ..., -0.4705,  0.5436, -0.7069],\n",
       "                      [-0.8998, -0.2299, -0.8009,  ..., -0.2721,  0.5920, -0.0599],\n",
       "                      [ 0.3799, -0.6249, -0.9368,  ...,  0.7654,  0.7934, -0.3919]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 0.5310,  0.0521, -0.1329,  0.5067,  0.9162,  0.6029, -0.0250,  0.0250,\n",
       "                       0.9825,  0.5359,  0.9095, -0.4889,  0.0625,  0.6467,  0.1050, -0.6041,\n",
       "                       0.8346, -0.4051,  0.0995, -0.5629, -0.3116, -0.2027, -0.0151, -0.1463,\n",
       "                       0.4428, -0.1002, -0.7529,  0.5969, -0.3857, -0.3062, -0.6474,  0.7534,\n",
       "                      -0.4494, -0.5918, -0.3104,  0.0845, -0.1734,  0.8718, -0.0159, -0.8766,\n",
       "                       0.1947, -0.5717, -0.1168,  0.0189,  0.8668,  0.7626,  0.2001,  0.0461,\n",
       "                       0.6282, -0.8322, -0.6803, -0.8648, -0.9920,  0.8788, -0.3917,  0.0540,\n",
       "                      -0.4317,  0.6123,  0.7788, -0.6676, -0.3065, -0.8043, -0.6532, -0.8610,\n",
       "                       0.0394, -0.3073,  0.7360, -0.1312, -0.3331, -0.0995,  0.8780, -0.5300,\n",
       "                      -0.2544,  0.9367, -0.9490, -0.2782,  0.4680,  0.5528, -0.4049, -0.5945,\n",
       "                       0.7353, -0.5110, -0.0274, -0.5534, -0.0970,  0.1617, -0.0191,  0.2357,\n",
       "                       0.7253, -0.7129,  0.6465, -0.1623,  0.3595, -0.3874, -0.5573, -0.7631,\n",
       "                       0.1645, -0.5998, -0.4358,  0.0068,  0.1674,  0.7478,  0.4879, -0.5958,\n",
       "                      -0.2076,  0.0408, -0.7317, -0.5395, -0.5313, -0.2820, -0.9758, -0.4367,\n",
       "                      -0.0167, -0.2915, -0.3599, -0.4619,  0.8568,  0.6413, -0.8288, -0.9003,\n",
       "                       0.5581, -0.6714,  0.7041, -0.2133, -0.1003,  0.3891, -0.5590,  0.9901])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.2139,  0.8804, -0.0999,  ..., -0.2927,  0.0119,  0.2484],\n",
       "                      [ 0.7089,  0.0155,  0.5653,  ..., -0.3236,  0.2144, -0.0706],\n",
       "                      [ 0.2834,  0.2389, -0.1897,  ...,  0.4620,  0.2861, -0.4333],\n",
       "                      ...,\n",
       "                      [-0.2751,  0.2851,  0.6028,  ..., -0.5389,  0.2428, -0.1321],\n",
       "                      [-0.7449, -0.3057, -0.4793,  ...,  0.3307,  0.7753,  0.3216],\n",
       "                      [-0.6936,  0.4286, -0.1288,  ...,  0.0220,  0.6100,  0.7416]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 0.1939,  0.3027, -0.7698,  0.1663, -0.7532, -0.5115,  0.5729, -0.1100,\n",
       "                      -0.7476, -0.4340, -0.4723, -0.8550, -0.2425,  0.0254, -0.5739, -0.0688,\n",
       "                      -0.6996,  0.2725, -0.5635, -0.0207,  0.7412, -0.7055, -0.7419,  0.1106,\n",
       "                      -0.5534,  0.8165,  0.4243, -0.5150, -0.8544,  0.6073,  0.1725, -0.0242,\n",
       "                      -0.1810, -0.0697, -0.0827,  0.8550, -0.3409, -0.0644,  0.3836, -0.0562,\n",
       "                      -0.1615, -0.6046, -0.0832,  0.5424, -0.5221,  0.6444, -0.4612, -0.3938,\n",
       "                       0.2936,  0.8177,  0.8736,  0.2485, -0.5956,  0.2558,  0.0725,  0.3403,\n",
       "                       0.2050,  0.2186,  0.4975,  0.3403,  0.5434, -0.1342,  0.6674, -0.3589,\n",
       "                      -0.1286,  0.8679, -0.2673, -0.0436,  0.8170,  0.1198, -0.6929,  0.1113,\n",
       "                      -0.8622,  0.5295,  0.5796,  0.6960, -0.5613,  0.1885,  0.7919,  0.6124,\n",
       "                      -0.3718, -0.4601,  0.8160,  0.2098,  0.1444, -0.1572,  0.4973, -0.7624,\n",
       "                       0.2797, -0.7965, -0.3447,  0.4390,  0.0185,  0.5365,  0.0685,  0.1325,\n",
       "                       0.5322, -0.5924, -0.5933,  0.5804, -0.0363,  0.2322, -0.2270, -0.0069,\n",
       "                      -0.3979,  0.6534,  0.7974,  0.7336, -0.1633,  0.6975, -0.2381, -0.3888,\n",
       "                       0.3502, -0.4774,  0.7124, -0.6442,  0.1411,  0.7439,  0.6008, -0.3045,\n",
       "                       0.4123, -0.1937,  0.3146,  0.7625,  0.6020, -0.7951,  0.0917,  0.7113])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 0.2526,  0.8655,  0.6731, -0.6622,  0.5529, -0.2617,  0.2826, -0.5639,\n",
       "                       -0.3699, -0.8762, -0.2219, -0.5416, -0.5397,  0.5238, -0.6347,  0.2506,\n",
       "                       -0.7685, -0.7575, -0.3843,  0.3898,  0.5837, -0.4119,  0.2009,  0.4579,\n",
       "                       -0.4014,  0.7624, -0.0811, -0.6644,  0.4818, -0.3738,  0.0497,  0.2887,\n",
       "                        0.5174,  0.5312,  0.6355, -0.0750,  0.5739, -0.6291,  0.6265,  0.4813,\n",
       "                       -0.5347,  0.4033,  0.3406,  0.6800, -0.1685, -0.1239,  0.8535, -0.2141,\n",
       "                        0.5418, -0.0957,  0.1148, -0.7576,  0.5762,  0.3125,  0.2697,  0.6712,\n",
       "                       -0.3756,  0.6191, -0.0208, -0.7577, -0.3909,  0.7637,  0.2231, -0.1236,\n",
       "                        0.1187,  0.5995, -0.0108,  0.1870, -0.6619,  0.7475,  0.5659, -0.0994,\n",
       "                        0.0902,  0.7970, -0.4039, -0.0014,  0.2887, -0.1438,  0.6969, -0.7117,\n",
       "                        0.7875, -0.5607,  0.3133,  0.0701,  0.1517,  0.7619,  0.7482, -0.3779,\n",
       "                       -0.7235,  0.3163,  0.7185,  0.8703, -0.8650,  0.5462, -0.7174, -0.6728,\n",
       "                       -0.4713,  0.2871,  0.7602,  0.4853,  0.1589,  0.6663, -0.4834, -0.4441,\n",
       "                        0.2954,  0.2835,  0.1274,  0.2730,  0.1496,  0.1437, -0.8097,  0.1480,\n",
       "                       -0.2313, -0.5946, -0.2438,  0.1833, -0.2077, -0.6753,  0.5688,  0.6692,\n",
       "                        0.1596, -0.7968, -0.5968,  0.8457,  0.2558, -0.3456, -0.0673,  0.7327]])),\n",
       "             ('linear_relu_stack.4.bias', tensor([-0.0220]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[ 0.4041, -0.1891, -0.8722,  ...,  0.4736, -0.5765, -0.8708],\n",
       "                      [-0.5780,  0.9331, -0.2195,  ...,  0.2756,  0.5369, -0.8328],\n",
       "                      [-0.2777,  0.9136,  0.3660,  ..., -0.3038, -0.6664,  0.8030],\n",
       "                      ...,\n",
       "                      [ 0.0984,  0.7373,  0.9838,  ..., -0.4729,  0.5499, -0.7078],\n",
       "                      [-0.8799, -0.3267, -0.7911,  ..., -0.2823,  0.5873, -0.0626],\n",
       "                      [ 0.3657, -0.5733, -0.9371,  ...,  0.7575,  0.7858, -0.4009]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 5.4423e-01,  6.5464e-02, -1.5226e-01,  5.1342e-01,  9.0922e-01,\n",
       "                       6.1600e-01, -2.5661e-02,  1.0518e-01,  9.6769e-01,  5.6970e-01,\n",
       "                       8.9724e-01, -4.9946e-01,  8.0889e-02,  6.4910e-01,  9.5444e-02,\n",
       "                      -5.3893e-01,  8.6731e-01, -3.9963e-01,  1.1287e-01, -5.5532e-01,\n",
       "                      -3.2117e-01, -2.0443e-01,  1.3313e-02, -1.5156e-01,  4.8146e-01,\n",
       "                      -8.3433e-02, -7.3706e-01,  6.0092e-01, -3.6579e-01, -3.1057e-01,\n",
       "                      -6.0460e-01,  8.0447e-01, -3.6749e-01, -5.5764e-01, -2.9809e-01,\n",
       "                       6.2364e-02, -1.9023e-01,  8.7898e-01,  1.5910e-02, -8.9973e-01,\n",
       "                       1.9874e-01, -5.6882e-01, -1.5566e-01,  2.4939e-02,  8.6988e-01,\n",
       "                       7.8691e-01,  2.1782e-01,  1.2374e-01,  6.2884e-01, -8.3174e-01,\n",
       "                      -6.7970e-01, -8.6877e-01, -9.7044e-01,  8.9195e-01, -4.0171e-01,\n",
       "                       3.2949e-02, -4.2968e-01,  6.3979e-01,  8.0697e-01, -5.8326e-01,\n",
       "                      -3.3114e-01, -8.0763e-01, -6.3242e-01, -8.7352e-01,  6.3698e-02,\n",
       "                      -3.1681e-01,  7.2536e-01, -1.1764e-01, -3.1747e-01, -6.0733e-02,\n",
       "                       8.7750e-01, -5.4208e-01, -2.3317e-01,  1.0241e+00, -9.6283e-01,\n",
       "                      -2.8760e-01,  4.9356e-01,  5.3990e-01, -4.1282e-01, -5.9140e-01,\n",
       "                       7.5621e-01, -4.5685e-01, -7.8868e-04, -5.5261e-01, -1.0120e-01,\n",
       "                       1.8522e-01, -2.0053e-02,  2.3170e-01,  7.0587e-01, -6.8419e-01,\n",
       "                       6.5637e-01, -1.4789e-01,  4.1254e-01, -3.5167e-01, -5.5204e-01,\n",
       "                      -7.9734e-01,  1.7579e-01, -5.1724e-01, -3.9531e-01,  2.4359e-02,\n",
       "                       2.0076e-01,  8.5360e-01,  5.0817e-01, -5.8817e-01, -1.9363e-01,\n",
       "                       7.0475e-02, -7.2055e-01, -5.6512e-01, -4.9026e-01, -3.0454e-01,\n",
       "                      -9.7188e-01, -4.3537e-01, -1.0766e-02, -2.4321e-01, -3.8255e-01,\n",
       "                      -4.4933e-01,  8.5165e-01,  6.6486e-01, -8.1568e-01, -9.0951e-01,\n",
       "                       6.2595e-01, -6.6876e-01,  7.4358e-01, -1.7533e-01, -1.3195e-01,\n",
       "                       3.7214e-01, -5.5474e-01,  9.5415e-01])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.2170,  0.8796, -0.0929,  ..., -0.2876,  0.0105,  0.2404],\n",
       "                      [ 0.6924,  0.0782,  0.6939,  ..., -0.2147,  0.2316, -0.1216],\n",
       "                      [ 0.2915,  0.2767, -0.1310,  ...,  0.5218,  0.2952, -0.4455],\n",
       "                      ...,\n",
       "                      [-0.2758,  0.2841,  0.5993,  ..., -0.5394,  0.2431, -0.1306],\n",
       "                      [-0.7447, -0.3051, -0.4791,  ...,  0.3308,  0.7756,  0.3214],\n",
       "                      [-0.6734,  0.4730, -0.0730,  ...,  0.0769,  0.6312,  0.7300]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 1.9329e-01,  3.1240e-01, -7.6362e-01,  1.6863e-01, -7.4914e-01,\n",
       "                      -5.1193e-01,  5.7389e-01, -1.0658e-01, -7.3820e-01, -4.2324e-01,\n",
       "                      -4.7079e-01, -8.5933e-01, -2.4294e-01,  3.1560e-02, -5.7415e-01,\n",
       "                      -7.0311e-02, -6.8579e-01,  3.0186e-01, -5.6025e-01, -1.7668e-02,\n",
       "                       7.4144e-01, -7.0861e-01, -7.3741e-01,  1.2099e-01, -5.5537e-01,\n",
       "                       8.1481e-01,  4.2461e-01, -5.1966e-01, -8.5784e-01,  6.0590e-01,\n",
       "                       1.7310e-01, -2.5214e-02, -1.8429e-01, -6.8079e-02, -7.9382e-02,\n",
       "                       8.5698e-01, -3.4068e-01, -5.7278e-02,  3.7924e-01, -5.3878e-02,\n",
       "                      -1.6471e-01, -6.0288e-01, -8.0793e-02,  5.4289e-01, -5.2237e-01,\n",
       "                       6.4443e-01, -4.5379e-01, -3.9487e-01,  2.9449e-01,  8.2224e-01,\n",
       "                       8.7470e-01,  2.4535e-01, -5.9455e-01,  2.5417e-01,  7.3666e-02,\n",
       "                       3.4267e-01,  2.0281e-01,  2.1432e-01,  4.9832e-01,  3.4065e-01,\n",
       "                       5.4240e-01, -1.3166e-01,  6.6720e-01, -3.5869e-01, -1.2887e-01,\n",
       "                       8.6983e-01, -2.6739e-01, -4.4523e-02,  8.2730e-01,  1.2462e-01,\n",
       "                      -6.9457e-01,  1.1334e-01, -8.5638e-01,  5.2795e-01,  5.7913e-01,\n",
       "                       6.9871e-01, -5.6259e-01,  1.8827e-01,  7.9051e-01,  6.0801e-01,\n",
       "                      -3.7342e-01, -4.5179e-01,  8.1480e-01,  2.1376e-01,  1.4488e-01,\n",
       "                      -1.5789e-01,  4.9224e-01, -7.5763e-01,  2.8706e-01, -7.9390e-01,\n",
       "                      -3.3303e-01,  4.3862e-01,  1.6965e-02,  5.3305e-01,  7.3596e-02,\n",
       "                       1.3734e-01,  5.3066e-01, -5.9228e-01, -5.8972e-01,  5.8901e-01,\n",
       "                      -3.5027e-02,  2.2589e-01, -2.2888e-01, -7.9232e-04, -4.0045e-01,\n",
       "                       6.5434e-01,  7.9709e-01,  7.3797e-01, -1.6307e-01,  6.9918e-01,\n",
       "                      -2.4319e-01, -3.8886e-01,  3.4944e-01, -4.7114e-01,  7.1178e-01,\n",
       "                      -6.4416e-01,  1.4273e-01,  7.4724e-01,  5.9792e-01, -2.9809e-01,\n",
       "                       4.1900e-01, -1.8079e-01,  3.1516e-01,  7.6718e-01,  6.0147e-01,\n",
       "                      -7.9517e-01,  9.1822e-02,  7.2208e-01])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[-0.0787,  1.2254,  0.7794, -0.7770,  0.6025, -0.1648,  0.3524, -0.6192,\n",
       "                       -1.2496, -1.4802, -0.2863, -0.2358, -0.5204,  0.7715, -0.6167,  0.2078,\n",
       "                       -1.2308, -1.4799, -0.8247,  0.5068,  0.5308, -0.1640,  0.4906,  0.8341,\n",
       "                       -0.3262,  0.7163,  0.1566, -0.5012,  0.1775, -0.2126,  0.1800,  0.1614,\n",
       "                        0.2803,  0.6012,  0.6844,  0.2463,  0.5185, -0.8508,  0.4947,  0.6247,\n",
       "                       -0.4287,  0.7181,  0.4531,  0.6738, -0.3301,  0.0366,  0.9537, -0.1109,\n",
       "                        0.5794, -0.5013,  0.1910, -0.5337,  0.6465,  0.0047,  0.4424,  0.7225,\n",
       "                       -0.3020,  0.4937,  0.3251, -0.7408, -0.3125,  0.8438,  0.1586, -0.1064,\n",
       "                       -0.0803,  0.5182, -0.1114,  0.0272, -1.1090,  0.8033,  0.5210,  0.2766,\n",
       "                        0.4922,  0.8044, -0.3795,  0.2596, -0.1126,  0.0755,  0.5841, -0.5584,\n",
       "                        0.4420, -0.8872, -0.0746,  0.7406,  0.1588,  0.6908,  0.6522, -0.6071,\n",
       "                       -1.1208,  0.6146,  0.8908,  0.8490, -0.6454,  0.4167, -0.8545, -0.9218,\n",
       "                       -0.2009,  0.2798,  0.7715,  1.0486,  0.1986,  0.2801, -0.4191, -0.6608,\n",
       "                        0.1086,  0.4508,  0.1375,  0.4893, -0.1916,  0.2840, -0.5259,  0.1386,\n",
       "                        0.0556, -0.6505, -0.0631,  0.1607, -0.3730, -0.7542,  0.4787,  0.7467,\n",
       "                        0.4784, -1.2968, -0.5442,  0.9534,  0.1420, -0.3301,  0.0747,  0.9455]])),\n",
       "             ('linear_relu_stack.4.bias', tensor([-0.0228]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'large_model_post' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mean_pre_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(large_model_pre[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m----> 2\u001b[0m mean_post_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mlarge_model_post\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      3\u001b[0m std_pre_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(large_model_pre[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      4\u001b[0m std_post_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(large_model_post[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'large_model_post' is not defined"
     ]
    }
   ],
   "source": [
    "mean_pre_0 = np.mean(large_model_pre[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_post_0 = np.mean(large_model_post[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_pre_0 = np.std(large_model_pre[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_post_0 = np.std(large_model_post[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_pre_1 = np.mean(large_model_pre[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_post_1 = np.mean(large_model_post[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_pre_1 = np.std(large_model_pre[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_post_1 = np.std(large_model_post[\"linear_relu_stack.2.weight\"].numpy())\n",
    "print(\"Pre: \")\n",
    "print(\" Layer 1: \" + str(mean_pre_0) + \", \" + str(std_pre_0))\n",
    "print(\" Layer 2: \" + str(mean_pre_1) + \", \" + str(std_pre_1))\n",
    "print(\"Post: \")\n",
    "print(\" Layer 1: \" + str(mean_post_0) + \", \" + str(std_post_0))\n",
    "print(\" Layer 2: \" + str(mean_post_1) + \", \" + str(std_post_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "for item in large_model_post:\n",
    "    if item[20:] == \"weight\":\n",
    "        table = pd.DataFrame(large_model_post[item])\n",
    "        table.to_csv(\"results/mul10_relu_relu_small_regress_weight_post_\" + str(ID) + \".csv\",index=False)\n",
    "    if item[20:] == \"bias\":\n",
    "        series = pd.Series(large_model_post[item])\n",
    "        series.to_csv(\"results/mul10_relu_relu_small_regress_bias_post_\" + str(ID) + \".csv\",index=False)\n",
    "    ID += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "for item in large_model_pre:\n",
    "    if item[20:] == \"weight\":\n",
    "        table = pd.DataFrame(large_model_pre[item])\n",
    "        table.to_csv(\"results/mul10_relu_relu_small_regress_weight_pre_\" + str(ID) + \".csv\",index=False)\n",
    "    if item[20:] == \"bias\":\n",
    "        series = pd.Series(large_model_pre[item])\n",
    "        series.to_csv(\"results/mul10_relu_relu_small_regress_bias_pre_\" + str(ID) + \".csv\",index=False)\n",
    "    ID += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the newer experiments, it seems that Binary Classification partialy follows what was found with MNIST, and Regression doesn't really follow at all. For regression this makes sense, as the network is trying to predict a value along all real numbers as oposed to some set of choices. For Binary classification, it has always been close, but not really solid. There were times where it did follow MNIST, and other times where it didn't follow at all. It could be possible the data in the images in MNIST are different than the generated sets, but i have no idea at this point on how to characterize the MNIST set to be similiar with sklearn's make_classification. Have not yet tested the make_multiclass_classification, and have not yet dived into the Heatmaps of weights for the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_multiclass_problem = make_multilabel_classification(n_samples=25000, n_features=500, n_classes=10, n_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_multiclass_train_data = GeneratedDataset(large_multiclass_problem[0][int(len(large_multiclass_problem[0])*.2):], large_multiclass_problem[1][int(len(large_multiclass_problem[1])*.2):])\n",
    "large_multiclass_test_data = GeneratedDataset(large_multiclass_problem[0][:int(len(large_multiclass_problem[0])*.2)], large_multiclass_problem[1][:int(len(large_multiclass_problem[1])*.2)])\n",
    "large_train_loader_multiclass = DataLoader(large_multiclass_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_multiclass = DataLoader(large_multiclass_test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeMulticlassNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "model = LargeMulticlassNetwork().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimize = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # pred = (pred > 0.5).type(torch.float)\n",
    "            # correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.441471 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.449458 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.431501 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.441189 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.432633 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.425189 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.433316 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.418330 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.416986 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.409183 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.402614 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.394229 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.364168 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.339444 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.247414 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.172838 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.307687 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.970508 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.900927 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.853474 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.873333 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.765273 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.689242 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.681034 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.615214 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.624922 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.587897 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.611373 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.570010 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.535191 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LargeMulticlassNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=512, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_multiclass = copy.deepcopy(model.cpu().state_dict())\n",
    "model.to(device)\n",
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(large_train_loader_multiclass, model, loss, optimize)\n",
    "    test(large_test_loader_multiclass, model, loss)\n",
    "print(\"Done!\")\n",
    "post_trained_multiclass = copy.deepcopy(model.cpu().state_dict())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0442, -0.0267, -0.0142,  ..., -0.0404,  0.0069, -0.0183],\n",
       "                      [-0.0080, -0.0214,  0.0314,  ...,  0.0024,  0.0401,  0.0208],\n",
       "                      [ 0.0328,  0.0305,  0.0125,  ..., -0.0054,  0.0332,  0.0372],\n",
       "                      ...,\n",
       "                      [ 0.0295,  0.0069,  0.0092,  ..., -0.0432,  0.0014,  0.0428],\n",
       "                      [-0.0401, -0.0193, -0.0308,  ...,  0.0003, -0.0158,  0.0322],\n",
       "                      [-0.0280,  0.0170,  0.0132,  ..., -0.0377, -0.0099, -0.0394]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 4.2919e-02, -1.5841e-02, -1.6990e-02,  3.6192e-02, -2.4385e-02,\n",
       "                       9.3218e-03, -3.9001e-02,  2.3250e-02,  9.7217e-03,  4.5774e-04,\n",
       "                       3.9148e-02,  1.4828e-02,  1.2344e-02,  3.8993e-02, -4.0246e-02,\n",
       "                      -1.2096e-03, -2.7187e-02, -3.4861e-02,  2.7642e-02,  4.5555e-03,\n",
       "                      -1.9925e-03, -1.5783e-02, -1.4978e-02, -4.4663e-02,  2.4437e-02,\n",
       "                      -1.5667e-02,  1.3683e-02, -2.8462e-02,  3.6868e-02,  2.3563e-02,\n",
       "                       1.1429e-02,  1.1516e-02,  2.6937e-02,  6.4324e-03, -2.5738e-02,\n",
       "                      -1.4432e-02,  8.3003e-04, -3.0492e-02,  4.3186e-03, -2.2549e-03,\n",
       "                      -3.2131e-02, -2.4438e-02, -1.0092e-02, -1.9646e-02, -3.7702e-02,\n",
       "                       4.0567e-02,  4.3226e-02, -2.5316e-02, -2.9232e-02,  2.6409e-02,\n",
       "                      -4.2559e-02,  4.0492e-02, -2.3125e-02, -2.2076e-02,  4.2645e-02,\n",
       "                      -3.0410e-02,  3.5413e-02, -7.3846e-04, -4.1316e-02, -1.2369e-02,\n",
       "                       5.4307e-03, -2.8943e-02,  4.1284e-02, -3.7023e-02,  2.2492e-02,\n",
       "                      -3.0297e-02,  4.4070e-02,  1.8783e-02,  3.5993e-02, -2.4614e-02,\n",
       "                       3.3787e-02, -1.1708e-02,  2.0215e-02, -9.5376e-03,  3.9534e-03,\n",
       "                      -3.6681e-02,  1.0901e-02, -2.6258e-02, -1.5801e-02,  1.7827e-02,\n",
       "                      -3.4527e-02,  2.8176e-02,  2.0573e-02,  3.9996e-02, -2.9532e-02,\n",
       "                       2.2322e-02, -1.3402e-02, -4.3952e-02,  2.1299e-02, -3.7516e-02,\n",
       "                      -8.4000e-03, -2.6488e-02,  2.2765e-02,  9.2786e-03, -3.9571e-02,\n",
       "                      -4.2792e-03,  1.8525e-02,  1.0290e-02, -1.0074e-02, -2.5316e-02,\n",
       "                       4.2359e-02,  1.3514e-02, -1.9487e-02,  1.7086e-02, -2.5341e-02,\n",
       "                       3.8779e-04,  2.9961e-02, -1.8712e-02,  1.7061e-02, -3.4872e-02,\n",
       "                       3.5349e-02,  4.1470e-02,  2.5135e-02, -1.8869e-02,  2.6922e-02,\n",
       "                       2.5502e-02,  1.7437e-02, -4.3251e-02,  3.1088e-02,  3.6938e-02,\n",
       "                      -9.2920e-03, -6.3694e-03, -3.9495e-02,  6.7332e-04,  1.6638e-02,\n",
       "                      -2.8597e-02,  5.7137e-04, -3.2410e-02, -1.9607e-02, -3.0811e-02,\n",
       "                       3.1279e-03,  1.7080e-02,  1.9491e-03, -2.4371e-02, -3.4631e-02,\n",
       "                      -3.0162e-02,  3.7304e-02,  2.7698e-02, -2.9097e-02,  2.7587e-02,\n",
       "                       1.9266e-02,  3.5160e-02,  4.3132e-02,  9.7055e-03, -3.3284e-02,\n",
       "                       2.5808e-02,  4.2663e-02,  5.3936e-03,  3.0570e-02, -3.2827e-02,\n",
       "                       1.7493e-02, -1.1049e-02,  3.8063e-02,  2.1314e-02, -3.5826e-03,\n",
       "                       2.7030e-02, -4.0066e-03, -1.8182e-03, -2.3191e-02,  5.1671e-03,\n",
       "                       3.9179e-02,  1.4990e-02,  2.5045e-02, -3.2122e-02,  3.0938e-02,\n",
       "                      -1.7865e-02,  1.7510e-02, -3.0486e-02, -2.8647e-02, -3.2022e-02,\n",
       "                      -3.3439e-02, -3.5878e-02,  2.1962e-02, -3.8739e-02, -4.0597e-02,\n",
       "                       7.1588e-03,  2.0890e-02,  9.7208e-03,  7.3897e-03, -1.5755e-02,\n",
       "                       1.6791e-02, -4.0454e-02,  3.1809e-02,  4.0667e-02,  2.1450e-03,\n",
       "                      -1.6465e-02,  6.6190e-03,  6.9814e-03, -5.7527e-03, -4.2159e-02,\n",
       "                      -3.9608e-02, -2.1316e-02, -4.1696e-02,  1.1263e-02,  3.0121e-02,\n",
       "                       1.1055e-02, -5.3364e-04,  4.2215e-02,  2.7819e-02,  3.6696e-02,\n",
       "                      -3.4681e-03, -3.1591e-03,  3.9385e-02,  4.0099e-02,  1.9089e-02,\n",
       "                      -2.3736e-02, -1.9680e-03,  1.5087e-02,  3.1345e-02,  7.4309e-03,\n",
       "                      -1.2527e-02,  4.8264e-03,  3.0957e-02,  4.0005e-02, -3.4140e-02,\n",
       "                       1.3837e-02,  2.1858e-02, -6.7481e-03,  3.6432e-02,  3.6533e-03,\n",
       "                      -2.6390e-02, -1.6164e-02, -4.4794e-03,  3.4618e-02,  3.3310e-03,\n",
       "                       4.4632e-02,  1.4539e-02,  2.4907e-02, -2.0921e-02, -8.1949e-03,\n",
       "                      -2.2362e-03, -2.6006e-02, -1.0870e-02,  2.9540e-02, -3.7819e-02,\n",
       "                       2.2243e-02,  3.3581e-02, -3.1793e-02,  3.4690e-02, -5.5975e-03,\n",
       "                      -2.1055e-02,  1.4616e-02,  2.2538e-02,  7.7122e-03,  1.5132e-04,\n",
       "                       1.7453e-02, -3.1845e-03, -1.4725e-02, -7.0839e-03, -7.7325e-04,\n",
       "                      -2.0728e-02,  5.0794e-03, -3.6899e-02,  1.1463e-02, -2.1333e-03,\n",
       "                      -1.9512e-02, -2.8665e-02,  3.6995e-02,  7.1451e-03, -1.6025e-02,\n",
       "                       2.9667e-02, -9.0934e-03, -3.0711e-02,  2.6737e-02,  3.1398e-03,\n",
       "                      -1.2134e-02, -1.7778e-02, -2.9033e-02,  2.6522e-02, -1.8482e-02,\n",
       "                       6.3252e-03,  4.7878e-03, -3.9361e-02,  2.1052e-02, -1.0538e-03,\n",
       "                       4.7135e-04,  9.2799e-03,  3.3807e-03,  1.2948e-02,  2.4776e-02,\n",
       "                      -3.2196e-02, -4.4745e-03,  4.0036e-02, -3.2269e-02,  8.4117e-03,\n",
       "                       3.7415e-02,  2.6734e-02, -3.6336e-02,  5.1802e-03,  3.6500e-02,\n",
       "                       1.5014e-02, -1.1141e-02, -1.2126e-02,  4.1024e-02, -9.1174e-03,\n",
       "                       1.0423e-02,  2.6420e-02,  2.9675e-02,  3.6432e-02, -2.7257e-02,\n",
       "                       2.4060e-02, -3.7234e-02, -2.3114e-02,  2.3158e-03, -8.1141e-05,\n",
       "                      -2.2075e-02,  9.4981e-03, -3.3749e-02, -2.5561e-02,  4.2973e-02,\n",
       "                      -1.0284e-02,  2.7228e-02,  2.0740e-02, -3.8514e-02, -4.8146e-03,\n",
       "                       3.5118e-02,  3.3850e-02, -6.4603e-03,  2.2083e-02, -1.3166e-02,\n",
       "                       1.6354e-04,  4.3696e-02, -3.8667e-02,  3.9255e-02,  3.0686e-02,\n",
       "                       2.3177e-02, -2.7718e-02, -1.4331e-02,  2.5791e-02, -7.5083e-03,\n",
       "                       3.5781e-02,  1.8480e-02, -2.0134e-02, -3.8802e-02,  1.1074e-02,\n",
       "                      -3.3912e-02,  4.2291e-02,  3.2608e-03,  3.2025e-02,  5.7109e-05,\n",
       "                       2.8834e-02,  3.0357e-02,  1.2946e-02, -2.8580e-02,  5.3944e-03,\n",
       "                      -1.6753e-02,  1.2634e-02, -3.4618e-02, -2.0429e-02, -4.4215e-02,\n",
       "                       5.1954e-03,  1.1770e-02,  2.9606e-02, -1.3361e-02,  7.2780e-03,\n",
       "                      -6.4604e-03,  2.9159e-02, -1.3256e-02, -3.5466e-02,  1.2563e-02,\n",
       "                      -8.7818e-03, -1.0850e-02,  1.3771e-02, -4.3470e-02,  6.1031e-03,\n",
       "                      -4.3301e-02, -1.3212e-02,  1.0330e-02,  4.1972e-02,  3.2686e-02,\n",
       "                      -2.5274e-02,  2.5661e-02, -4.1330e-02,  3.5527e-02,  3.6354e-02,\n",
       "                      -2.7914e-02, -3.5491e-02, -3.3542e-02, -2.5462e-02, -2.2135e-02,\n",
       "                       2.5090e-02,  4.8668e-03, -4.1108e-02,  4.3589e-02, -6.7858e-03,\n",
       "                      -3.1859e-02, -3.5064e-03,  3.1647e-02,  2.2125e-03,  1.3724e-02,\n",
       "                      -2.1358e-02,  1.6079e-02, -3.8844e-02,  2.3398e-02,  1.4325e-02,\n",
       "                      -2.3102e-02, -2.3028e-02, -4.8190e-03,  2.2204e-02, -1.1203e-03,\n",
       "                      -2.0977e-02, -9.2610e-03, -5.9871e-04,  1.1788e-02, -3.0470e-02,\n",
       "                      -2.5715e-02, -4.1657e-02,  2.2117e-02,  3.4363e-02, -2.9430e-02,\n",
       "                       4.2899e-02,  2.7564e-02, -1.0463e-02,  2.7081e-02, -2.2238e-02,\n",
       "                      -1.8444e-02, -3.3858e-02,  3.8548e-02, -4.3726e-02, -4.0435e-02,\n",
       "                       2.6269e-02, -4.1063e-02,  1.1340e-02,  2.5527e-02,  2.0959e-02,\n",
       "                       1.5809e-03, -1.5378e-02,  3.4883e-02,  4.1344e-02,  3.7476e-02,\n",
       "                      -1.3932e-02,  2.4987e-02, -2.8856e-02,  2.4991e-02, -1.4385e-02,\n",
       "                      -3.1513e-02, -1.2340e-03,  2.2979e-02,  1.2414e-03,  1.8118e-02,\n",
       "                      -3.6073e-02, -5.3088e-04, -1.5426e-02,  4.2410e-02, -2.1601e-02,\n",
       "                       3.8026e-02,  4.3829e-02, -2.0694e-02,  2.4534e-02, -1.7281e-02,\n",
       "                       4.1231e-02, -2.6891e-02,  2.5363e-02,  1.7329e-02,  6.0904e-03,\n",
       "                      -1.3550e-02,  3.1440e-02, -1.8452e-02, -2.0341e-02, -2.4197e-02,\n",
       "                      -2.8669e-02,  6.9259e-03, -3.9963e-02, -3.1522e-02, -2.4140e-02,\n",
       "                       1.5685e-02, -3.2849e-02, -3.1740e-02, -3.4168e-02,  2.3480e-02,\n",
       "                      -2.0965e-02, -1.2151e-02, -3.4091e-02,  2.4980e-02,  9.8147e-03,\n",
       "                      -3.0488e-02,  9.9239e-03, -1.6309e-02, -1.5159e-02, -1.5041e-03,\n",
       "                       1.0409e-02,  2.6331e-02,  3.3297e-02,  3.7531e-02,  8.3748e-03,\n",
       "                       2.5474e-02, -1.7451e-02,  1.6494e-02, -3.7877e-02,  3.8508e-02,\n",
       "                       3.9447e-03,  4.3836e-02, -3.8693e-02, -2.6466e-02,  3.2550e-03,\n",
       "                      -2.5961e-02, -2.6318e-02, -1.2325e-02, -3.7317e-02,  2.2609e-02,\n",
       "                       6.2834e-04, -1.1728e-02,  4.0327e-02,  4.1645e-02, -4.0620e-02,\n",
       "                      -2.2461e-02, -4.0615e-02, -1.5436e-02, -2.7801e-02, -2.1813e-02,\n",
       "                       2.1569e-03, -4.0714e-02])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0256,  0.0010, -0.0174,  ...,  0.0165,  0.0378, -0.0139],\n",
       "                      [ 0.0030, -0.0202,  0.0408,  ...,  0.0286,  0.0323, -0.0223],\n",
       "                      [-0.0441, -0.0331,  0.0194,  ...,  0.0092, -0.0363, -0.0261],\n",
       "                      ...,\n",
       "                      [ 0.0014, -0.0069, -0.0060,  ...,  0.0023,  0.0175, -0.0419],\n",
       "                      [-0.0028,  0.0431, -0.0389,  ...,  0.0171,  0.0254,  0.0181],\n",
       "                      [-0.0344, -0.0100,  0.0259,  ..., -0.0105,  0.0129,  0.0189]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 6.2033e-03, -3.6454e-02,  2.4840e-02, -2.3864e-02, -3.1969e-02,\n",
       "                      -3.1136e-02,  3.5751e-02,  4.5672e-03, -4.1532e-02,  3.7931e-02,\n",
       "                      -4.1443e-02, -1.3818e-02, -3.7158e-03, -2.5977e-02,  1.9919e-02,\n",
       "                      -2.0179e-02,  1.1743e-02, -1.6106e-02, -3.7786e-02, -5.8146e-03,\n",
       "                      -2.0921e-02,  1.8075e-02,  3.3292e-02, -2.7041e-02, -1.7289e-02,\n",
       "                      -1.8372e-02,  1.0185e-02, -1.3901e-02, -3.7852e-02, -2.6518e-02,\n",
       "                       2.9819e-02, -2.2631e-02, -4.2205e-02, -2.3796e-02, -1.1658e-02,\n",
       "                      -2.7403e-03, -2.2736e-02,  3.4476e-02, -2.6470e-02, -4.1084e-02,\n",
       "                       1.9263e-02, -3.1561e-02, -3.8552e-02, -3.9993e-02,  2.5609e-02,\n",
       "                      -1.6175e-02, -3.9501e-02,  6.5688e-03,  3.2491e-02, -2.2135e-02,\n",
       "                      -1.3266e-02, -4.3471e-02,  4.2182e-02,  1.1393e-02, -9.3449e-03,\n",
       "                      -1.4078e-02,  2.8474e-02, -4.0110e-02,  1.7702e-02,  3.0147e-02,\n",
       "                      -2.1448e-03,  3.9763e-02, -5.4118e-03, -6.3866e-03,  9.9372e-03,\n",
       "                       2.2240e-02,  2.4133e-02, -1.0339e-02,  4.3277e-02,  2.7752e-02,\n",
       "                      -6.9769e-03, -2.3918e-02,  9.8319e-03, -3.5194e-02, -2.5265e-02,\n",
       "                       4.0292e-02, -2.4896e-02, -1.8491e-02,  1.5668e-02, -5.5232e-03,\n",
       "                       3.3664e-03, -2.1803e-02,  2.4222e-02,  1.5621e-03, -3.2835e-02,\n",
       "                       1.8487e-02,  3.1218e-02,  4.2126e-02,  3.0105e-03, -3.1263e-02,\n",
       "                       3.2268e-02,  2.4725e-02,  1.7561e-02,  3.5035e-02, -3.6859e-02,\n",
       "                       1.7509e-02,  3.3741e-02, -3.1527e-02, -2.4409e-02,  2.5611e-02,\n",
       "                       1.5623e-02, -4.3195e-02,  3.6609e-02,  1.9280e-02, -9.9381e-04,\n",
       "                       2.1039e-02, -3.9929e-02, -2.1669e-02,  2.1034e-02, -4.1365e-02,\n",
       "                      -1.9672e-02,  4.3977e-02, -7.2013e-03, -4.0071e-02, -2.6460e-02,\n",
       "                       4.0228e-02, -3.3410e-02, -3.0551e-02,  3.6291e-02, -1.7544e-02,\n",
       "                      -1.3964e-02,  2.5043e-02, -3.1431e-02,  4.3047e-02, -2.5323e-02,\n",
       "                      -1.7679e-02, -3.1617e-02, -9.7093e-03,  2.0165e-02,  4.3757e-02,\n",
       "                      -3.5220e-02,  1.5539e-02,  2.0077e-02, -2.4188e-02,  3.9338e-02,\n",
       "                       2.1934e-02,  2.3574e-02,  1.3812e-02, -4.0228e-02,  3.2230e-02,\n",
       "                       2.6740e-02, -2.9540e-02, -3.6990e-02,  2.0265e-02, -1.8293e-02,\n",
       "                      -3.1039e-02, -1.4023e-02, -3.2946e-03, -3.6064e-02, -7.6480e-04,\n",
       "                      -1.8280e-02, -2.3067e-02, -2.0000e-02,  3.0118e-02,  4.0151e-02,\n",
       "                      -2.4201e-02, -2.3983e-02, -3.8728e-02,  2.6660e-02, -2.1182e-02,\n",
       "                      -3.4128e-02,  1.4151e-02, -4.7820e-03,  1.3584e-02, -2.1897e-02,\n",
       "                      -4.1441e-02, -2.5371e-02, -2.3245e-03,  4.0731e-02, -5.5432e-04,\n",
       "                      -2.5350e-02, -2.5078e-02, -7.5411e-03,  9.4693e-04, -2.5524e-02,\n",
       "                       6.0427e-03, -2.5517e-02, -2.2149e-02, -7.2291e-03,  3.2129e-02,\n",
       "                       3.9876e-02,  1.1445e-02,  4.0360e-02, -2.7096e-02, -1.6440e-02,\n",
       "                      -1.7710e-02, -3.5369e-02, -2.4748e-02,  9.0148e-05,  2.6411e-02,\n",
       "                      -3.4152e-02,  2.7652e-02,  3.3392e-02, -3.7633e-02,  9.4756e-04,\n",
       "                      -1.5747e-03, -3.6267e-03,  6.7521e-03, -1.7051e-02, -2.8549e-02,\n",
       "                       4.6245e-04,  9.0299e-03,  4.2078e-02, -7.3356e-03, -1.4321e-02,\n",
       "                      -1.6048e-02, -3.9840e-02, -4.2772e-02, -1.4447e-02,  3.7584e-02,\n",
       "                      -1.1443e-03, -3.8152e-02, -1.2811e-02, -4.3828e-02, -3.9582e-02,\n",
       "                      -2.2855e-02, -6.2829e-03,  2.3375e-02, -1.1864e-02, -4.2742e-02,\n",
       "                      -3.2915e-02,  2.0263e-02,  1.5206e-02,  3.8419e-02, -4.2916e-02,\n",
       "                      -3.1959e-02, -3.2758e-02, -2.6452e-03, -1.9173e-02,  1.0084e-02,\n",
       "                       2.8580e-02,  2.9653e-02, -4.3661e-03, -1.4818e-02,  3.8734e-02,\n",
       "                      -2.1492e-02,  1.9090e-02, -1.5937e-02,  3.8854e-02,  3.3636e-02,\n",
       "                       2.6380e-02,  4.2184e-02,  2.1365e-02,  4.1200e-02,  6.9404e-04,\n",
       "                       1.7581e-02, -2.7826e-02, -2.9965e-02,  3.1629e-02,  2.6742e-02,\n",
       "                      -2.2913e-03, -1.2512e-02,  3.6464e-02, -3.6170e-02, -8.7660e-03,\n",
       "                       3.9268e-02, -4.9457e-04, -3.0544e-02, -1.0526e-02, -4.4018e-02,\n",
       "                      -1.6380e-02,  1.4597e-02,  3.6199e-02, -4.2220e-02, -4.0165e-02,\n",
       "                      -6.9954e-03,  2.0229e-02,  1.2524e-02, -1.1027e-02,  1.1747e-02,\n",
       "                       2.3912e-02,  3.9065e-02,  3.5779e-02,  2.7214e-02,  3.4514e-02,\n",
       "                      -1.5217e-02, -3.0314e-02,  1.7211e-02,  4.4081e-02, -2.7803e-02,\n",
       "                      -1.6058e-02,  3.2702e-02,  8.3409e-03, -3.0393e-03, -2.4910e-02,\n",
       "                      -3.0768e-02, -1.7979e-02,  5.1641e-03,  1.5313e-02, -5.2246e-03,\n",
       "                       3.4373e-02,  2.8719e-02, -4.2348e-03,  2.0141e-02,  1.2015e-03,\n",
       "                      -4.2326e-02, -1.0387e-02,  2.3848e-02, -2.9246e-02, -4.0365e-02,\n",
       "                      -3.9043e-02,  2.9051e-02,  1.8590e-02,  3.8999e-02, -1.0382e-02,\n",
       "                      -4.2601e-02, -4.1286e-02,  7.0312e-03, -3.0621e-03,  2.5145e-02,\n",
       "                      -3.4343e-02, -1.9705e-02, -8.0682e-03, -6.5501e-03,  2.1942e-02,\n",
       "                       7.0072e-04,  1.1047e-02, -5.4365e-03,  1.8751e-02, -2.9690e-02,\n",
       "                      -3.7400e-02,  1.7656e-02,  1.5130e-02,  8.7649e-03, -1.9781e-02,\n",
       "                       4.1289e-03,  4.0576e-02, -6.5696e-03, -3.7925e-02,  2.7019e-02,\n",
       "                       2.8550e-02,  2.4116e-02,  4.2587e-02, -1.6571e-02, -3.4307e-02,\n",
       "                       1.7989e-02,  2.8853e-02,  1.7492e-03,  2.2968e-02,  2.1538e-02,\n",
       "                      -4.2874e-02, -1.8878e-02,  8.0375e-04,  3.4803e-02,  6.7028e-03,\n",
       "                       2.2051e-02, -3.1635e-02, -1.0441e-02, -2.5631e-02,  3.4103e-02,\n",
       "                      -8.4086e-03,  2.9611e-02, -4.1284e-02,  1.6610e-02,  1.4786e-03,\n",
       "                       2.4627e-02,  1.0624e-02, -7.5199e-03,  2.7799e-02, -1.6320e-02,\n",
       "                      -4.0914e-02,  2.6218e-02,  2.8966e-02,  4.4234e-03,  3.0508e-02,\n",
       "                      -1.1787e-03, -4.0418e-02,  1.4005e-02, -2.6044e-02, -1.9852e-02,\n",
       "                      -1.2046e-02,  1.4320e-02,  1.2873e-02, -2.4248e-03, -2.7927e-02,\n",
       "                      -2.7561e-02, -1.6471e-02, -1.3622e-02, -1.3938e-03, -2.2441e-03,\n",
       "                       3.2373e-02,  3.8010e-02,  3.8571e-02, -1.5675e-03,  3.3987e-02,\n",
       "                       1.4656e-02,  7.2132e-04,  1.7321e-02,  3.3643e-03, -4.0623e-02,\n",
       "                       3.8659e-02, -3.2499e-02,  4.4068e-02,  1.7173e-02,  3.1392e-02,\n",
       "                      -3.8927e-02,  4.3004e-02,  1.3745e-02, -3.7865e-02,  2.3152e-02,\n",
       "                       2.0490e-02, -1.4676e-02, -1.3384e-02,  7.8231e-04, -1.1975e-02,\n",
       "                      -2.0788e-02,  2.3413e-02,  1.9510e-02, -1.5731e-02, -3.9309e-02,\n",
       "                      -2.5501e-04, -3.6751e-03,  4.1739e-04,  1.1103e-02, -6.5124e-03,\n",
       "                       1.8081e-02,  2.5861e-02,  1.9916e-02, -1.0695e-02,  7.0389e-03,\n",
       "                      -3.7878e-03, -9.0262e-03, -3.8327e-02,  3.9878e-02,  1.5522e-03,\n",
       "                       3.0163e-02, -4.3178e-02, -3.0371e-02,  3.1131e-02, -9.3988e-03,\n",
       "                      -3.5250e-02, -7.9107e-03,  4.3239e-02, -3.0932e-02, -7.7175e-03,\n",
       "                      -2.6006e-05, -1.8710e-02, -4.3663e-02,  3.2342e-02,  2.6736e-02,\n",
       "                      -2.2481e-02,  3.2487e-02, -2.5653e-02,  3.5485e-02,  3.5686e-02,\n",
       "                       3.8440e-02,  1.3515e-02,  2.8741e-02, -1.7213e-02, -2.3856e-02,\n",
       "                       3.6070e-02,  4.2841e-04, -2.7174e-02, -2.2725e-02, -1.4189e-02,\n",
       "                       4.1410e-02, -8.2058e-04, -6.1205e-03,  1.6040e-02,  2.5603e-02,\n",
       "                      -1.8715e-02, -1.4629e-02, -1.0982e-02, -3.6595e-02,  2.4020e-02,\n",
       "                      -2.6407e-04, -1.2782e-02,  2.6339e-02, -3.4681e-02, -1.3579e-02,\n",
       "                      -3.5085e-02,  1.5315e-02,  2.3951e-02,  3.9016e-03,  1.4659e-02,\n",
       "                      -1.1397e-02,  1.0259e-02, -1.5553e-02,  2.7056e-02, -1.6432e-02,\n",
       "                       3.3425e-02, -3.5855e-02, -2.1224e-02,  2.1337e-02,  3.7060e-03,\n",
       "                      -1.0146e-02,  2.4103e-02,  2.1835e-03,  1.3621e-02, -4.3992e-02,\n",
       "                      -3.2596e-02,  2.4500e-02,  3.7306e-03,  6.9662e-03, -3.5724e-02,\n",
       "                       3.6388e-02, -3.8721e-04, -1.0207e-02, -5.6509e-04,  2.0900e-02,\n",
       "                       3.7206e-02,  6.4011e-03,  1.7566e-02,  2.3906e-02,  2.2166e-02,\n",
       "                       2.0667e-02,  3.6967e-02,  2.8169e-02, -1.4140e-02, -2.7136e-02,\n",
       "                      -2.1473e-02, -7.8390e-03])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 0.0306,  0.0085,  0.0207,  ...,  0.0164,  0.0421,  0.0182],\n",
       "                      [ 0.0283, -0.0305, -0.0020,  ...,  0.0120,  0.0250, -0.0203],\n",
       "                      [-0.0155, -0.0098, -0.0252,  ...,  0.0438, -0.0066,  0.0102],\n",
       "                      ...,\n",
       "                      [ 0.0077,  0.0049,  0.0091,  ..., -0.0136, -0.0010,  0.0198],\n",
       "                      [ 0.0302, -0.0237, -0.0080,  ..., -0.0049,  0.0064,  0.0272],\n",
       "                      [ 0.0296, -0.0205,  0.0398,  ..., -0.0260,  0.0130, -0.0196]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.0300,  0.0015,  0.0090, -0.0390, -0.0266, -0.0429,  0.0337, -0.0287,\n",
       "                      -0.0435, -0.0272]))])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0418, -0.0115, -0.0255,  ..., -0.0429,  0.0054, -0.0207],\n",
       "                      [-0.0004, -0.0126,  0.0411,  ..., -0.0120,  0.0354,  0.0257],\n",
       "                      [ 0.0204,  0.0260,  0.0241,  ..., -0.0063,  0.0149,  0.0230],\n",
       "                      ...,\n",
       "                      [ 0.0067,  0.0153,  0.0120,  ..., -0.0384, -0.0111,  0.0280],\n",
       "                      [-0.0528, -0.0116, -0.0123,  ..., -0.0145, -0.0355,  0.0126],\n",
       "                      [-0.0372,  0.0035,  0.0203,  ..., -0.0230, -0.0341, -0.0667]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 0.0228, -0.0306, -0.0354,  0.0174, -0.0305, -0.0114, -0.0583, -0.0008,\n",
       "                      -0.0036, -0.0196,  0.0155,  0.0022,  0.0083,  0.0215, -0.0586, -0.0144,\n",
       "                      -0.0454, -0.0503,  0.0060, -0.0172, -0.0233, -0.0360, -0.0346, -0.0632,\n",
       "                       0.0055, -0.0293, -0.0087, -0.0359,  0.0125,  0.0031, -0.0110, -0.0035,\n",
       "                       0.0097, -0.0151, -0.0454, -0.0229, -0.0219, -0.0574, -0.0020, -0.0262,\n",
       "                      -0.0419, -0.0453, -0.0315, -0.0314, -0.0533,  0.0280,  0.0204, -0.0446,\n",
       "                      -0.0444,  0.0063, -0.0567,  0.0200, -0.0301, -0.0363,  0.0158, -0.0525,\n",
       "                       0.0179, -0.0216, -0.0526, -0.0317, -0.0154, -0.0430,  0.0217, -0.0529,\n",
       "                       0.0022, -0.0431,  0.0252,  0.0034,  0.0210, -0.0406,  0.0162, -0.0246,\n",
       "                       0.0009, -0.0330, -0.0205, -0.0507, -0.0068, -0.0356, -0.0376, -0.0014,\n",
       "                      -0.0525,  0.0095, -0.0031,  0.0293, -0.0422,  0.0105, -0.0348, -0.0605,\n",
       "                       0.0055, -0.0470, -0.0194, -0.0438,  0.0061, -0.0049, -0.0614, -0.0223,\n",
       "                      -0.0030, -0.0046, -0.0303, -0.0407,  0.0222, -0.0122, -0.0347, -0.0039,\n",
       "                      -0.0407, -0.0151,  0.0066, -0.0323, -0.0053, -0.0487,  0.0125,  0.0140,\n",
       "                      -0.0022, -0.0389,  0.0017,  0.0101,  0.0019, -0.0594,  0.0105,  0.0106,\n",
       "                      -0.0218, -0.0231, -0.0528, -0.0192, -0.0072, -0.0399, -0.0202, -0.0418,\n",
       "                      -0.0319, -0.0473, -0.0161,  0.0023, -0.0154, -0.0440, -0.0470, -0.0471,\n",
       "                       0.0144,  0.0168, -0.0487,  0.0118, -0.0023,  0.0230,  0.0157, -0.0097,\n",
       "                      -0.0520,  0.0061,  0.0188, -0.0059,  0.0105, -0.0534, -0.0008, -0.0283,\n",
       "                       0.0178, -0.0015, -0.0202,  0.0070, -0.0185, -0.0103, -0.0294, -0.0195,\n",
       "                       0.0267, -0.0070,  0.0059, -0.0374,  0.0164, -0.0431, -0.0072, -0.0324,\n",
       "                      -0.0417, -0.0471, -0.0534, -0.0522, -0.0041, -0.0567, -0.0604, -0.0108,\n",
       "                      -0.0051, -0.0084, -0.0125, -0.0287, -0.0037, -0.0450,  0.0140,  0.0207,\n",
       "                      -0.0101, -0.0359, -0.0113, -0.0101, -0.0227, -0.0563, -0.0614, -0.0413,\n",
       "                      -0.0581, -0.0063,  0.0157, -0.0085, -0.0191,  0.0298,  0.0107,  0.0136,\n",
       "                      -0.0204, -0.0231,  0.0204,  0.0122, -0.0019, -0.0410, -0.0241, -0.0097,\n",
       "                       0.0038, -0.0140, -0.0293, -0.0177,  0.0009,  0.0203, -0.0500, -0.0134,\n",
       "                       0.0137, -0.0227,  0.0217, -0.0137, -0.0418, -0.0384, -0.0196,  0.0087,\n",
       "                      -0.0097,  0.0274, -0.0020,  0.0084, -0.0318, -0.0251, -0.0196, -0.0469,\n",
       "                      -0.0222,  0.0107, -0.0537,  0.0002,  0.0148, -0.0441,  0.0115, -0.0157,\n",
       "                      -0.0391, -0.0017,  0.0026, -0.0122, -0.0145,  0.0045, -0.0200, -0.0303,\n",
       "                      -0.0170, -0.0188, -0.0327, -0.0106, -0.0415, -0.0175, -0.0164, -0.0365,\n",
       "                      -0.0427,  0.0210, -0.0017, -0.0251,  0.0045, -0.0234, -0.0422,  0.0113,\n",
       "                      -0.0119, -0.0222, -0.0369, -0.0444,  0.0031, -0.0371, -0.0194, -0.0131,\n",
       "                      -0.0504,  0.0014, -0.0218, -0.0198, -0.0080, -0.0114, -0.0031,  0.0004,\n",
       "                      -0.0538, -0.0217,  0.0147, -0.0471, -0.0139,  0.0200,  0.0024, -0.0510,\n",
       "                      -0.0113,  0.0144, -0.0011, -0.0329, -0.0361,  0.0241, -0.0301, -0.0098,\n",
       "                       0.0061,  0.0112,  0.0164, -0.0420,  0.0083, -0.0535, -0.0402, -0.0230,\n",
       "                      -0.0146, -0.0432, -0.0125, -0.0451, -0.0438,  0.0232, -0.0285,  0.0014,\n",
       "                      -0.0010, -0.0570, -0.0208,  0.0109,  0.0150, -0.0350, -0.0041, -0.0238,\n",
       "                      -0.0131,  0.0237, -0.0599,  0.0210,  0.0027, -0.0032, -0.0494, -0.0292,\n",
       "                       0.0023, -0.0258,  0.0170, -0.0024, -0.0275, -0.0574, -0.0057, -0.0514,\n",
       "                       0.0266, -0.0195,  0.0094, -0.0102,  0.0112,  0.0069, -0.0104, -0.0435,\n",
       "                      -0.0090, -0.0317, -0.0044, -0.0443, -0.0339, -0.0621, -0.0151,  0.0016,\n",
       "                       0.0175, -0.0364, -0.0107, -0.0212,  0.0066, -0.0311, -0.0509,  0.0068,\n",
       "                      -0.0217, -0.0244, -0.0007, -0.0564, -0.0086, -0.0607, -0.0316, -0.0072,\n",
       "                       0.0219,  0.0124, -0.0538,  0.0086, -0.0579,  0.0161,  0.0217, -0.0500,\n",
       "                      -0.0550, -0.0440, -0.0401, -0.0389,  0.0128, -0.0179, -0.0455,  0.0190,\n",
       "                      -0.0251, -0.0522, -0.0245,  0.0127, -0.0121, -0.0039, -0.0415, -0.0037,\n",
       "                      -0.0582,  0.0037,  0.0033, -0.0341, -0.0409, -0.0262,  0.0031, -0.0203,\n",
       "                      -0.0404, -0.0297, -0.0179, -0.0094, -0.0455, -0.0399, -0.0569,  0.0006,\n",
       "                       0.0143, -0.0438,  0.0265,  0.0061, -0.0316,  0.0101, -0.0405, -0.0359,\n",
       "                      -0.0512,  0.0203, -0.0557, -0.0567, -0.0033, -0.0548, -0.0118,  0.0074,\n",
       "                      -0.0014, -0.0152, -0.0259,  0.0139,  0.0174,  0.0146, -0.0328,  0.0057,\n",
       "                      -0.0411,  0.0149, -0.0280, -0.0492, -0.0225,  0.0069, -0.0223, -0.0013,\n",
       "                      -0.0559, -0.0305, -0.0360,  0.0237, -0.0356,  0.0099,  0.0275, -0.0261,\n",
       "                       0.0086, -0.0323,  0.0226, -0.0464,  0.0116,  0.0002, -0.0121, -0.0341,\n",
       "                       0.0107, -0.0342, -0.0304, -0.0462, -0.0489, -0.0098, -0.0639, -0.0302,\n",
       "                      -0.0375, -0.0067, -0.0478, -0.0404, -0.0620,  0.0087, -0.0328, -0.0356,\n",
       "                      -0.0573, -0.0008, -0.0145, -0.0497, -0.0102, -0.0345, -0.0309, -0.0177,\n",
       "                      -0.0072,  0.0083,  0.0147,  0.0145, -0.0012,  0.0043, -0.0265,  0.0028,\n",
       "                      -0.0496,  0.0190, -0.0137,  0.0274, -0.0557, -0.0444, -0.0129, -0.0461,\n",
       "                      -0.0444, -0.0281, -0.0571,  0.0086, -0.0222, -0.0333,  0.0152,  0.0207,\n",
       "                      -0.0602, -0.0390, -0.0587, -0.0317, -0.0417, -0.0439, -0.0132, -0.0560])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0155, -0.0034, -0.0303,  ...,  0.0214,  0.0218, -0.0175],\n",
       "                      [-0.0079, -0.0765,  0.0400,  ...,  0.0715,  0.0540, -0.0559],\n",
       "                      [-0.0550, -0.0462,  0.0142,  ..., -0.0014, -0.0442, -0.0347],\n",
       "                      ...,\n",
       "                      [-0.0034, -0.0060, -0.0108,  ..., -0.0104,  0.0085, -0.0423],\n",
       "                      [ 0.0086,  0.0238, -0.0490,  ...,  0.0416,  0.0470, -0.0172],\n",
       "                      [-0.0468, -0.0215,  0.0152,  ..., -0.0153,  0.0073, -0.0024]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([-0.0004, -0.0664,  0.0049, -0.0405, -0.0475, -0.0488,  0.0247, -0.0079,\n",
       "                      -0.0492,  0.0211, -0.0652, -0.0267, -0.0191, -0.0403,  0.0117, -0.0438,\n",
       "                       0.0060, -0.0346, -0.0403, -0.0127, -0.0261, -0.0013,  0.0192, -0.0252,\n",
       "                      -0.0292, -0.0241,  0.0001, -0.0322, -0.0418, -0.0388,  0.0302, -0.0357,\n",
       "                      -0.0424, -0.0360, -0.0198, -0.0199, -0.0345,  0.0283, -0.0349, -0.0509,\n",
       "                       0.0074, -0.0498, -0.0566, -0.0455,  0.0025, -0.0307, -0.0490, -0.0136,\n",
       "                       0.0273, -0.0327, -0.0270, -0.0556,  0.0400,  0.0043, -0.0269, -0.0326,\n",
       "                       0.0126, -0.0463,  0.0090,  0.0218, -0.0189,  0.0275, -0.0178, -0.0162,\n",
       "                      -0.0045,  0.0171,  0.0167, -0.0232,  0.0364,  0.0111, -0.0168, -0.0359,\n",
       "                       0.0077, -0.0483, -0.0392,  0.0322, -0.0320, -0.0317, -0.0072, -0.0184,\n",
       "                      -0.0126, -0.0305,  0.0114, -0.0189, -0.0437,  0.0054,  0.0261,  0.0358,\n",
       "                      -0.0099, -0.0502,  0.0181,  0.0183,  0.0056,  0.0263, -0.0525,  0.0022,\n",
       "                       0.0233, -0.0488, -0.0390,  0.0153, -0.0033, -0.0489,  0.0130,  0.0065,\n",
       "                      -0.0040,  0.0164, -0.0502, -0.0360,  0.0203, -0.0478, -0.0355,  0.0423,\n",
       "                      -0.0127, -0.0476, -0.0330,  0.0282, -0.0467, -0.0332,  0.0233, -0.0348,\n",
       "                      -0.0313,  0.0146, -0.0475,  0.0271, -0.0288, -0.0291, -0.0448, -0.0212,\n",
       "                       0.0276,  0.0278, -0.0479,  0.0049,  0.0129, -0.0336,  0.0250,  0.0109,\n",
       "                       0.0168,  0.0036, -0.0542,  0.0278,  0.0147, -0.0409, -0.0521,  0.0183,\n",
       "                      -0.0319, -0.0378, -0.0241, -0.0166, -0.0497, -0.0127, -0.0319, -0.0303,\n",
       "                      -0.0223,  0.0306,  0.0292, -0.0120, -0.0377, -0.0292,  0.0166, -0.0310,\n",
       "                      -0.0442,  0.0150, -0.0175,  0.0063, -0.0301, -0.0544, -0.0403, -0.0125,\n",
       "                       0.0087, -0.0076, -0.0387, -0.0388, -0.0137,  0.0059, -0.0382, -0.0038,\n",
       "                      -0.0336, -0.0327, -0.0060,  0.0201,  0.0310,  0.0109,  0.0289, -0.0397,\n",
       "                      -0.0385, -0.0325, -0.0412, -0.0429, -0.0048,  0.0084, -0.0498,  0.0159,\n",
       "                       0.0210, -0.0549, -0.0057, -0.0093, -0.0214,  0.0050, -0.0369, -0.0426,\n",
       "                      -0.0114,  0.0028,  0.0274, -0.0183, -0.0275, -0.0101, -0.0539, -0.0529,\n",
       "                      -0.0305,  0.0242,  0.0007, -0.0419, -0.0302, -0.0478, -0.0423, -0.0364,\n",
       "                      -0.0005,  0.0212, -0.0184, -0.0540, -0.0364,  0.0039,  0.0050,  0.0340,\n",
       "                      -0.0560, -0.0473, -0.0448, -0.0212, -0.0316,  0.0088,  0.0186,  0.0198,\n",
       "                      -0.0187, -0.0292,  0.0420, -0.0336,  0.0021, -0.0244,  0.0368,  0.0106,\n",
       "                       0.0093,  0.0383,  0.0123,  0.0261, -0.0042,  0.0072, -0.0327, -0.0518,\n",
       "                       0.0239,  0.0143, -0.0094, -0.0243,  0.0293, -0.0416, -0.0282,  0.0372,\n",
       "                      -0.0214, -0.0430, -0.0329, -0.0549, -0.0279,  0.0041,  0.0332, -0.0542,\n",
       "                      -0.0546, -0.0289,  0.0074, -0.0050, -0.0250, -0.0010,  0.0089,  0.0100,\n",
       "                      -0.0023,  0.0126,  0.0277, -0.0197, -0.0387,  0.0007,  0.0321, -0.0337,\n",
       "                      -0.0237,  0.0241,  0.0091, -0.0050, -0.0430, -0.0344, -0.0280, -0.0024,\n",
       "                       0.0055, -0.0100,  0.0185,  0.0215, -0.0225,  0.0071, -0.0086, -0.0501,\n",
       "                      -0.0376,  0.0089, -0.0523, -0.0503, -0.0461,  0.0136,  0.0086,  0.0342,\n",
       "                      -0.0209, -0.0449, -0.0502, -0.0062, -0.0173,  0.0086, -0.0507, -0.0453,\n",
       "                      -0.0285, -0.0191,  0.0163, -0.0063,  0.0133, -0.0041,  0.0044, -0.0478,\n",
       "                      -0.0430,  0.0050,  0.0090,  0.0008, -0.0205, -0.0046,  0.0346, -0.0232,\n",
       "                      -0.0501,  0.0177,  0.0230,  0.0131,  0.0274, -0.0279, -0.0436,  0.0071,\n",
       "                       0.0235, -0.0150,  0.0067,  0.0195, -0.0545, -0.0294, -0.0160,  0.0195,\n",
       "                       0.0061,  0.0102, -0.0402, -0.0162, -0.0405,  0.0202, -0.0218,  0.0200,\n",
       "                      -0.0547,  0.0061, -0.0165,  0.0070,  0.0088, -0.0138,  0.0201, -0.0138,\n",
       "                      -0.0520,  0.0256,  0.0220, -0.0064,  0.0137, -0.0136, -0.0507,  0.0003,\n",
       "                      -0.0290, -0.0522, -0.0258, -0.0013, -0.0036, -0.0100, -0.0415, -0.0442,\n",
       "                      -0.0283, -0.0312, -0.0120, -0.0091,  0.0217,  0.0298,  0.0299, -0.0132,\n",
       "                       0.0237, -0.0007, -0.0081,  0.0052, -0.0045, -0.0483,  0.0403, -0.0498,\n",
       "                       0.0369,  0.0174,  0.0136, -0.0513,  0.0304,  0.0021, -0.0471,  0.0197,\n",
       "                       0.0010, -0.0284, -0.0262, -0.0071, -0.0175, -0.0346,  0.0192,  0.0120,\n",
       "                      -0.0170, -0.0470, -0.0078, -0.0239, -0.0154,  0.0050, -0.0152,  0.0048,\n",
       "                       0.0122,  0.0111, -0.0204,  0.0033, -0.0217, -0.0291, -0.0406,  0.0195,\n",
       "                      -0.0053,  0.0245, -0.0569, -0.0430,  0.0235, -0.0257, -0.0446, -0.0103,\n",
       "                       0.0380, -0.0450, -0.0201,  0.0028, -0.0297, -0.0530,  0.0234,  0.0215,\n",
       "                      -0.0352,  0.0236, -0.0455,  0.0325,  0.0260,  0.0271, -0.0057,  0.0252,\n",
       "                      -0.0301, -0.0292,  0.0115, -0.0112, -0.0360, -0.0333, -0.0270,  0.0247,\n",
       "                      -0.0154, -0.0135,  0.0090,  0.0098, -0.0253, -0.0202, -0.0216, -0.0499,\n",
       "                       0.0045, -0.0167, -0.0252,  0.0212, -0.0475, -0.0165, -0.0471,  0.0063,\n",
       "                       0.0157, -0.0111, -0.0007, -0.0061,  0.0038, -0.0365,  0.0273, -0.0193,\n",
       "                       0.0231, -0.0490, -0.0290, -0.0053, -0.0140, -0.0186,  0.0209, -0.0024,\n",
       "                       0.0023, -0.0544, -0.0460,  0.0228, -0.0067, -0.0051, -0.0311,  0.0270,\n",
       "                      -0.0130, -0.0171, -0.0120,  0.0073,  0.0324, -0.0130,  0.0058,  0.0109,\n",
       "                       0.0096,  0.0090,  0.0205,  0.0168, -0.0312, -0.0427, -0.0355, -0.0224])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 0.1731,  0.4431, -0.0346,  ..., -0.0619,  0.2773,  0.0175],\n",
       "                      [ 0.1551, -0.1612, -0.0157,  ...,  0.1290,  0.0737, -0.0793],\n",
       "                      [-0.0284, -0.0527, -0.0537,  ...,  0.0162, -0.0490, -0.0064],\n",
       "                      ...,\n",
       "                      [-0.0452,  0.2208,  0.0368,  ..., -0.0740,  0.0906,  0.0635],\n",
       "                      [-0.0101, -0.0339,  0.0144,  ..., -0.0143, -0.0188,  0.0652],\n",
       "                      [ 0.0968, -0.1843,  0.0317,  ...,  0.0096,  0.0896, -0.0980]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.1103, -0.1155, -0.0244,  0.0543, -0.0127, -0.2644,  0.1078,  0.1540,\n",
       "                       0.1378, -0.1203]))])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_trained_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre: \n",
      " Layer 1: 8.3034996e-07, 0.025849843\n",
      " Layer 2: 7.102816e-05, 0.025543222\n",
      " Layer 3: 0.00018317447, 0.02547586\n",
      "Post: \n",
      " Layer 1: -0.0015452711, 0.03388202\n",
      " Layer 2: -0.005922225, 0.03329243\n",
      " Layer 3: 0.0001834683, 0.123158395\n"
     ]
    }
   ],
   "source": [
    "mean_pre_0 = np.mean(pre_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_post_0 = np.mean(post_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_pre_0 = np.std(pre_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_post_0 = np.std(post_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_pre_1 = np.mean(pre_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_post_1 = np.mean(post_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_pre_1 = np.std(pre_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_post_1 = np.std(post_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_pre_2 = np.mean(pre_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "mean_post_2 = np.mean(post_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "std_pre_2 = np.std(pre_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "std_post_2 = np.std(post_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "print(\"Pre: \")\n",
    "print(\" Layer 1: \" + str(mean_pre_0) + \", \" + str(std_pre_0))\n",
    "print(\" Layer 2: \" + str(mean_pre_1) + \", \" + str(std_pre_1))\n",
    "print(\" Layer 3: \" + str(mean_pre_2) + \", \" + str(std_pre_2))\n",
    "print(\"Post: \")\n",
    "print(\" Layer 1: \" + str(mean_post_0) + \", \" + str(std_post_0))\n",
    "print(\" Layer 2: \" + str(mean_post_1) + \", \" + str(std_post_1))\n",
    "print(\" Layer 3: \" + str(mean_post_2) + \", \" + str(std_post_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "for item in post_trained_multiclass:\n",
    "    if item[20:] == \"weight\":\n",
    "        table = pd.DataFrame(post_trained_multiclass[item])\n",
    "        table.to_csv(\"results/nonMNIST_multi_weight_post_\" + str(ID) + \".csv\",index=False)\n",
    "    if item[20:] == \"bias\":\n",
    "        series = pd.Series(post_trained_multiclass[item])\n",
    "        series.to_csv(\"results/nonMNIST_multi_bias_post_\" + str(ID) + \".csv\",index=False)\n",
    "    ID += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think these results indicate that rather than there being special characteristics of the MNIST dataset, The Neural Networks have different update patterns depending on the type of problem. It could be possible that the Loss functions impact the way a network updates, since the loss directly impacts the update pattern. However, given the results of the previous sets, this doesn't seem to be the case. I have not been testing for this, though, so this should be taken lightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9/23/2024 - There is a separation between Classification problems and regression problems in terms of network updates. Classification problems build upon patterns previously found in the network, unless it is very extreme. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
