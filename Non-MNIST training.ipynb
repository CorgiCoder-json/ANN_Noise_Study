{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmer: Jacob Maurer\n",
    "date: 9/21/2024\n",
    "Description: This file is meant to make models that are trained on data other than the MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_regression, make_multilabel_classification\n",
    "import copy\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these experiments, we are going to assume about half of the features are informative, with no redundant or repeated features. The samples are going to be (5*number of features) * 10. This is going to be constant across regression and classification. The final sets will be balanced, seperate experiments will be run with unbalanced sets. The batch size will be 10.The split between train and test sets will be 80% training and 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_classification_problem = make_classification(n_samples = 5000, n_features=100, n_informative=50)\n",
    "medium_classification_problem = make_classification(n_samples = 15000, n_features=300, n_informative=150)\n",
    "large_classification_problem = make_classification(n_samples = 25000, n_features=500, n_informative = 250)\n",
    "small_regression_problem = make_regression(n_samples = 5000, n_features=100, n_informative=10)\n",
    "medium_regression_problem = make_regression(n_samples = 15000, n_features=300, n_informative=25)\n",
    "large_regression_problem = make_regression(n_samples = 25000, n_features=500, n_informative = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_c = pd.DataFrame(small_classification_problem[0])\n",
    "small_c['100'] = small_classification_problem[1]\n",
    "medium_c = pd.DataFrame(medium_classification_problem[0])\n",
    "medium_c['300'] = medium_classification_problem[1]\n",
    "large_c = pd.DataFrame(large_classification_problem[0])\n",
    "large_c['500'] = large_classification_problem[1]\n",
    "small_r = pd.DataFrame(small_regression_problem[0])\n",
    "small_r['100'] = small_regression_problem[1]\n",
    "medium_r = pd.DataFrame(medium_regression_problem[0])\n",
    "medium_r['300'] = medium_regression_problem[1]\n",
    "large_r = pd.DataFrame(large_regression_problem[0])\n",
    "large_r['500'] = large_regression_problem[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x = copy.deepcopy(x_data)\n",
    "        self.y = copy.deepcopy(y_data)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], np.float32(self.y[index])\n",
    "\n",
    "small_classification_train_data = GeneratedDataset(small_classification_problem[0][int(len(small_classification_problem[0])*.2):], small_classification_problem[1][int(len(small_classification_problem[1])*.2):])\n",
    "small_classification_test_data = GeneratedDataset(small_classification_problem[0][:int(len(small_classification_problem[0])*.2)], small_classification_problem[1][:int(len(small_classification_problem[1])*.2)])\n",
    "medium_classification_train_data = GeneratedDataset(medium_classification_problem[0][int(len(medium_classification_problem[0])*.2):], medium_classification_problem[1][int(len(medium_classification_problem[1])*.2):])\n",
    "medium_classification_test_data = GeneratedDataset(medium_classification_problem[0][:int(len(medium_classification_problem[0])*.2)], medium_classification_problem[1][:int(len(medium_classification_problem[1])*.2)])\n",
    "large_classification_train_data = GeneratedDataset(large_classification_problem[0][int(len(large_classification_problem[0])*.2):], large_classification_problem[1][int(len(large_classification_problem[1])*.2):])\n",
    "large_classification_test_data = GeneratedDataset(large_classification_problem[0][:int(len(large_classification_problem[0])*.2)], large_classification_problem[1][:int(len(large_classification_problem[1])*.2)])\n",
    "small_train_loader_class = DataLoader(small_classification_train_data, batch_size=100, shuffle=True)\n",
    "small_test_loader_class = DataLoader(small_classification_test_data, batch_size=100, shuffle=True)\n",
    "medium_train_loader_class = DataLoader(medium_classification_train_data, batch_size=100, shuffle=True)\n",
    "medium_test_loader_class = DataLoader(medium_classification_test_data, batch_size=100, shuffle=True)\n",
    "large_train_loader_class = DataLoader(large_classification_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_class = DataLoader(large_classification_test_data, batch_size=100, shuffle=True)\n",
    "small_regression_train_data = GeneratedDataset(small_regression_problem[0][int(len(small_regression_problem[0])*.2):], small_regression_problem[1][int(len(small_regression_problem[1])*.2):])\n",
    "small_regression_test_data = GeneratedDataset(small_regression_problem[0][:int(len(small_regression_problem[0])*.2)], small_regression_problem[1][:int(len(small_regression_problem[1])*.2)])\n",
    "medium_regression_train_data = GeneratedDataset(medium_regression_problem[0][int(len(medium_regression_problem[0])*.2):], medium_regression_problem[1][int(len(medium_regression_problem[1])*.2):])\n",
    "medium_regression_test_data = GeneratedDataset(medium_regression_problem[0][:int(len(medium_regression_problem[0])*.2)], medium_regression_problem[1][:int(len(medium_regression_problem[1])*.2)])\n",
    "large_regression_train_data = GeneratedDataset(large_regression_problem[0][int(len(large_regression_problem[0])*.2):], large_regression_problem[1][int(len(large_regression_problem[1])*.2):])\n",
    "large_regression_test_data = GeneratedDataset(large_regression_problem[0][:int(len(large_regression_problem[0])*.2)], large_regression_problem[1][:int(len(large_regression_problem[1])*.2)])\n",
    "small_train_loader_regress = DataLoader(small_regression_train_data, batch_size=100, shuffle=True)\n",
    "small_test_loader_regress = DataLoader(small_regression_test_data, batch_size=100, shuffle=True)\n",
    "medium_train_loader_regress = DataLoader(medium_regression_train_data, batch_size=100, shuffle=True)\n",
    "medium_test_loader_regress = DataLoader(medium_regression_test_data, batch_size=100, shuffle=True)\n",
    "large_train_loader_regress = DataLoader(large_regression_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_regress = DataLoader(large_regression_test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class MediumClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class LargeClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class SmallRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class MediumRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class LargeRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_class = SmallClassifyNetwork().to(device)\n",
    "medium_model_class = MediumClassifyNetwork().to(device)\n",
    "large_model_class = LargeClassifyNetwork().to(device)\n",
    "small_model_regress = SmallRegressNetwork().to(device)\n",
    "medium_model_regress = MediumRegressNetwork().to(device)\n",
    "large_model_regress = LargeRegressNetwork().to(device)\n",
    "\n",
    "loss_fn_class = nn.BCELoss()\n",
    "optimizer_class = torch.optim.SGD(small_model_class.parameters(), lr=1e-2)\n",
    "loss_fn_regress = nn.MSELoss()\n",
    "optimizer_regress = torch.optim.SGD(large_model_regress.parameters(), lr=1e-2)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.unsqueeze(1)).item()\n",
    "            # pred = (pred > 0.5).type(torch.float)\n",
    "            # correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.662808 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.646982 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.631076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.615583 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.600557 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.585801 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.570873 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.555447 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.541383 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.527308 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.514487 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.502262 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.490826 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.478564 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.468120 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.458694 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.450911 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.443114 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.436004 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.429552 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.423503 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.418058 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.413901 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.409299 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.405322 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.402503 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.399008 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.395536 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.394572 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.390631 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(small_train_loader_class, small_model_class, loss_fn_class, optimizer_class)\n",
    "    test(small_test_loader_class, small_model_class, loss_fn_class)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallClassifyNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_pre = copy.deepcopy(small_model_class.cpu().state_dict())\n",
    "small_model_class.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallClassifyNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_post = copy.deepcopy(small_model_class.cpu().state_dict())\n",
    "small_model_class.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[ 0.0772,  0.0918,  0.0799,  ...,  0.0031,  0.0884,  0.0373],\n",
       "                      [ 0.0779,  0.0235,  0.0840,  ...,  0.0954,  0.0120, -0.0264],\n",
       "                      [-0.0450, -0.0970, -0.0868,  ..., -0.0757,  0.0655, -0.0519],\n",
       "                      ...,\n",
       "                      [-0.0301, -0.0165, -0.0780,  ..., -0.0884,  0.0058,  0.0583],\n",
       "                      [-0.0374, -0.0556,  0.0535,  ...,  0.0527, -0.0830,  0.0281],\n",
       "                      [-0.0634,  0.0806, -0.0161,  ...,  0.0795,  0.0931,  0.0919]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([-0.0045, -0.0199,  0.0393,  0.0219, -0.0557, -0.0642,  0.0049,  0.0947,\n",
       "                      -0.0732,  0.0202, -0.0381,  0.0847,  0.0042, -0.0565,  0.0537,  0.0012,\n",
       "                       0.0850, -0.0440,  0.0604, -0.0527, -0.0665,  0.0807,  0.0535, -0.0325,\n",
       "                       0.0795,  0.0145,  0.0394, -0.0724, -0.0535, -0.0582, -0.0579, -0.0577,\n",
       "                      -0.0186,  0.0835, -0.0141, -0.0320, -0.0908, -0.0270,  0.0493,  0.0316,\n",
       "                       0.0805,  0.0852,  0.0786, -0.0159, -0.0148,  0.0601, -0.0481,  0.0609,\n",
       "                       0.0585,  0.0448,  0.0968,  0.0575, -0.0188, -0.0006,  0.0977, -0.0836,\n",
       "                      -0.0990,  0.0494,  0.0691, -0.0067, -0.0114, -0.0711,  0.0980,  0.0135,\n",
       "                       0.0073,  0.0354,  0.0628, -0.0197, -0.0380,  0.0494, -0.0301, -0.0514,\n",
       "                       0.0487,  0.0393,  0.0921,  0.0760, -0.0555, -0.0792, -0.0487,  0.0687,\n",
       "                      -0.0881, -0.0922,  0.0946,  0.0895,  0.0800,  0.0369, -0.0043,  0.0381,\n",
       "                       0.0521, -0.0550,  0.0906,  0.0636, -0.0388, -0.0013, -0.0592, -0.0823,\n",
       "                      -0.0609, -0.0917, -0.0101, -0.0928, -0.0743, -0.0339,  0.0787, -0.0187,\n",
       "                       0.0170,  0.0336,  0.0890, -0.0475, -0.0518,  0.0785,  0.0490,  0.0294,\n",
       "                       0.0665,  0.0831,  0.0569,  0.0825, -0.0432,  0.0074,  0.0952,  0.0004,\n",
       "                       0.0837,  0.0059, -0.0445,  0.0750,  0.0768, -0.0011, -0.0208,  0.0296])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.0196, -0.0380, -0.0218,  0.0229,  0.0787, -0.0861,  0.0614, -0.0027,\n",
       "                        0.0609,  0.0600, -0.0669, -0.0541,  0.0483,  0.0661,  0.0227, -0.0605,\n",
       "                        0.0145, -0.0324,  0.0477, -0.0118,  0.0154, -0.0003, -0.0526, -0.0442,\n",
       "                       -0.0085, -0.0687,  0.0495,  0.0725, -0.0540,  0.0701, -0.0294, -0.0775,\n",
       "                       -0.0771, -0.0334,  0.0504, -0.0522, -0.0626, -0.0436,  0.0758,  0.0086,\n",
       "                       -0.0713,  0.0036,  0.0563,  0.0634,  0.0683, -0.0467, -0.0598, -0.0638,\n",
       "                        0.0048,  0.0119,  0.0850,  0.0597,  0.0163, -0.0278, -0.0032,  0.0176,\n",
       "                       -0.0275, -0.0316, -0.0136, -0.0845, -0.0343, -0.0061,  0.0301, -0.0518,\n",
       "                       -0.0857,  0.0554,  0.0609,  0.0275,  0.0109,  0.0416,  0.0787,  0.0052,\n",
       "                       -0.0633, -0.0771, -0.0275, -0.0542,  0.0794, -0.0154, -0.0486,  0.0514,\n",
       "                       -0.0029,  0.0002, -0.0003, -0.0496, -0.0326,  0.0353, -0.0458, -0.0673,\n",
       "                        0.0086,  0.0429,  0.0468, -0.0309, -0.0210, -0.0057, -0.0650,  0.0504,\n",
       "                        0.0428,  0.0002, -0.0252,  0.0773, -0.0690, -0.0385,  0.0183, -0.0367,\n",
       "                        0.0869,  0.0612, -0.0426,  0.0495,  0.0407,  0.0341,  0.0840, -0.0091,\n",
       "                       -0.0389, -0.0063, -0.0539,  0.0788,  0.0014,  0.0676, -0.0097,  0.0207,\n",
       "                       -0.0670,  0.0666,  0.0680,  0.0462, -0.0732, -0.0086,  0.0240,  0.0173]])),\n",
       "             ('linear_relu_stack.2.bias', tensor([-0.0308]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[ 0.0779,  0.0917,  0.0799,  ...,  0.0030,  0.0885,  0.0374],\n",
       "                      [ 0.0707,  0.0239,  0.0831,  ...,  0.0954,  0.0112, -0.0266],\n",
       "                      [-0.0390, -0.0973, -0.0864,  ..., -0.0758,  0.0655, -0.0519],\n",
       "                      ...,\n",
       "                      [-0.0342, -0.0159, -0.0784,  ..., -0.0880,  0.0053,  0.0578],\n",
       "                      [-0.0349, -0.0559,  0.0539,  ...,  0.0522, -0.0830,  0.0283],\n",
       "                      [-0.0601,  0.0794, -0.0156,  ...,  0.0787,  0.0933,  0.0918]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([-0.0046, -0.0199,  0.0393,  0.0219, -0.0494, -0.0650,  0.0050,  0.0947,\n",
       "                      -0.0734,  0.0234, -0.0374,  0.0856,  0.0028, -0.0575,  0.0536,  0.0003,\n",
       "                       0.0844, -0.0441,  0.0608, -0.0531, -0.0669,  0.0818,  0.0550, -0.0314,\n",
       "                       0.0792,  0.0165,  0.0399, -0.0727, -0.0551, -0.0572, -0.0556, -0.0579,\n",
       "                      -0.0175,  0.0854, -0.0139, -0.0308, -0.0913, -0.0225,  0.0488,  0.0307,\n",
       "                       0.0790,  0.0862,  0.0780, -0.0149, -0.0147,  0.0600, -0.0476,  0.0631,\n",
       "                       0.0583,  0.0444,  0.0952,  0.0587, -0.0182, -0.0002,  0.0998, -0.0832,\n",
       "                      -0.0984,  0.0488,  0.0689, -0.0057, -0.0107, -0.0717,  0.0978,  0.0116,\n",
       "                       0.0066,  0.0353,  0.0627, -0.0195, -0.0381,  0.0500, -0.0273, -0.0512,\n",
       "                       0.0439,  0.0384,  0.0919,  0.0755, -0.0554, -0.0815, -0.0452,  0.0685,\n",
       "                      -0.0877, -0.0922,  0.0947,  0.0898,  0.0743,  0.0377, -0.0044,  0.0377,\n",
       "                       0.0520, -0.0540,  0.0903,  0.0635, -0.0376, -0.0005, -0.0608, -0.0823,\n",
       "                      -0.0614, -0.0910, -0.0100, -0.0932, -0.0756, -0.0348,  0.0792, -0.0185,\n",
       "                       0.0200,  0.0331,  0.0890, -0.0499, -0.0520,  0.0769,  0.0390,  0.0296,\n",
       "                       0.0644,  0.0834,  0.0595,  0.0811, -0.0428,  0.0072,  0.0965,  0.0018,\n",
       "                       0.0834,  0.0059, -0.0452,  0.0753,  0.0763, -0.0001, -0.0214,  0.0308])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.0020, -0.0901,  0.0843,  0.0496,  0.2196, -0.1682,  0.2565,  0.0018,\n",
       "                        0.0100,  0.1843, -0.1346, -0.1445,  0.0897,  0.1751, -0.0005, -0.1388,\n",
       "                        0.1356, -0.0452,  0.2674, -0.0249, -0.0246, -0.1232, -0.1250, -0.1856,\n",
       "                       -0.0409, -0.1348,  0.0382,  0.0271, -0.2490,  0.3151, -0.0785, -0.0325,\n",
       "                       -0.1766, -0.0382,  0.3334, -0.2271, -0.2204, -0.2710,  0.0490, -0.0914,\n",
       "                       -0.2997,  0.1339,  0.1359,  0.0074,  0.1049, -0.0074, -0.0248, -0.0810,\n",
       "                       -0.0571,  0.0099,  0.0698,  0.0713,  0.0361, -0.0363, -0.1546,  0.1506,\n",
       "                       -0.0729, -0.0748,  0.0740, -0.0927, -0.0900, -0.0654,  0.0050, -0.1243,\n",
       "                       -0.0409, -0.0250,  0.0298,  0.0467,  0.0013,  0.0273,  0.2422, -0.0339,\n",
       "                       -0.2103, -0.0271, -0.1679, -0.1602,  0.1049,  0.1899, -0.2205,  0.0630,\n",
       "                        0.1055, -0.0023,  0.0598,  0.0104, -0.2413,  0.2298, -0.0490, -0.2662,\n",
       "                       -0.0012,  0.0817,  0.0918,  0.0360, -0.1657,  0.1623, -0.1775, -0.0308,\n",
       "                        0.1988,  0.0693,  0.0011,  0.3245, -0.1279, -0.1309,  0.2398, -0.0049,\n",
       "                        0.2302,  0.2084,  0.0410,  0.1835, -0.0373,  0.1995,  0.3222,  0.0317,\n",
       "                       -0.1570, -0.0290, -0.2215,  0.0633, -0.0772, -0.0778, -0.0399,  0.1012,\n",
       "                       -0.0263,  0.0430,  0.0504,  0.0898, -0.0968, -0.0709,  0.0432,  0.0839]])),\n",
       "             ('linear_relu_stack.2.bias', tensor([-0.0599]))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'large_model_post' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mean_pre_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(large_model_pre[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m----> 2\u001b[0m mean_post_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mlarge_model_post\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      3\u001b[0m std_pre_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(large_model_pre[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      4\u001b[0m std_post_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(large_model_post[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'large_model_post' is not defined"
     ]
    }
   ],
   "source": [
    "mean_pre_0 = np.mean(large_model_pre[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_post_0 = np.mean(large_model_post[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_pre_0 = np.std(large_model_pre[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_post_0 = np.std(large_model_post[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_pre_1 = np.mean(large_model_pre[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_post_1 = np.mean(large_model_post[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_pre_1 = np.std(large_model_pre[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_post_1 = np.std(large_model_post[\"linear_relu_stack.2.weight\"].numpy())\n",
    "print(\"Pre: \")\n",
    "print(\" Layer 1: \" + str(mean_pre_0) + \", \" + str(std_pre_0))\n",
    "print(\" Layer 2: \" + str(mean_pre_1) + \", \" + str(std_pre_1))\n",
    "print(\"Post: \")\n",
    "print(\" Layer 1: \" + str(mean_post_0) + \", \" + str(std_post_0))\n",
    "print(\" Layer 2: \" + str(mean_post_1) + \", \" + str(std_post_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "for item in large_model_post:\n",
    "    if item[20:] == \"weight\":\n",
    "        table = pd.DataFrame(large_model_post[item])\n",
    "        table.to_csv(\"results/small_class_weight_post_\" + str(ID) + \".csv\",index=False)\n",
    "    if item[20:] == \"bias\":\n",
    "        series = pd.Series(large_model_post[item])\n",
    "        series.to_csv(\"results/small_class_bias_post_\" + str(ID) + \".csv\",index=False)\n",
    "    ID += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the newer experiments, it seems that Binary Classification partialy follows what was found with MNIST, and Regression doesn't really follow at all. For regression this makes sense, as the network is trying to predict a value along all real numbers as oposed to some set of choices. For Binary classification, it has always been close, but not really solid. There were times where it did follow MNIST, and other times where it didn't follow at all. It could be possible the data in the images in MNIST are different than the generated sets, but i have no idea at this point on how to characterize the MNIST set to be similiar with sklearn's make_classification. Have not yet tested the make_multiclass_classification, and have not yet dived into the Heatmaps of weights for the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_multiclass_problem = make_multilabel_classification(n_samples=25000, n_features=500, n_classes=10, n_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_multiclass_train_data = GeneratedDataset(large_multiclass_problem[0][int(len(large_multiclass_problem[0])*.2):], large_multiclass_problem[1][int(len(large_multiclass_problem[1])*.2):])\n",
    "large_multiclass_test_data = GeneratedDataset(large_multiclass_problem[0][:int(len(large_multiclass_problem[0])*.2)], large_multiclass_problem[1][:int(len(large_multiclass_problem[1])*.2)])\n",
    "large_train_loader_multiclass = DataLoader(large_multiclass_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_multiclass = DataLoader(large_multiclass_test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeMulticlassNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "model = LargeMulticlassNetwork().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimize = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # pred = (pred > 0.5).type(torch.float)\n",
    "            # correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.441471 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.449458 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.431501 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.441189 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.432633 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.425189 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.433316 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.418330 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.416986 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.409183 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.402614 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.394229 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.364168 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.339444 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.247414 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.172838 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.307687 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.970508 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.900927 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.853474 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.873333 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.765273 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.689242 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.681034 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.615214 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.624922 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.587897 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.611373 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.570010 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.535191 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LargeMulticlassNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=512, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_multiclass = copy.deepcopy(model.cpu().state_dict())\n",
    "model.to(device)\n",
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(large_train_loader_multiclass, model, loss, optimize)\n",
    "    test(large_test_loader_multiclass, model, loss)\n",
    "print(\"Done!\")\n",
    "post_trained_multiclass = copy.deepcopy(model.cpu().state_dict())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0442, -0.0267, -0.0142,  ..., -0.0404,  0.0069, -0.0183],\n",
       "                      [-0.0080, -0.0214,  0.0314,  ...,  0.0024,  0.0401,  0.0208],\n",
       "                      [ 0.0328,  0.0305,  0.0125,  ..., -0.0054,  0.0332,  0.0372],\n",
       "                      ...,\n",
       "                      [ 0.0295,  0.0069,  0.0092,  ..., -0.0432,  0.0014,  0.0428],\n",
       "                      [-0.0401, -0.0193, -0.0308,  ...,  0.0003, -0.0158,  0.0322],\n",
       "                      [-0.0280,  0.0170,  0.0132,  ..., -0.0377, -0.0099, -0.0394]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 4.2919e-02, -1.5841e-02, -1.6990e-02,  3.6192e-02, -2.4385e-02,\n",
       "                       9.3218e-03, -3.9001e-02,  2.3250e-02,  9.7217e-03,  4.5774e-04,\n",
       "                       3.9148e-02,  1.4828e-02,  1.2344e-02,  3.8993e-02, -4.0246e-02,\n",
       "                      -1.2096e-03, -2.7187e-02, -3.4861e-02,  2.7642e-02,  4.5555e-03,\n",
       "                      -1.9925e-03, -1.5783e-02, -1.4978e-02, -4.4663e-02,  2.4437e-02,\n",
       "                      -1.5667e-02,  1.3683e-02, -2.8462e-02,  3.6868e-02,  2.3563e-02,\n",
       "                       1.1429e-02,  1.1516e-02,  2.6937e-02,  6.4324e-03, -2.5738e-02,\n",
       "                      -1.4432e-02,  8.3003e-04, -3.0492e-02,  4.3186e-03, -2.2549e-03,\n",
       "                      -3.2131e-02, -2.4438e-02, -1.0092e-02, -1.9646e-02, -3.7702e-02,\n",
       "                       4.0567e-02,  4.3226e-02, -2.5316e-02, -2.9232e-02,  2.6409e-02,\n",
       "                      -4.2559e-02,  4.0492e-02, -2.3125e-02, -2.2076e-02,  4.2645e-02,\n",
       "                      -3.0410e-02,  3.5413e-02, -7.3846e-04, -4.1316e-02, -1.2369e-02,\n",
       "                       5.4307e-03, -2.8943e-02,  4.1284e-02, -3.7023e-02,  2.2492e-02,\n",
       "                      -3.0297e-02,  4.4070e-02,  1.8783e-02,  3.5993e-02, -2.4614e-02,\n",
       "                       3.3787e-02, -1.1708e-02,  2.0215e-02, -9.5376e-03,  3.9534e-03,\n",
       "                      -3.6681e-02,  1.0901e-02, -2.6258e-02, -1.5801e-02,  1.7827e-02,\n",
       "                      -3.4527e-02,  2.8176e-02,  2.0573e-02,  3.9996e-02, -2.9532e-02,\n",
       "                       2.2322e-02, -1.3402e-02, -4.3952e-02,  2.1299e-02, -3.7516e-02,\n",
       "                      -8.4000e-03, -2.6488e-02,  2.2765e-02,  9.2786e-03, -3.9571e-02,\n",
       "                      -4.2792e-03,  1.8525e-02,  1.0290e-02, -1.0074e-02, -2.5316e-02,\n",
       "                       4.2359e-02,  1.3514e-02, -1.9487e-02,  1.7086e-02, -2.5341e-02,\n",
       "                       3.8779e-04,  2.9961e-02, -1.8712e-02,  1.7061e-02, -3.4872e-02,\n",
       "                       3.5349e-02,  4.1470e-02,  2.5135e-02, -1.8869e-02,  2.6922e-02,\n",
       "                       2.5502e-02,  1.7437e-02, -4.3251e-02,  3.1088e-02,  3.6938e-02,\n",
       "                      -9.2920e-03, -6.3694e-03, -3.9495e-02,  6.7332e-04,  1.6638e-02,\n",
       "                      -2.8597e-02,  5.7137e-04, -3.2410e-02, -1.9607e-02, -3.0811e-02,\n",
       "                       3.1279e-03,  1.7080e-02,  1.9491e-03, -2.4371e-02, -3.4631e-02,\n",
       "                      -3.0162e-02,  3.7304e-02,  2.7698e-02, -2.9097e-02,  2.7587e-02,\n",
       "                       1.9266e-02,  3.5160e-02,  4.3132e-02,  9.7055e-03, -3.3284e-02,\n",
       "                       2.5808e-02,  4.2663e-02,  5.3936e-03,  3.0570e-02, -3.2827e-02,\n",
       "                       1.7493e-02, -1.1049e-02,  3.8063e-02,  2.1314e-02, -3.5826e-03,\n",
       "                       2.7030e-02, -4.0066e-03, -1.8182e-03, -2.3191e-02,  5.1671e-03,\n",
       "                       3.9179e-02,  1.4990e-02,  2.5045e-02, -3.2122e-02,  3.0938e-02,\n",
       "                      -1.7865e-02,  1.7510e-02, -3.0486e-02, -2.8647e-02, -3.2022e-02,\n",
       "                      -3.3439e-02, -3.5878e-02,  2.1962e-02, -3.8739e-02, -4.0597e-02,\n",
       "                       7.1588e-03,  2.0890e-02,  9.7208e-03,  7.3897e-03, -1.5755e-02,\n",
       "                       1.6791e-02, -4.0454e-02,  3.1809e-02,  4.0667e-02,  2.1450e-03,\n",
       "                      -1.6465e-02,  6.6190e-03,  6.9814e-03, -5.7527e-03, -4.2159e-02,\n",
       "                      -3.9608e-02, -2.1316e-02, -4.1696e-02,  1.1263e-02,  3.0121e-02,\n",
       "                       1.1055e-02, -5.3364e-04,  4.2215e-02,  2.7819e-02,  3.6696e-02,\n",
       "                      -3.4681e-03, -3.1591e-03,  3.9385e-02,  4.0099e-02,  1.9089e-02,\n",
       "                      -2.3736e-02, -1.9680e-03,  1.5087e-02,  3.1345e-02,  7.4309e-03,\n",
       "                      -1.2527e-02,  4.8264e-03,  3.0957e-02,  4.0005e-02, -3.4140e-02,\n",
       "                       1.3837e-02,  2.1858e-02, -6.7481e-03,  3.6432e-02,  3.6533e-03,\n",
       "                      -2.6390e-02, -1.6164e-02, -4.4794e-03,  3.4618e-02,  3.3310e-03,\n",
       "                       4.4632e-02,  1.4539e-02,  2.4907e-02, -2.0921e-02, -8.1949e-03,\n",
       "                      -2.2362e-03, -2.6006e-02, -1.0870e-02,  2.9540e-02, -3.7819e-02,\n",
       "                       2.2243e-02,  3.3581e-02, -3.1793e-02,  3.4690e-02, -5.5975e-03,\n",
       "                      -2.1055e-02,  1.4616e-02,  2.2538e-02,  7.7122e-03,  1.5132e-04,\n",
       "                       1.7453e-02, -3.1845e-03, -1.4725e-02, -7.0839e-03, -7.7325e-04,\n",
       "                      -2.0728e-02,  5.0794e-03, -3.6899e-02,  1.1463e-02, -2.1333e-03,\n",
       "                      -1.9512e-02, -2.8665e-02,  3.6995e-02,  7.1451e-03, -1.6025e-02,\n",
       "                       2.9667e-02, -9.0934e-03, -3.0711e-02,  2.6737e-02,  3.1398e-03,\n",
       "                      -1.2134e-02, -1.7778e-02, -2.9033e-02,  2.6522e-02, -1.8482e-02,\n",
       "                       6.3252e-03,  4.7878e-03, -3.9361e-02,  2.1052e-02, -1.0538e-03,\n",
       "                       4.7135e-04,  9.2799e-03,  3.3807e-03,  1.2948e-02,  2.4776e-02,\n",
       "                      -3.2196e-02, -4.4745e-03,  4.0036e-02, -3.2269e-02,  8.4117e-03,\n",
       "                       3.7415e-02,  2.6734e-02, -3.6336e-02,  5.1802e-03,  3.6500e-02,\n",
       "                       1.5014e-02, -1.1141e-02, -1.2126e-02,  4.1024e-02, -9.1174e-03,\n",
       "                       1.0423e-02,  2.6420e-02,  2.9675e-02,  3.6432e-02, -2.7257e-02,\n",
       "                       2.4060e-02, -3.7234e-02, -2.3114e-02,  2.3158e-03, -8.1141e-05,\n",
       "                      -2.2075e-02,  9.4981e-03, -3.3749e-02, -2.5561e-02,  4.2973e-02,\n",
       "                      -1.0284e-02,  2.7228e-02,  2.0740e-02, -3.8514e-02, -4.8146e-03,\n",
       "                       3.5118e-02,  3.3850e-02, -6.4603e-03,  2.2083e-02, -1.3166e-02,\n",
       "                       1.6354e-04,  4.3696e-02, -3.8667e-02,  3.9255e-02,  3.0686e-02,\n",
       "                       2.3177e-02, -2.7718e-02, -1.4331e-02,  2.5791e-02, -7.5083e-03,\n",
       "                       3.5781e-02,  1.8480e-02, -2.0134e-02, -3.8802e-02,  1.1074e-02,\n",
       "                      -3.3912e-02,  4.2291e-02,  3.2608e-03,  3.2025e-02,  5.7109e-05,\n",
       "                       2.8834e-02,  3.0357e-02,  1.2946e-02, -2.8580e-02,  5.3944e-03,\n",
       "                      -1.6753e-02,  1.2634e-02, -3.4618e-02, -2.0429e-02, -4.4215e-02,\n",
       "                       5.1954e-03,  1.1770e-02,  2.9606e-02, -1.3361e-02,  7.2780e-03,\n",
       "                      -6.4604e-03,  2.9159e-02, -1.3256e-02, -3.5466e-02,  1.2563e-02,\n",
       "                      -8.7818e-03, -1.0850e-02,  1.3771e-02, -4.3470e-02,  6.1031e-03,\n",
       "                      -4.3301e-02, -1.3212e-02,  1.0330e-02,  4.1972e-02,  3.2686e-02,\n",
       "                      -2.5274e-02,  2.5661e-02, -4.1330e-02,  3.5527e-02,  3.6354e-02,\n",
       "                      -2.7914e-02, -3.5491e-02, -3.3542e-02, -2.5462e-02, -2.2135e-02,\n",
       "                       2.5090e-02,  4.8668e-03, -4.1108e-02,  4.3589e-02, -6.7858e-03,\n",
       "                      -3.1859e-02, -3.5064e-03,  3.1647e-02,  2.2125e-03,  1.3724e-02,\n",
       "                      -2.1358e-02,  1.6079e-02, -3.8844e-02,  2.3398e-02,  1.4325e-02,\n",
       "                      -2.3102e-02, -2.3028e-02, -4.8190e-03,  2.2204e-02, -1.1203e-03,\n",
       "                      -2.0977e-02, -9.2610e-03, -5.9871e-04,  1.1788e-02, -3.0470e-02,\n",
       "                      -2.5715e-02, -4.1657e-02,  2.2117e-02,  3.4363e-02, -2.9430e-02,\n",
       "                       4.2899e-02,  2.7564e-02, -1.0463e-02,  2.7081e-02, -2.2238e-02,\n",
       "                      -1.8444e-02, -3.3858e-02,  3.8548e-02, -4.3726e-02, -4.0435e-02,\n",
       "                       2.6269e-02, -4.1063e-02,  1.1340e-02,  2.5527e-02,  2.0959e-02,\n",
       "                       1.5809e-03, -1.5378e-02,  3.4883e-02,  4.1344e-02,  3.7476e-02,\n",
       "                      -1.3932e-02,  2.4987e-02, -2.8856e-02,  2.4991e-02, -1.4385e-02,\n",
       "                      -3.1513e-02, -1.2340e-03,  2.2979e-02,  1.2414e-03,  1.8118e-02,\n",
       "                      -3.6073e-02, -5.3088e-04, -1.5426e-02,  4.2410e-02, -2.1601e-02,\n",
       "                       3.8026e-02,  4.3829e-02, -2.0694e-02,  2.4534e-02, -1.7281e-02,\n",
       "                       4.1231e-02, -2.6891e-02,  2.5363e-02,  1.7329e-02,  6.0904e-03,\n",
       "                      -1.3550e-02,  3.1440e-02, -1.8452e-02, -2.0341e-02, -2.4197e-02,\n",
       "                      -2.8669e-02,  6.9259e-03, -3.9963e-02, -3.1522e-02, -2.4140e-02,\n",
       "                       1.5685e-02, -3.2849e-02, -3.1740e-02, -3.4168e-02,  2.3480e-02,\n",
       "                      -2.0965e-02, -1.2151e-02, -3.4091e-02,  2.4980e-02,  9.8147e-03,\n",
       "                      -3.0488e-02,  9.9239e-03, -1.6309e-02, -1.5159e-02, -1.5041e-03,\n",
       "                       1.0409e-02,  2.6331e-02,  3.3297e-02,  3.7531e-02,  8.3748e-03,\n",
       "                       2.5474e-02, -1.7451e-02,  1.6494e-02, -3.7877e-02,  3.8508e-02,\n",
       "                       3.9447e-03,  4.3836e-02, -3.8693e-02, -2.6466e-02,  3.2550e-03,\n",
       "                      -2.5961e-02, -2.6318e-02, -1.2325e-02, -3.7317e-02,  2.2609e-02,\n",
       "                       6.2834e-04, -1.1728e-02,  4.0327e-02,  4.1645e-02, -4.0620e-02,\n",
       "                      -2.2461e-02, -4.0615e-02, -1.5436e-02, -2.7801e-02, -2.1813e-02,\n",
       "                       2.1569e-03, -4.0714e-02])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0256,  0.0010, -0.0174,  ...,  0.0165,  0.0378, -0.0139],\n",
       "                      [ 0.0030, -0.0202,  0.0408,  ...,  0.0286,  0.0323, -0.0223],\n",
       "                      [-0.0441, -0.0331,  0.0194,  ...,  0.0092, -0.0363, -0.0261],\n",
       "                      ...,\n",
       "                      [ 0.0014, -0.0069, -0.0060,  ...,  0.0023,  0.0175, -0.0419],\n",
       "                      [-0.0028,  0.0431, -0.0389,  ...,  0.0171,  0.0254,  0.0181],\n",
       "                      [-0.0344, -0.0100,  0.0259,  ..., -0.0105,  0.0129,  0.0189]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 6.2033e-03, -3.6454e-02,  2.4840e-02, -2.3864e-02, -3.1969e-02,\n",
       "                      -3.1136e-02,  3.5751e-02,  4.5672e-03, -4.1532e-02,  3.7931e-02,\n",
       "                      -4.1443e-02, -1.3818e-02, -3.7158e-03, -2.5977e-02,  1.9919e-02,\n",
       "                      -2.0179e-02,  1.1743e-02, -1.6106e-02, -3.7786e-02, -5.8146e-03,\n",
       "                      -2.0921e-02,  1.8075e-02,  3.3292e-02, -2.7041e-02, -1.7289e-02,\n",
       "                      -1.8372e-02,  1.0185e-02, -1.3901e-02, -3.7852e-02, -2.6518e-02,\n",
       "                       2.9819e-02, -2.2631e-02, -4.2205e-02, -2.3796e-02, -1.1658e-02,\n",
       "                      -2.7403e-03, -2.2736e-02,  3.4476e-02, -2.6470e-02, -4.1084e-02,\n",
       "                       1.9263e-02, -3.1561e-02, -3.8552e-02, -3.9993e-02,  2.5609e-02,\n",
       "                      -1.6175e-02, -3.9501e-02,  6.5688e-03,  3.2491e-02, -2.2135e-02,\n",
       "                      -1.3266e-02, -4.3471e-02,  4.2182e-02,  1.1393e-02, -9.3449e-03,\n",
       "                      -1.4078e-02,  2.8474e-02, -4.0110e-02,  1.7702e-02,  3.0147e-02,\n",
       "                      -2.1448e-03,  3.9763e-02, -5.4118e-03, -6.3866e-03,  9.9372e-03,\n",
       "                       2.2240e-02,  2.4133e-02, -1.0339e-02,  4.3277e-02,  2.7752e-02,\n",
       "                      -6.9769e-03, -2.3918e-02,  9.8319e-03, -3.5194e-02, -2.5265e-02,\n",
       "                       4.0292e-02, -2.4896e-02, -1.8491e-02,  1.5668e-02, -5.5232e-03,\n",
       "                       3.3664e-03, -2.1803e-02,  2.4222e-02,  1.5621e-03, -3.2835e-02,\n",
       "                       1.8487e-02,  3.1218e-02,  4.2126e-02,  3.0105e-03, -3.1263e-02,\n",
       "                       3.2268e-02,  2.4725e-02,  1.7561e-02,  3.5035e-02, -3.6859e-02,\n",
       "                       1.7509e-02,  3.3741e-02, -3.1527e-02, -2.4409e-02,  2.5611e-02,\n",
       "                       1.5623e-02, -4.3195e-02,  3.6609e-02,  1.9280e-02, -9.9381e-04,\n",
       "                       2.1039e-02, -3.9929e-02, -2.1669e-02,  2.1034e-02, -4.1365e-02,\n",
       "                      -1.9672e-02,  4.3977e-02, -7.2013e-03, -4.0071e-02, -2.6460e-02,\n",
       "                       4.0228e-02, -3.3410e-02, -3.0551e-02,  3.6291e-02, -1.7544e-02,\n",
       "                      -1.3964e-02,  2.5043e-02, -3.1431e-02,  4.3047e-02, -2.5323e-02,\n",
       "                      -1.7679e-02, -3.1617e-02, -9.7093e-03,  2.0165e-02,  4.3757e-02,\n",
       "                      -3.5220e-02,  1.5539e-02,  2.0077e-02, -2.4188e-02,  3.9338e-02,\n",
       "                       2.1934e-02,  2.3574e-02,  1.3812e-02, -4.0228e-02,  3.2230e-02,\n",
       "                       2.6740e-02, -2.9540e-02, -3.6990e-02,  2.0265e-02, -1.8293e-02,\n",
       "                      -3.1039e-02, -1.4023e-02, -3.2946e-03, -3.6064e-02, -7.6480e-04,\n",
       "                      -1.8280e-02, -2.3067e-02, -2.0000e-02,  3.0118e-02,  4.0151e-02,\n",
       "                      -2.4201e-02, -2.3983e-02, -3.8728e-02,  2.6660e-02, -2.1182e-02,\n",
       "                      -3.4128e-02,  1.4151e-02, -4.7820e-03,  1.3584e-02, -2.1897e-02,\n",
       "                      -4.1441e-02, -2.5371e-02, -2.3245e-03,  4.0731e-02, -5.5432e-04,\n",
       "                      -2.5350e-02, -2.5078e-02, -7.5411e-03,  9.4693e-04, -2.5524e-02,\n",
       "                       6.0427e-03, -2.5517e-02, -2.2149e-02, -7.2291e-03,  3.2129e-02,\n",
       "                       3.9876e-02,  1.1445e-02,  4.0360e-02, -2.7096e-02, -1.6440e-02,\n",
       "                      -1.7710e-02, -3.5369e-02, -2.4748e-02,  9.0148e-05,  2.6411e-02,\n",
       "                      -3.4152e-02,  2.7652e-02,  3.3392e-02, -3.7633e-02,  9.4756e-04,\n",
       "                      -1.5747e-03, -3.6267e-03,  6.7521e-03, -1.7051e-02, -2.8549e-02,\n",
       "                       4.6245e-04,  9.0299e-03,  4.2078e-02, -7.3356e-03, -1.4321e-02,\n",
       "                      -1.6048e-02, -3.9840e-02, -4.2772e-02, -1.4447e-02,  3.7584e-02,\n",
       "                      -1.1443e-03, -3.8152e-02, -1.2811e-02, -4.3828e-02, -3.9582e-02,\n",
       "                      -2.2855e-02, -6.2829e-03,  2.3375e-02, -1.1864e-02, -4.2742e-02,\n",
       "                      -3.2915e-02,  2.0263e-02,  1.5206e-02,  3.8419e-02, -4.2916e-02,\n",
       "                      -3.1959e-02, -3.2758e-02, -2.6452e-03, -1.9173e-02,  1.0084e-02,\n",
       "                       2.8580e-02,  2.9653e-02, -4.3661e-03, -1.4818e-02,  3.8734e-02,\n",
       "                      -2.1492e-02,  1.9090e-02, -1.5937e-02,  3.8854e-02,  3.3636e-02,\n",
       "                       2.6380e-02,  4.2184e-02,  2.1365e-02,  4.1200e-02,  6.9404e-04,\n",
       "                       1.7581e-02, -2.7826e-02, -2.9965e-02,  3.1629e-02,  2.6742e-02,\n",
       "                      -2.2913e-03, -1.2512e-02,  3.6464e-02, -3.6170e-02, -8.7660e-03,\n",
       "                       3.9268e-02, -4.9457e-04, -3.0544e-02, -1.0526e-02, -4.4018e-02,\n",
       "                      -1.6380e-02,  1.4597e-02,  3.6199e-02, -4.2220e-02, -4.0165e-02,\n",
       "                      -6.9954e-03,  2.0229e-02,  1.2524e-02, -1.1027e-02,  1.1747e-02,\n",
       "                       2.3912e-02,  3.9065e-02,  3.5779e-02,  2.7214e-02,  3.4514e-02,\n",
       "                      -1.5217e-02, -3.0314e-02,  1.7211e-02,  4.4081e-02, -2.7803e-02,\n",
       "                      -1.6058e-02,  3.2702e-02,  8.3409e-03, -3.0393e-03, -2.4910e-02,\n",
       "                      -3.0768e-02, -1.7979e-02,  5.1641e-03,  1.5313e-02, -5.2246e-03,\n",
       "                       3.4373e-02,  2.8719e-02, -4.2348e-03,  2.0141e-02,  1.2015e-03,\n",
       "                      -4.2326e-02, -1.0387e-02,  2.3848e-02, -2.9246e-02, -4.0365e-02,\n",
       "                      -3.9043e-02,  2.9051e-02,  1.8590e-02,  3.8999e-02, -1.0382e-02,\n",
       "                      -4.2601e-02, -4.1286e-02,  7.0312e-03, -3.0621e-03,  2.5145e-02,\n",
       "                      -3.4343e-02, -1.9705e-02, -8.0682e-03, -6.5501e-03,  2.1942e-02,\n",
       "                       7.0072e-04,  1.1047e-02, -5.4365e-03,  1.8751e-02, -2.9690e-02,\n",
       "                      -3.7400e-02,  1.7656e-02,  1.5130e-02,  8.7649e-03, -1.9781e-02,\n",
       "                       4.1289e-03,  4.0576e-02, -6.5696e-03, -3.7925e-02,  2.7019e-02,\n",
       "                       2.8550e-02,  2.4116e-02,  4.2587e-02, -1.6571e-02, -3.4307e-02,\n",
       "                       1.7989e-02,  2.8853e-02,  1.7492e-03,  2.2968e-02,  2.1538e-02,\n",
       "                      -4.2874e-02, -1.8878e-02,  8.0375e-04,  3.4803e-02,  6.7028e-03,\n",
       "                       2.2051e-02, -3.1635e-02, -1.0441e-02, -2.5631e-02,  3.4103e-02,\n",
       "                      -8.4086e-03,  2.9611e-02, -4.1284e-02,  1.6610e-02,  1.4786e-03,\n",
       "                       2.4627e-02,  1.0624e-02, -7.5199e-03,  2.7799e-02, -1.6320e-02,\n",
       "                      -4.0914e-02,  2.6218e-02,  2.8966e-02,  4.4234e-03,  3.0508e-02,\n",
       "                      -1.1787e-03, -4.0418e-02,  1.4005e-02, -2.6044e-02, -1.9852e-02,\n",
       "                      -1.2046e-02,  1.4320e-02,  1.2873e-02, -2.4248e-03, -2.7927e-02,\n",
       "                      -2.7561e-02, -1.6471e-02, -1.3622e-02, -1.3938e-03, -2.2441e-03,\n",
       "                       3.2373e-02,  3.8010e-02,  3.8571e-02, -1.5675e-03,  3.3987e-02,\n",
       "                       1.4656e-02,  7.2132e-04,  1.7321e-02,  3.3643e-03, -4.0623e-02,\n",
       "                       3.8659e-02, -3.2499e-02,  4.4068e-02,  1.7173e-02,  3.1392e-02,\n",
       "                      -3.8927e-02,  4.3004e-02,  1.3745e-02, -3.7865e-02,  2.3152e-02,\n",
       "                       2.0490e-02, -1.4676e-02, -1.3384e-02,  7.8231e-04, -1.1975e-02,\n",
       "                      -2.0788e-02,  2.3413e-02,  1.9510e-02, -1.5731e-02, -3.9309e-02,\n",
       "                      -2.5501e-04, -3.6751e-03,  4.1739e-04,  1.1103e-02, -6.5124e-03,\n",
       "                       1.8081e-02,  2.5861e-02,  1.9916e-02, -1.0695e-02,  7.0389e-03,\n",
       "                      -3.7878e-03, -9.0262e-03, -3.8327e-02,  3.9878e-02,  1.5522e-03,\n",
       "                       3.0163e-02, -4.3178e-02, -3.0371e-02,  3.1131e-02, -9.3988e-03,\n",
       "                      -3.5250e-02, -7.9107e-03,  4.3239e-02, -3.0932e-02, -7.7175e-03,\n",
       "                      -2.6006e-05, -1.8710e-02, -4.3663e-02,  3.2342e-02,  2.6736e-02,\n",
       "                      -2.2481e-02,  3.2487e-02, -2.5653e-02,  3.5485e-02,  3.5686e-02,\n",
       "                       3.8440e-02,  1.3515e-02,  2.8741e-02, -1.7213e-02, -2.3856e-02,\n",
       "                       3.6070e-02,  4.2841e-04, -2.7174e-02, -2.2725e-02, -1.4189e-02,\n",
       "                       4.1410e-02, -8.2058e-04, -6.1205e-03,  1.6040e-02,  2.5603e-02,\n",
       "                      -1.8715e-02, -1.4629e-02, -1.0982e-02, -3.6595e-02,  2.4020e-02,\n",
       "                      -2.6407e-04, -1.2782e-02,  2.6339e-02, -3.4681e-02, -1.3579e-02,\n",
       "                      -3.5085e-02,  1.5315e-02,  2.3951e-02,  3.9016e-03,  1.4659e-02,\n",
       "                      -1.1397e-02,  1.0259e-02, -1.5553e-02,  2.7056e-02, -1.6432e-02,\n",
       "                       3.3425e-02, -3.5855e-02, -2.1224e-02,  2.1337e-02,  3.7060e-03,\n",
       "                      -1.0146e-02,  2.4103e-02,  2.1835e-03,  1.3621e-02, -4.3992e-02,\n",
       "                      -3.2596e-02,  2.4500e-02,  3.7306e-03,  6.9662e-03, -3.5724e-02,\n",
       "                       3.6388e-02, -3.8721e-04, -1.0207e-02, -5.6509e-04,  2.0900e-02,\n",
       "                       3.7206e-02,  6.4011e-03,  1.7566e-02,  2.3906e-02,  2.2166e-02,\n",
       "                       2.0667e-02,  3.6967e-02,  2.8169e-02, -1.4140e-02, -2.7136e-02,\n",
       "                      -2.1473e-02, -7.8390e-03])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 0.0306,  0.0085,  0.0207,  ...,  0.0164,  0.0421,  0.0182],\n",
       "                      [ 0.0283, -0.0305, -0.0020,  ...,  0.0120,  0.0250, -0.0203],\n",
       "                      [-0.0155, -0.0098, -0.0252,  ...,  0.0438, -0.0066,  0.0102],\n",
       "                      ...,\n",
       "                      [ 0.0077,  0.0049,  0.0091,  ..., -0.0136, -0.0010,  0.0198],\n",
       "                      [ 0.0302, -0.0237, -0.0080,  ..., -0.0049,  0.0064,  0.0272],\n",
       "                      [ 0.0296, -0.0205,  0.0398,  ..., -0.0260,  0.0130, -0.0196]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.0300,  0.0015,  0.0090, -0.0390, -0.0266, -0.0429,  0.0337, -0.0287,\n",
       "                      -0.0435, -0.0272]))])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0418, -0.0115, -0.0255,  ..., -0.0429,  0.0054, -0.0207],\n",
       "                      [-0.0004, -0.0126,  0.0411,  ..., -0.0120,  0.0354,  0.0257],\n",
       "                      [ 0.0204,  0.0260,  0.0241,  ..., -0.0063,  0.0149,  0.0230],\n",
       "                      ...,\n",
       "                      [ 0.0067,  0.0153,  0.0120,  ..., -0.0384, -0.0111,  0.0280],\n",
       "                      [-0.0528, -0.0116, -0.0123,  ..., -0.0145, -0.0355,  0.0126],\n",
       "                      [-0.0372,  0.0035,  0.0203,  ..., -0.0230, -0.0341, -0.0667]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 0.0228, -0.0306, -0.0354,  0.0174, -0.0305, -0.0114, -0.0583, -0.0008,\n",
       "                      -0.0036, -0.0196,  0.0155,  0.0022,  0.0083,  0.0215, -0.0586, -0.0144,\n",
       "                      -0.0454, -0.0503,  0.0060, -0.0172, -0.0233, -0.0360, -0.0346, -0.0632,\n",
       "                       0.0055, -0.0293, -0.0087, -0.0359,  0.0125,  0.0031, -0.0110, -0.0035,\n",
       "                       0.0097, -0.0151, -0.0454, -0.0229, -0.0219, -0.0574, -0.0020, -0.0262,\n",
       "                      -0.0419, -0.0453, -0.0315, -0.0314, -0.0533,  0.0280,  0.0204, -0.0446,\n",
       "                      -0.0444,  0.0063, -0.0567,  0.0200, -0.0301, -0.0363,  0.0158, -0.0525,\n",
       "                       0.0179, -0.0216, -0.0526, -0.0317, -0.0154, -0.0430,  0.0217, -0.0529,\n",
       "                       0.0022, -0.0431,  0.0252,  0.0034,  0.0210, -0.0406,  0.0162, -0.0246,\n",
       "                       0.0009, -0.0330, -0.0205, -0.0507, -0.0068, -0.0356, -0.0376, -0.0014,\n",
       "                      -0.0525,  0.0095, -0.0031,  0.0293, -0.0422,  0.0105, -0.0348, -0.0605,\n",
       "                       0.0055, -0.0470, -0.0194, -0.0438,  0.0061, -0.0049, -0.0614, -0.0223,\n",
       "                      -0.0030, -0.0046, -0.0303, -0.0407,  0.0222, -0.0122, -0.0347, -0.0039,\n",
       "                      -0.0407, -0.0151,  0.0066, -0.0323, -0.0053, -0.0487,  0.0125,  0.0140,\n",
       "                      -0.0022, -0.0389,  0.0017,  0.0101,  0.0019, -0.0594,  0.0105,  0.0106,\n",
       "                      -0.0218, -0.0231, -0.0528, -0.0192, -0.0072, -0.0399, -0.0202, -0.0418,\n",
       "                      -0.0319, -0.0473, -0.0161,  0.0023, -0.0154, -0.0440, -0.0470, -0.0471,\n",
       "                       0.0144,  0.0168, -0.0487,  0.0118, -0.0023,  0.0230,  0.0157, -0.0097,\n",
       "                      -0.0520,  0.0061,  0.0188, -0.0059,  0.0105, -0.0534, -0.0008, -0.0283,\n",
       "                       0.0178, -0.0015, -0.0202,  0.0070, -0.0185, -0.0103, -0.0294, -0.0195,\n",
       "                       0.0267, -0.0070,  0.0059, -0.0374,  0.0164, -0.0431, -0.0072, -0.0324,\n",
       "                      -0.0417, -0.0471, -0.0534, -0.0522, -0.0041, -0.0567, -0.0604, -0.0108,\n",
       "                      -0.0051, -0.0084, -0.0125, -0.0287, -0.0037, -0.0450,  0.0140,  0.0207,\n",
       "                      -0.0101, -0.0359, -0.0113, -0.0101, -0.0227, -0.0563, -0.0614, -0.0413,\n",
       "                      -0.0581, -0.0063,  0.0157, -0.0085, -0.0191,  0.0298,  0.0107,  0.0136,\n",
       "                      -0.0204, -0.0231,  0.0204,  0.0122, -0.0019, -0.0410, -0.0241, -0.0097,\n",
       "                       0.0038, -0.0140, -0.0293, -0.0177,  0.0009,  0.0203, -0.0500, -0.0134,\n",
       "                       0.0137, -0.0227,  0.0217, -0.0137, -0.0418, -0.0384, -0.0196,  0.0087,\n",
       "                      -0.0097,  0.0274, -0.0020,  0.0084, -0.0318, -0.0251, -0.0196, -0.0469,\n",
       "                      -0.0222,  0.0107, -0.0537,  0.0002,  0.0148, -0.0441,  0.0115, -0.0157,\n",
       "                      -0.0391, -0.0017,  0.0026, -0.0122, -0.0145,  0.0045, -0.0200, -0.0303,\n",
       "                      -0.0170, -0.0188, -0.0327, -0.0106, -0.0415, -0.0175, -0.0164, -0.0365,\n",
       "                      -0.0427,  0.0210, -0.0017, -0.0251,  0.0045, -0.0234, -0.0422,  0.0113,\n",
       "                      -0.0119, -0.0222, -0.0369, -0.0444,  0.0031, -0.0371, -0.0194, -0.0131,\n",
       "                      -0.0504,  0.0014, -0.0218, -0.0198, -0.0080, -0.0114, -0.0031,  0.0004,\n",
       "                      -0.0538, -0.0217,  0.0147, -0.0471, -0.0139,  0.0200,  0.0024, -0.0510,\n",
       "                      -0.0113,  0.0144, -0.0011, -0.0329, -0.0361,  0.0241, -0.0301, -0.0098,\n",
       "                       0.0061,  0.0112,  0.0164, -0.0420,  0.0083, -0.0535, -0.0402, -0.0230,\n",
       "                      -0.0146, -0.0432, -0.0125, -0.0451, -0.0438,  0.0232, -0.0285,  0.0014,\n",
       "                      -0.0010, -0.0570, -0.0208,  0.0109,  0.0150, -0.0350, -0.0041, -0.0238,\n",
       "                      -0.0131,  0.0237, -0.0599,  0.0210,  0.0027, -0.0032, -0.0494, -0.0292,\n",
       "                       0.0023, -0.0258,  0.0170, -0.0024, -0.0275, -0.0574, -0.0057, -0.0514,\n",
       "                       0.0266, -0.0195,  0.0094, -0.0102,  0.0112,  0.0069, -0.0104, -0.0435,\n",
       "                      -0.0090, -0.0317, -0.0044, -0.0443, -0.0339, -0.0621, -0.0151,  0.0016,\n",
       "                       0.0175, -0.0364, -0.0107, -0.0212,  0.0066, -0.0311, -0.0509,  0.0068,\n",
       "                      -0.0217, -0.0244, -0.0007, -0.0564, -0.0086, -0.0607, -0.0316, -0.0072,\n",
       "                       0.0219,  0.0124, -0.0538,  0.0086, -0.0579,  0.0161,  0.0217, -0.0500,\n",
       "                      -0.0550, -0.0440, -0.0401, -0.0389,  0.0128, -0.0179, -0.0455,  0.0190,\n",
       "                      -0.0251, -0.0522, -0.0245,  0.0127, -0.0121, -0.0039, -0.0415, -0.0037,\n",
       "                      -0.0582,  0.0037,  0.0033, -0.0341, -0.0409, -0.0262,  0.0031, -0.0203,\n",
       "                      -0.0404, -0.0297, -0.0179, -0.0094, -0.0455, -0.0399, -0.0569,  0.0006,\n",
       "                       0.0143, -0.0438,  0.0265,  0.0061, -0.0316,  0.0101, -0.0405, -0.0359,\n",
       "                      -0.0512,  0.0203, -0.0557, -0.0567, -0.0033, -0.0548, -0.0118,  0.0074,\n",
       "                      -0.0014, -0.0152, -0.0259,  0.0139,  0.0174,  0.0146, -0.0328,  0.0057,\n",
       "                      -0.0411,  0.0149, -0.0280, -0.0492, -0.0225,  0.0069, -0.0223, -0.0013,\n",
       "                      -0.0559, -0.0305, -0.0360,  0.0237, -0.0356,  0.0099,  0.0275, -0.0261,\n",
       "                       0.0086, -0.0323,  0.0226, -0.0464,  0.0116,  0.0002, -0.0121, -0.0341,\n",
       "                       0.0107, -0.0342, -0.0304, -0.0462, -0.0489, -0.0098, -0.0639, -0.0302,\n",
       "                      -0.0375, -0.0067, -0.0478, -0.0404, -0.0620,  0.0087, -0.0328, -0.0356,\n",
       "                      -0.0573, -0.0008, -0.0145, -0.0497, -0.0102, -0.0345, -0.0309, -0.0177,\n",
       "                      -0.0072,  0.0083,  0.0147,  0.0145, -0.0012,  0.0043, -0.0265,  0.0028,\n",
       "                      -0.0496,  0.0190, -0.0137,  0.0274, -0.0557, -0.0444, -0.0129, -0.0461,\n",
       "                      -0.0444, -0.0281, -0.0571,  0.0086, -0.0222, -0.0333,  0.0152,  0.0207,\n",
       "                      -0.0602, -0.0390, -0.0587, -0.0317, -0.0417, -0.0439, -0.0132, -0.0560])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0155, -0.0034, -0.0303,  ...,  0.0214,  0.0218, -0.0175],\n",
       "                      [-0.0079, -0.0765,  0.0400,  ...,  0.0715,  0.0540, -0.0559],\n",
       "                      [-0.0550, -0.0462,  0.0142,  ..., -0.0014, -0.0442, -0.0347],\n",
       "                      ...,\n",
       "                      [-0.0034, -0.0060, -0.0108,  ..., -0.0104,  0.0085, -0.0423],\n",
       "                      [ 0.0086,  0.0238, -0.0490,  ...,  0.0416,  0.0470, -0.0172],\n",
       "                      [-0.0468, -0.0215,  0.0152,  ..., -0.0153,  0.0073, -0.0024]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([-0.0004, -0.0664,  0.0049, -0.0405, -0.0475, -0.0488,  0.0247, -0.0079,\n",
       "                      -0.0492,  0.0211, -0.0652, -0.0267, -0.0191, -0.0403,  0.0117, -0.0438,\n",
       "                       0.0060, -0.0346, -0.0403, -0.0127, -0.0261, -0.0013,  0.0192, -0.0252,\n",
       "                      -0.0292, -0.0241,  0.0001, -0.0322, -0.0418, -0.0388,  0.0302, -0.0357,\n",
       "                      -0.0424, -0.0360, -0.0198, -0.0199, -0.0345,  0.0283, -0.0349, -0.0509,\n",
       "                       0.0074, -0.0498, -0.0566, -0.0455,  0.0025, -0.0307, -0.0490, -0.0136,\n",
       "                       0.0273, -0.0327, -0.0270, -0.0556,  0.0400,  0.0043, -0.0269, -0.0326,\n",
       "                       0.0126, -0.0463,  0.0090,  0.0218, -0.0189,  0.0275, -0.0178, -0.0162,\n",
       "                      -0.0045,  0.0171,  0.0167, -0.0232,  0.0364,  0.0111, -0.0168, -0.0359,\n",
       "                       0.0077, -0.0483, -0.0392,  0.0322, -0.0320, -0.0317, -0.0072, -0.0184,\n",
       "                      -0.0126, -0.0305,  0.0114, -0.0189, -0.0437,  0.0054,  0.0261,  0.0358,\n",
       "                      -0.0099, -0.0502,  0.0181,  0.0183,  0.0056,  0.0263, -0.0525,  0.0022,\n",
       "                       0.0233, -0.0488, -0.0390,  0.0153, -0.0033, -0.0489,  0.0130,  0.0065,\n",
       "                      -0.0040,  0.0164, -0.0502, -0.0360,  0.0203, -0.0478, -0.0355,  0.0423,\n",
       "                      -0.0127, -0.0476, -0.0330,  0.0282, -0.0467, -0.0332,  0.0233, -0.0348,\n",
       "                      -0.0313,  0.0146, -0.0475,  0.0271, -0.0288, -0.0291, -0.0448, -0.0212,\n",
       "                       0.0276,  0.0278, -0.0479,  0.0049,  0.0129, -0.0336,  0.0250,  0.0109,\n",
       "                       0.0168,  0.0036, -0.0542,  0.0278,  0.0147, -0.0409, -0.0521,  0.0183,\n",
       "                      -0.0319, -0.0378, -0.0241, -0.0166, -0.0497, -0.0127, -0.0319, -0.0303,\n",
       "                      -0.0223,  0.0306,  0.0292, -0.0120, -0.0377, -0.0292,  0.0166, -0.0310,\n",
       "                      -0.0442,  0.0150, -0.0175,  0.0063, -0.0301, -0.0544, -0.0403, -0.0125,\n",
       "                       0.0087, -0.0076, -0.0387, -0.0388, -0.0137,  0.0059, -0.0382, -0.0038,\n",
       "                      -0.0336, -0.0327, -0.0060,  0.0201,  0.0310,  0.0109,  0.0289, -0.0397,\n",
       "                      -0.0385, -0.0325, -0.0412, -0.0429, -0.0048,  0.0084, -0.0498,  0.0159,\n",
       "                       0.0210, -0.0549, -0.0057, -0.0093, -0.0214,  0.0050, -0.0369, -0.0426,\n",
       "                      -0.0114,  0.0028,  0.0274, -0.0183, -0.0275, -0.0101, -0.0539, -0.0529,\n",
       "                      -0.0305,  0.0242,  0.0007, -0.0419, -0.0302, -0.0478, -0.0423, -0.0364,\n",
       "                      -0.0005,  0.0212, -0.0184, -0.0540, -0.0364,  0.0039,  0.0050,  0.0340,\n",
       "                      -0.0560, -0.0473, -0.0448, -0.0212, -0.0316,  0.0088,  0.0186,  0.0198,\n",
       "                      -0.0187, -0.0292,  0.0420, -0.0336,  0.0021, -0.0244,  0.0368,  0.0106,\n",
       "                       0.0093,  0.0383,  0.0123,  0.0261, -0.0042,  0.0072, -0.0327, -0.0518,\n",
       "                       0.0239,  0.0143, -0.0094, -0.0243,  0.0293, -0.0416, -0.0282,  0.0372,\n",
       "                      -0.0214, -0.0430, -0.0329, -0.0549, -0.0279,  0.0041,  0.0332, -0.0542,\n",
       "                      -0.0546, -0.0289,  0.0074, -0.0050, -0.0250, -0.0010,  0.0089,  0.0100,\n",
       "                      -0.0023,  0.0126,  0.0277, -0.0197, -0.0387,  0.0007,  0.0321, -0.0337,\n",
       "                      -0.0237,  0.0241,  0.0091, -0.0050, -0.0430, -0.0344, -0.0280, -0.0024,\n",
       "                       0.0055, -0.0100,  0.0185,  0.0215, -0.0225,  0.0071, -0.0086, -0.0501,\n",
       "                      -0.0376,  0.0089, -0.0523, -0.0503, -0.0461,  0.0136,  0.0086,  0.0342,\n",
       "                      -0.0209, -0.0449, -0.0502, -0.0062, -0.0173,  0.0086, -0.0507, -0.0453,\n",
       "                      -0.0285, -0.0191,  0.0163, -0.0063,  0.0133, -0.0041,  0.0044, -0.0478,\n",
       "                      -0.0430,  0.0050,  0.0090,  0.0008, -0.0205, -0.0046,  0.0346, -0.0232,\n",
       "                      -0.0501,  0.0177,  0.0230,  0.0131,  0.0274, -0.0279, -0.0436,  0.0071,\n",
       "                       0.0235, -0.0150,  0.0067,  0.0195, -0.0545, -0.0294, -0.0160,  0.0195,\n",
       "                       0.0061,  0.0102, -0.0402, -0.0162, -0.0405,  0.0202, -0.0218,  0.0200,\n",
       "                      -0.0547,  0.0061, -0.0165,  0.0070,  0.0088, -0.0138,  0.0201, -0.0138,\n",
       "                      -0.0520,  0.0256,  0.0220, -0.0064,  0.0137, -0.0136, -0.0507,  0.0003,\n",
       "                      -0.0290, -0.0522, -0.0258, -0.0013, -0.0036, -0.0100, -0.0415, -0.0442,\n",
       "                      -0.0283, -0.0312, -0.0120, -0.0091,  0.0217,  0.0298,  0.0299, -0.0132,\n",
       "                       0.0237, -0.0007, -0.0081,  0.0052, -0.0045, -0.0483,  0.0403, -0.0498,\n",
       "                       0.0369,  0.0174,  0.0136, -0.0513,  0.0304,  0.0021, -0.0471,  0.0197,\n",
       "                       0.0010, -0.0284, -0.0262, -0.0071, -0.0175, -0.0346,  0.0192,  0.0120,\n",
       "                      -0.0170, -0.0470, -0.0078, -0.0239, -0.0154,  0.0050, -0.0152,  0.0048,\n",
       "                       0.0122,  0.0111, -0.0204,  0.0033, -0.0217, -0.0291, -0.0406,  0.0195,\n",
       "                      -0.0053,  0.0245, -0.0569, -0.0430,  0.0235, -0.0257, -0.0446, -0.0103,\n",
       "                       0.0380, -0.0450, -0.0201,  0.0028, -0.0297, -0.0530,  0.0234,  0.0215,\n",
       "                      -0.0352,  0.0236, -0.0455,  0.0325,  0.0260,  0.0271, -0.0057,  0.0252,\n",
       "                      -0.0301, -0.0292,  0.0115, -0.0112, -0.0360, -0.0333, -0.0270,  0.0247,\n",
       "                      -0.0154, -0.0135,  0.0090,  0.0098, -0.0253, -0.0202, -0.0216, -0.0499,\n",
       "                       0.0045, -0.0167, -0.0252,  0.0212, -0.0475, -0.0165, -0.0471,  0.0063,\n",
       "                       0.0157, -0.0111, -0.0007, -0.0061,  0.0038, -0.0365,  0.0273, -0.0193,\n",
       "                       0.0231, -0.0490, -0.0290, -0.0053, -0.0140, -0.0186,  0.0209, -0.0024,\n",
       "                       0.0023, -0.0544, -0.0460,  0.0228, -0.0067, -0.0051, -0.0311,  0.0270,\n",
       "                      -0.0130, -0.0171, -0.0120,  0.0073,  0.0324, -0.0130,  0.0058,  0.0109,\n",
       "                       0.0096,  0.0090,  0.0205,  0.0168, -0.0312, -0.0427, -0.0355, -0.0224])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 0.1731,  0.4431, -0.0346,  ..., -0.0619,  0.2773,  0.0175],\n",
       "                      [ 0.1551, -0.1612, -0.0157,  ...,  0.1290,  0.0737, -0.0793],\n",
       "                      [-0.0284, -0.0527, -0.0537,  ...,  0.0162, -0.0490, -0.0064],\n",
       "                      ...,\n",
       "                      [-0.0452,  0.2208,  0.0368,  ..., -0.0740,  0.0906,  0.0635],\n",
       "                      [-0.0101, -0.0339,  0.0144,  ..., -0.0143, -0.0188,  0.0652],\n",
       "                      [ 0.0968, -0.1843,  0.0317,  ...,  0.0096,  0.0896, -0.0980]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.1103, -0.1155, -0.0244,  0.0543, -0.0127, -0.2644,  0.1078,  0.1540,\n",
       "                       0.1378, -0.1203]))])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_trained_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre: \n",
      " Layer 1: 8.3034996e-07, 0.025849843\n",
      " Layer 2: 7.102816e-05, 0.025543222\n",
      " Layer 3: 0.00018317447, 0.02547586\n",
      "Post: \n",
      " Layer 1: -0.0015452711, 0.03388202\n",
      " Layer 2: -0.005922225, 0.03329243\n",
      " Layer 3: 0.0001834683, 0.123158395\n"
     ]
    }
   ],
   "source": [
    "mean_pre_0 = np.mean(pre_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_post_0 = np.mean(post_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_pre_0 = np.std(pre_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_post_0 = np.std(post_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_pre_1 = np.mean(pre_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_post_1 = np.mean(post_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_pre_1 = np.std(pre_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_post_1 = np.std(post_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_pre_2 = np.mean(pre_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "mean_post_2 = np.mean(post_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "std_pre_2 = np.std(pre_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "std_post_2 = np.std(post_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "print(\"Pre: \")\n",
    "print(\" Layer 1: \" + str(mean_pre_0) + \", \" + str(std_pre_0))\n",
    "print(\" Layer 2: \" + str(mean_pre_1) + \", \" + str(std_pre_1))\n",
    "print(\" Layer 3: \" + str(mean_pre_2) + \", \" + str(std_pre_2))\n",
    "print(\"Post: \")\n",
    "print(\" Layer 1: \" + str(mean_post_0) + \", \" + str(std_post_0))\n",
    "print(\" Layer 2: \" + str(mean_post_1) + \", \" + str(std_post_1))\n",
    "print(\" Layer 3: \" + str(mean_post_2) + \", \" + str(std_post_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "for item in post_trained_multiclass:\n",
    "    if item[20:] == \"weight\":\n",
    "        table = pd.DataFrame(post_trained_multiclass[item])\n",
    "        table.to_csv(\"results/nonMNIST_multi_weight_post_\" + str(ID) + \".csv\",index=False)\n",
    "    if item[20:] == \"bias\":\n",
    "        series = pd.Series(post_trained_multiclass[item])\n",
    "        series.to_csv(\"results/nonMNIST_multi_bias_post_\" + str(ID) + \".csv\",index=False)\n",
    "    ID += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think these results indicate that rather than there being special characteristics of the MNIST dataset, The Neural Networks have different update patterns depending on the type of problem. It could be possible that the Loss functions impact the way a network updates, since the loss directly impacts the update pattern. However, given the results of the previous sets, this doesn't seem to be the case. I have not been testing for this, though, so this should be taken lightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9/23/2024 - There is a separation between Classification problems and regression problems in terms of network updates. Classification problems build upon patterns previously found in the network, unless it is very extreme. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
