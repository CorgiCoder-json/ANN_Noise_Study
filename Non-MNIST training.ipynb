{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmer: Jacob Maurer\n",
    "date: 9/21/2024\n",
    "Description: This file is meant to make models that are trained on data other than the MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "import copy\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these experiments, we are going to assume about half of the features are informative, with no redundant or repeated features. The samples are going to be (5*number of features) * 10. This is going to be constant across regression and classification. The final sets will be balanced, seperate experiments will be run with unbalanced sets. The batch size will be 10.The split between train and test sets will be 80% training and 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_classification_problem = make_classification(n_samples = 5000, n_features=100, n_informative=50)\n",
    "medium_classification_problem = make_classification(n_samples = 15000, n_features=300, n_informative=150)\n",
    "large_classification_problem = make_classification(n_samples = 25000, n_features=500, n_informative = 250)\n",
    "small_regression_problem = make_regression(n_samples = 1000, n_features=20, n_informative=10)\n",
    "medium_regression_problem = make_regression(n_samples = 2500, n_features=50, n_informative=25)\n",
    "large_regression_problem = make_regression(n_samples = 5000, n_features=100, n_informative = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x = copy.deepcopy(x_data)\n",
    "        self.y = copy.deepcopy(y_data)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], np.float32(self.y[index])\n",
    "\n",
    "small_classification_train_data = GeneratedDataset(small_classification_problem[0][int(len(small_classification_problem[0])*.2):], small_classification_problem[1][int(len(small_classification_problem[1])*.2):])\n",
    "small_classification_test_data = GeneratedDataset(small_classification_problem[0][:int(len(small_classification_problem[0])*.2)], small_classification_problem[1][:int(len(small_classification_problem[1])*.2)])\n",
    "medium_classification_train_data = GeneratedDataset(medium_classification_problem[0][int(len(medium_classification_problem[0])*.2):], medium_classification_problem[1][int(len(medium_classification_problem[1])*.2):])\n",
    "medium_classification_test_data = GeneratedDataset(medium_classification_problem[0][:int(len(medium_classification_problem[0])*.2)], medium_classification_problem[1][:int(len(medium_classification_problem[1])*.2)])\n",
    "large_classification_train_data = GeneratedDataset(large_classification_problem[0][int(len(large_classification_problem[0])*.2):], large_classification_problem[1][int(len(large_classification_problem[1])*.2):])\n",
    "large_classification_test_data = GeneratedDataset(large_classification_problem[0][:int(len(large_classification_problem[0])*.2)], large_classification_problem[1][:int(len(large_classification_problem[1])*.2)])\n",
    "small_train_loader = DataLoader(small_classification_train_data, batch_size=10, shuffle=True)\n",
    "small_test_loader = DataLoader(small_classification_test_data, batch_size=10, shuffle=True)\n",
    "medium_train_loader = DataLoader(medium_classification_train_data, batch_size=10)\n",
    "medium_test_loader = DataLoader(medium_classification_test_data, batch_size=10)\n",
    "large_train_loader = DataLoader(large_classification_train_data, batch_size=10)\n",
    "large_test_loader = DataLoader(large_classification_test_data, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class MediumClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class LargeClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = SmallClassifyNetwork().to(device)\n",
    "medium_model = MediumClassifyNetwork().to(device)\n",
    "large_model = LargeClassifyNetwork().to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(large_model.parameters(), lr=1e-2)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.unsqueeze(1)).item()\n",
    "            pred = (pred > 0.5).type(torch.float)\n",
    "            correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.401364 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.389523 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.382163 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.375246 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.367895 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.358071 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.352950 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.341708 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.339110 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.326802 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(large_train_loader, large_model, loss_fn, optimizer)\n",
    "    test(large_test_loader, large_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model_pre = copy.deepcopy(large_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model_post = copy.deepcopy(large_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[ 0.0379, -0.0064, -0.0266,  ..., -0.0334,  0.0407,  0.0027],\n",
       "                      [ 0.0253,  0.0159, -0.0326,  ...,  0.0147,  0.0414, -0.0191],\n",
       "                      [ 0.0030, -0.0338, -0.0050,  ..., -0.0092, -0.0066, -0.0229],\n",
       "                      ...,\n",
       "                      [-0.0281,  0.0161,  0.0443,  ...,  0.0396,  0.0011,  0.0139],\n",
       "                      [ 0.0197,  0.0180, -0.0068,  ..., -0.0189, -0.0062, -0.0197],\n",
       "                      [-0.0294,  0.0242, -0.0334,  ...,  0.0082, -0.0394,  0.0273]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 0.0158, -0.0231,  0.0004, -0.0303, -0.0023,  0.0165,  0.0293, -0.0235,\n",
       "                       0.0132,  0.0101, -0.0253, -0.0303, -0.0115,  0.0231,  0.0201, -0.0059,\n",
       "                      -0.0267, -0.0158, -0.0380, -0.0224,  0.0093,  0.0437,  0.0047,  0.0325,\n",
       "                       0.0248,  0.0388,  0.0434, -0.0063, -0.0243,  0.0328,  0.0254,  0.0311,\n",
       "                       0.0387, -0.0088, -0.0230,  0.0398,  0.0317, -0.0020,  0.0272,  0.0096,\n",
       "                      -0.0102,  0.0126,  0.0327,  0.0154,  0.0146, -0.0366, -0.0229, -0.0327,\n",
       "                       0.0389, -0.0421, -0.0247,  0.0332,  0.0330, -0.0349,  0.0384, -0.0200,\n",
       "                      -0.0263, -0.0080, -0.0054, -0.0363,  0.0050, -0.0075, -0.0067,  0.0424,\n",
       "                       0.0247, -0.0165,  0.0001, -0.0203,  0.0063,  0.0276,  0.0062, -0.0244,\n",
       "                      -0.0117,  0.0291,  0.0371,  0.0229, -0.0079, -0.0282,  0.0028,  0.0442,\n",
       "                       0.0213, -0.0354, -0.0395, -0.0269,  0.0031, -0.0076,  0.0124,  0.0238,\n",
       "                       0.0338,  0.0259, -0.0285, -0.0140, -0.0104, -0.0127,  0.0202, -0.0275,\n",
       "                       0.0159, -0.0164, -0.0085,  0.0118,  0.0392,  0.0028, -0.0048,  0.0073,\n",
       "                      -0.0268, -0.0118, -0.0217, -0.0034,  0.0262, -0.0323,  0.0442, -0.0163,\n",
       "                      -0.0299, -0.0238, -0.0404,  0.0176,  0.0364, -0.0084,  0.0219, -0.0213,\n",
       "                      -0.0378, -0.0365, -0.0116,  0.0164,  0.0148, -0.0127, -0.0022, -0.0016,\n",
       "                      -0.0395,  0.0102,  0.0316,  0.0079,  0.0386, -0.0385, -0.0254, -0.0336,\n",
       "                       0.0008, -0.0416, -0.0240,  0.0365,  0.0102,  0.0246,  0.0161, -0.0398,\n",
       "                      -0.0164,  0.0442, -0.0023,  0.0028, -0.0075,  0.0020,  0.0326, -0.0184,\n",
       "                       0.0269, -0.0382, -0.0089,  0.0242,  0.0201, -0.0041,  0.0164,  0.0222,\n",
       "                      -0.0436,  0.0103,  0.0145, -0.0050,  0.0204, -0.0073,  0.0148,  0.0346,\n",
       "                       0.0163, -0.0442,  0.0019,  0.0024,  0.0162,  0.0421, -0.0267, -0.0352,\n",
       "                       0.0124,  0.0009, -0.0162,  0.0003,  0.0417, -0.0246, -0.0178, -0.0047,\n",
       "                      -0.0140, -0.0433, -0.0048,  0.0159,  0.0209,  0.0393, -0.0446, -0.0024,\n",
       "                       0.0054,  0.0381, -0.0428, -0.0128, -0.0209, -0.0368, -0.0302, -0.0244,\n",
       "                       0.0317, -0.0388, -0.0030, -0.0026, -0.0091, -0.0225, -0.0344,  0.0295,\n",
       "                       0.0074, -0.0266, -0.0083,  0.0214,  0.0420, -0.0251,  0.0410,  0.0114,\n",
       "                       0.0365, -0.0283, -0.0242, -0.0141, -0.0060, -0.0177, -0.0104, -0.0064,\n",
       "                      -0.0114,  0.0258, -0.0255,  0.0388,  0.0239,  0.0084,  0.0223, -0.0311,\n",
       "                       0.0212,  0.0243,  0.0329,  0.0092,  0.0050, -0.0325, -0.0272, -0.0337,\n",
       "                      -0.0257,  0.0152, -0.0054, -0.0432,  0.0300,  0.0194, -0.0275, -0.0419,\n",
       "                      -0.0016,  0.0218, -0.0284,  0.0128, -0.0187, -0.0262,  0.0337, -0.0150])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0121,  0.0506, -0.0604, -0.0379, -0.0547,  0.0221,  0.0266, -0.0583,\n",
       "                       -0.0622,  0.0349, -0.0441, -0.0434,  0.0528, -0.0096,  0.0375, -0.0151,\n",
       "                       -0.0049, -0.0171, -0.0466, -0.0164, -0.0182, -0.0166,  0.0459, -0.0481,\n",
       "                        0.0466,  0.0477,  0.0119, -0.0532,  0.0099,  0.0246, -0.0511,  0.0609,\n",
       "                       -0.0183,  0.0110,  0.0160, -0.0622, -0.0233,  0.0585,  0.0257, -0.0148,\n",
       "                       -0.0065, -0.0045, -0.0256, -0.0565,  0.0335,  0.0178, -0.0484,  0.0510,\n",
       "                       -0.0092, -0.0574,  0.0310, -0.0006,  0.0609, -0.0151, -0.0266,  0.0233,\n",
       "                        0.0545, -0.0181, -0.0134, -0.0231, -0.0185,  0.0477,  0.0241, -0.0399,\n",
       "                        0.0251, -0.0573,  0.0409,  0.0500,  0.0056, -0.0178,  0.0568, -0.0561,\n",
       "                        0.0262, -0.0211, -0.0034,  0.0174, -0.0149, -0.0313,  0.0393,  0.0618,\n",
       "                       -0.0078, -0.0146,  0.0070, -0.0590, -0.0322, -0.0602, -0.0205,  0.0211,\n",
       "                       -0.0392, -0.0259, -0.0231,  0.0439, -0.0390,  0.0204,  0.0503, -0.0331,\n",
       "                        0.0043, -0.0296, -0.0472, -0.0118,  0.0092, -0.0336,  0.0014,  0.0273,\n",
       "                        0.0410, -0.0624,  0.0430,  0.0574, -0.0307,  0.0530,  0.0170, -0.0237,\n",
       "                        0.0164, -0.0076, -0.0167,  0.0407,  0.0260,  0.0002,  0.0524, -0.0109,\n",
       "                       -0.0152, -0.0478,  0.0585, -0.0031, -0.0561, -0.0286,  0.0505,  0.0179,\n",
       "                       -0.0394,  0.0254, -0.0611, -0.0150,  0.0615, -0.0372, -0.0578, -0.0449,\n",
       "                       -0.0501,  0.0484,  0.0431, -0.0018,  0.0304, -0.0448,  0.0459, -0.0491,\n",
       "                        0.0047,  0.0037,  0.0584, -0.0436, -0.0156, -0.0149, -0.0459, -0.0584,\n",
       "                        0.0248, -0.0514, -0.0177,  0.0278, -0.0243, -0.0355, -0.0493, -0.0193,\n",
       "                       -0.0383, -0.0317,  0.0020,  0.0614, -0.0151, -0.0061,  0.0262, -0.0209,\n",
       "                        0.0381, -0.0462, -0.0434,  0.0079,  0.0281, -0.0107, -0.0394, -0.0356,\n",
       "                       -0.0077,  0.0520,  0.0206,  0.0382,  0.0557,  0.0311,  0.0338, -0.0521,\n",
       "                        0.0013, -0.0455, -0.0483, -0.0551,  0.0328, -0.0429, -0.0079, -0.0067,\n",
       "                       -0.0552,  0.0563, -0.0306,  0.0515,  0.0581, -0.0421, -0.0011,  0.0449,\n",
       "                       -0.0311, -0.0078,  0.0003,  0.0473, -0.0004,  0.0542, -0.0287, -0.0026,\n",
       "                        0.0288,  0.0189, -0.0183, -0.0586, -0.0448,  0.0541,  0.0544,  0.0394,\n",
       "                        0.0085, -0.0174,  0.0217,  0.0443, -0.0015, -0.0121,  0.0274, -0.0138,\n",
       "                        0.0373,  0.0382, -0.0497, -0.0219, -0.0276,  0.0161,  0.0560,  0.0476,\n",
       "                        0.0115,  0.0536, -0.0346,  0.0010, -0.0444, -0.0190, -0.0110,  0.0176,\n",
       "                       -0.0041,  0.0090,  0.0515, -0.0430, -0.0342,  0.0287,  0.0057,  0.0004,\n",
       "                        0.0552, -0.0142, -0.0099, -0.0100,  0.0348,  0.0510,  0.0292, -0.0206]])),\n",
       "             ('linear_relu_stack.2.bias', tensor([-0.0335]))])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[ 6.2502e-02, -1.1271e-02, -3.2048e-02,  ..., -3.0521e-02,\n",
       "                        5.2069e-02,  3.7101e-03],\n",
       "                      [ 1.7841e-02,  1.6986e-02, -4.0750e-02,  ...,  2.2018e-02,\n",
       "                        4.7628e-02, -1.3162e-02],\n",
       "                      [-1.2945e-02, -2.6445e-02, -6.4124e-03,  ..., -1.7109e-02,\n",
       "                       -8.6872e-03, -1.5072e-02],\n",
       "                      ...,\n",
       "                      [-4.8798e-02,  1.7136e-02,  3.7464e-02,  ...,  5.0496e-02,\n",
       "                       -3.5864e-02,  1.4923e-02],\n",
       "                      [ 2.6517e-02,  2.0910e-02, -8.5393e-04,  ..., -1.4728e-02,\n",
       "                        1.5843e-02, -1.4095e-02],\n",
       "                      [-2.4148e-02,  2.0616e-02, -3.3068e-02,  ...,  7.4630e-05,\n",
       "                       -7.4698e-02,  2.4198e-02]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 2.4380e-02, -1.7776e-02, -1.5122e-02, -3.0732e-02, -2.1950e-03,\n",
       "                       1.3525e-02,  3.7635e-02, -2.4309e-02,  6.1363e-03,  3.6629e-03,\n",
       "                      -4.0804e-02, -3.0592e-02, -5.3414e-03,  1.8825e-02, -2.8015e-02,\n",
       "                      -1.1000e-02, -2.3724e-02, -1.5454e-02, -3.7271e-02, -2.7861e-02,\n",
       "                       9.1158e-03,  4.1721e-02,  6.2970e-03,  4.2258e-02,  3.2284e-02,\n",
       "                       1.5969e-02,  3.2174e-02, -1.5861e-02, -2.1557e-02,  3.3179e-02,\n",
       "                       2.5584e-02,  3.5051e-02,  3.6765e-02, -4.6020e-03, -2.1574e-02,\n",
       "                       3.7423e-02,  4.6550e-02,  8.5139e-03,  1.8974e-02,  8.9235e-03,\n",
       "                      -4.0170e-03,  6.9762e-03,  3.1141e-02,  1.6069e-02,  1.4980e-02,\n",
       "                      -3.8377e-02, -1.5793e-02, -4.1630e-02,  4.1422e-02, -3.1399e-02,\n",
       "                      -2.5642e-02,  2.5655e-02,  5.0721e-02, -3.7662e-02,  3.7410e-02,\n",
       "                      -1.9781e-02, -2.8902e-02,  3.5391e-03, -9.9823e-03, -4.4693e-02,\n",
       "                       3.0895e-02,  7.8264e-03, -4.0997e-03,  4.5673e-02,  7.3269e-03,\n",
       "                      -1.6006e-02, -9.1950e-04, -4.1698e-02,  9.1680e-03,  3.1240e-02,\n",
       "                      -4.7488e-03, -2.3948e-02, -5.8539e-03,  3.0893e-02,  3.7098e-02,\n",
       "                       2.2668e-02, -9.1414e-03, -6.2427e-02, -4.2546e-05,  5.8752e-02,\n",
       "                       1.9936e-02, -3.9036e-02, -4.7568e-02, -2.7113e-02, -1.2481e-03,\n",
       "                      -1.1308e-02,  1.1086e-02,  1.6786e-02,  4.3497e-02,  3.5945e-02,\n",
       "                      -4.8425e-02, -1.6079e-02,  4.9328e-03, -1.2042e-02,  1.8707e-02,\n",
       "                      -3.4201e-02,  2.9645e-02, -9.3513e-03,  1.0620e-02,  9.8993e-03,\n",
       "                       3.4416e-02,  7.4084e-03,  5.8778e-03, -5.5381e-03, -1.0936e-02,\n",
       "                      -5.8933e-03, -2.2325e-02, -1.9458e-02,  3.0004e-02, -2.7212e-02,\n",
       "                       4.0111e-02, -3.6908e-02, -1.6355e-02, -2.8487e-02, -4.3577e-02,\n",
       "                       1.3747e-02,  2.4198e-02, -8.5180e-03,  2.4660e-02, -2.0042e-02,\n",
       "                      -4.3267e-02, -3.5524e-02, -4.3676e-02,  1.9033e-02,  5.3236e-02,\n",
       "                      -1.3812e-02,  2.8692e-02,  6.9853e-03, -1.4662e-02,  8.2818e-03,\n",
       "                       2.8639e-02,  1.5245e-02,  5.6223e-03, -1.7944e-02, -2.1923e-02,\n",
       "                      -1.8703e-03,  1.1145e-02, -3.3728e-02, -7.3757e-02,  3.1972e-02,\n",
       "                      -8.9496e-04,  3.1673e-02,  1.8256e-02, -4.3767e-02, -1.7427e-02,\n",
       "                       3.9709e-02, -1.1674e-03,  2.3001e-03, -1.2699e-02, -8.8130e-03,\n",
       "                       2.9489e-02, -1.6482e-03,  4.4897e-02, -2.3516e-02, -7.0068e-03,\n",
       "                       4.6377e-02,  2.4365e-02, -1.1809e-02,  9.8051e-03,  1.9998e-02,\n",
       "                      -3.8746e-02,  9.9647e-03,  2.8730e-03,  2.3838e-03,  2.5864e-02,\n",
       "                      -9.6432e-03,  1.8600e-02,  2.9254e-02,  1.7960e-02, -4.4929e-02,\n",
       "                       2.2198e-02,  2.0694e-03,  1.7131e-02,  4.1654e-02, -2.7750e-02,\n",
       "                      -4.9977e-02,  1.3151e-02, -1.0457e-02, -1.6133e-02, -7.3465e-03,\n",
       "                       5.4268e-02, -2.4400e-02, -2.7799e-02, -1.4186e-02,  2.0533e-04,\n",
       "                      -5.3284e-02,  6.6079e-03,  2.9577e-02,  1.8657e-02,  4.8314e-02,\n",
       "                      -3.2100e-02, -2.0706e-03,  2.0439e-02,  1.1133e-02, -2.9389e-02,\n",
       "                      -1.6218e-02,  8.9109e-03, -4.5424e-02, -2.6091e-02, -2.6128e-02,\n",
       "                       6.6225e-02, -2.6661e-02,  2.5007e-03, -6.5797e-03, -1.3297e-02,\n",
       "                      -2.3065e-02, -4.6432e-02,  3.1048e-02,  7.1862e-03, -2.9060e-02,\n",
       "                      -7.1803e-03,  2.4357e-02,  4.7248e-02, -1.9725e-02,  5.0634e-02,\n",
       "                       6.9143e-03,  3.4933e-02, -1.7794e-02, -3.1615e-02, -1.1256e-02,\n",
       "                      -8.9655e-03, -1.9201e-02, -2.0686e-02, -1.3980e-02, -5.2296e-02,\n",
       "                       3.0975e-02, -2.0642e-02,  3.8079e-02,  3.2606e-02,  6.8320e-03,\n",
       "                       1.8090e-02, -2.8866e-02,  1.9945e-02,  3.4367e-02,  4.4016e-02,\n",
       "                       1.3686e-02,  2.6355e-02, -2.4099e-02, -2.2594e-02, -3.3149e-02,\n",
       "                      -3.4911e-02,  2.5001e-02,  1.4749e-03, -4.0916e-02,  2.2256e-02,\n",
       "                       1.7811e-02, -3.1742e-02, -4.3001e-02,  2.0224e-03,  2.1649e-02,\n",
       "                      -2.4229e-02,  1.3243e-02, -2.6775e-02, -4.2525e-02,  4.5682e-02,\n",
       "                      -2.2573e-02])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.2712,  0.4214, -0.4298, -0.1775, -0.3475,  0.1853,  0.3379, -0.3926,\n",
       "                       -0.5949,  0.2976, -0.3665, -0.1844,  0.5255, -0.3375,  0.6003,  0.3765,\n",
       "                       -0.2804,  0.1481, -0.0623, -0.2627, -0.0038,  0.1918,  0.1139, -0.4274,\n",
       "                        0.4714,  0.4593,  0.3144, -0.4012,  0.3523, -0.0958, -0.2656,  0.2939,\n",
       "                       -0.1356, -0.3701, -0.1351, -0.2366, -0.3881,  0.5702,  0.2987,  0.1233,\n",
       "                       -0.2635,  0.2433,  0.1104, -0.0719,  0.1085, -0.2942, -0.3041,  0.3997,\n",
       "                       -0.1609, -0.4222,  0.2588, -0.3117,  0.5830, -0.3806,  0.1093,  0.3483,\n",
       "                        0.0484,  0.3049,  0.3124, -0.3432, -0.4046,  0.4823,  0.3223, -0.2519,\n",
       "                       -0.3390, -0.3003,  0.3530,  0.5207,  0.2527, -0.2435,  0.4090, -0.2437,\n",
       "                        0.3270, -0.1803, -0.1379,  0.0125,  0.2426, -0.5526,  0.0880,  0.5444,\n",
       "                       -0.1518, -0.0696,  0.3322, -0.2410, -0.3377, -0.3826, -0.0527,  0.3648,\n",
       "                       -0.4230, -0.2510, -0.5749,  0.2193, -0.2806, -0.0553,  0.3783, -0.2410,\n",
       "                       -0.2501, -0.2745, -0.3416,  0.3363,  0.2816, -0.2951, -0.2828,  0.4261,\n",
       "                        0.3656, -0.3003,  0.0697,  0.3739, -0.2389,  0.3800,  0.1592,  0.2942,\n",
       "                       -0.2963,  0.2044, -0.3272,  0.2089,  0.2415,  0.0498,  0.3323, -0.2841,\n",
       "                       -0.1972,  0.3332,  0.4981, -0.2529, -0.9147, -0.2334,  0.5721, -0.3589,\n",
       "                       -0.4438,  0.0722, -0.3645, -0.3237,  0.6865, -0.5428, -0.3274, -0.5819,\n",
       "                       -0.3181,  0.3844,  0.6040, -0.3719,  0.3191, -0.4410,  0.2898, -0.2245,\n",
       "                       -0.1148,  0.1541,  0.3809, -0.2851, -0.2063, -0.4053, -0.3460, -0.3940,\n",
       "                        0.4733, -0.4059, -0.0807,  0.5235, -0.1949, -0.4241, -0.3084,  0.2170,\n",
       "                       -0.2848,  0.0214,  0.3844,  0.3266, -0.2617, -0.2127,  0.2425, -0.2809,\n",
       "                       -0.1714, -0.2002, -0.5534,  0.0269,  0.2954,  0.0888, -0.1941, -0.3494,\n",
       "                       -0.0618,  0.3313,  0.0388,  0.3454,  0.5036,  0.1852,  0.3405, -0.3467,\n",
       "                       -0.3919, -0.3050, -0.3797, -0.2072,  0.2332, -0.3357, -0.4095, -0.0091,\n",
       "                       -0.3982,  0.4742, -0.3252,  0.3305,  0.5637, -0.5615, -0.0781,  0.3270,\n",
       "                       -0.4454, -0.2638, -0.1631,  0.3156,  0.1671,  0.2486, -0.3230, -0.0702,\n",
       "                        0.1040, -0.1218,  0.1274, -0.2561, -0.2504,  0.3918,  0.5351,  0.4187,\n",
       "                       -0.2523, -0.3599,  0.3967,  0.5575, -0.1899,  0.1417,  0.3835, -0.3604,\n",
       "                        0.5633,  0.3084, -0.4495, -0.1249, -0.3552,  0.1786,  0.4154,  0.3048,\n",
       "                        0.2541,  0.4043, -0.3109,  0.2744, -0.3786, -0.2307, -0.3327,  0.3072,\n",
       "                        0.3494,  0.3712,  0.4301, -0.2177, -0.2327,  0.5928, -0.2171,  0.2303,\n",
       "                        0.5146,  0.0382, -0.2210, -0.1638,  0.2215,  0.4584,  0.3914, -0.3444]])),\n",
       "             ('linear_relu_stack.2.bias', tensor([-0.0003]))])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre: \n",
      " Layer 1: -3.1782398e-05, 0.025849877\n",
      " Layer 2: -0.0011988243, 0.03633489\n",
      "Post: \n",
      " Layer 1: -2.958553e-05, 0.032174457\n",
      " Layer 2: -0.0013781609, 0.33564273\n"
     ]
    }
   ],
   "source": [
    "mean_pre_0 = np.mean(large_model_pre[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_post_0 = np.mean(large_model_post[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_pre_0 = np.std(large_model_pre[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_post_0 = np.std(large_model_post[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_pre_1 = np.mean(large_model_pre[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_post_1 = np.mean(large_model_post[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_pre_1 = np.std(large_model_pre[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_post_1 = np.std(large_model_post[\"linear_relu_stack.2.weight\"].numpy())\n",
    "print(\"Pre: \")\n",
    "print(\" Layer 1: \" + str(mean_pre_0) + \", \" + str(std_pre_0))\n",
    "print(\" Layer 2: \" + str(mean_pre_1) + \", \" + str(std_pre_1))\n",
    "print(\"Post: \")\n",
    "print(\" Layer 1: \" + str(mean_post_0) + \", \" + str(std_post_0))\n",
    "print(\" Layer 2: \" + str(mean_post_1) + \", \" + str(std_post_1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
