{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmer: Jacob Maurer\n",
    "date: 9/21/2024\n",
    "Description: This file is meant to make models that are trained on data other than the MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_regression, make_multilabel_classification\n",
    "import copy\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these experiments, we are going to assume about half of the features are informative, with no redundant or repeated features. The samples are going to be (5*number of features) * 10. This is going to be constant across regression and classification. The final sets will be balanced, seperate experiments will be run with unbalanced sets. The batch size will be 10.The split between train and test sets will be 80% training and 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_classification_problem = make_classification(n_samples = 5000, n_features=100, n_informative=50)\n",
    "medium_classification_problem = make_classification(n_samples = 15000, n_features=300, n_informative=150)\n",
    "large_classification_problem = make_classification(n_samples = 25000, n_features=500, n_informative = 250)\n",
    "small_regression_problem = make_regression(n_samples = 5000, n_features=100, n_informative=10)\n",
    "medium_regression_problem = make_regression(n_samples = 15000, n_features=300, n_informative=25)\n",
    "large_regression_problem = make_regression(n_samples = 25000, n_features=500, n_informative = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_c = pd.DataFrame(small_classification_problem[0])\n",
    "small_c['100'] = small_classification_problem[1]\n",
    "medium_c = pd.DataFrame(medium_classification_problem[0])\n",
    "medium_c['300'] = medium_classification_problem[1]\n",
    "large_c = pd.DataFrame(large_classification_problem[0])\n",
    "large_c['500'] = large_classification_problem[1]\n",
    "small_r = pd.DataFrame(small_regression_problem[0])\n",
    "small_r['100'] = small_regression_problem[1]\n",
    "medium_r = pd.DataFrame(medium_regression_problem[0])\n",
    "medium_r['300'] = medium_regression_problem[1]\n",
    "large_r = pd.DataFrame(large_regression_problem[0])\n",
    "large_r['500'] = large_regression_problem[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x = copy.deepcopy(x_data)\n",
    "        self.y = copy.deepcopy(y_data)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], np.float32(self.y[index])\n",
    "\n",
    "small_classification_train_data = GeneratedDataset(small_classification_problem[0][int(len(small_classification_problem[0])*.2):], small_classification_problem[1][int(len(small_classification_problem[1])*.2):])\n",
    "small_classification_test_data = GeneratedDataset(small_classification_problem[0][:int(len(small_classification_problem[0])*.2)], small_classification_problem[1][:int(len(small_classification_problem[1])*.2)])\n",
    "medium_classification_train_data = GeneratedDataset(medium_classification_problem[0][int(len(medium_classification_problem[0])*.2):], medium_classification_problem[1][int(len(medium_classification_problem[1])*.2):])\n",
    "medium_classification_test_data = GeneratedDataset(medium_classification_problem[0][:int(len(medium_classification_problem[0])*.2)], medium_classification_problem[1][:int(len(medium_classification_problem[1])*.2)])\n",
    "large_classification_train_data = GeneratedDataset(large_classification_problem[0][int(len(large_classification_problem[0])*.2):], large_classification_problem[1][int(len(large_classification_problem[1])*.2):])\n",
    "large_classification_test_data = GeneratedDataset(large_classification_problem[0][:int(len(large_classification_problem[0])*.2)], large_classification_problem[1][:int(len(large_classification_problem[1])*.2)])\n",
    "small_train_loader_class = DataLoader(small_classification_train_data, batch_size=100, shuffle=True)\n",
    "small_test_loader_class = DataLoader(small_classification_test_data, batch_size=100, shuffle=True)\n",
    "medium_train_loader_class = DataLoader(medium_classification_train_data, batch_size=100, shuffle=True)\n",
    "medium_test_loader_class = DataLoader(medium_classification_test_data, batch_size=100, shuffle=True)\n",
    "large_train_loader_class = DataLoader(large_classification_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_class = DataLoader(large_classification_test_data, batch_size=100, shuffle=True)\n",
    "small_regression_train_data = GeneratedDataset(small_regression_problem[0][int(len(small_regression_problem[0])*.2):], small_regression_problem[1][int(len(small_regression_problem[1])*.2):])\n",
    "small_regression_test_data = GeneratedDataset(small_regression_problem[0][:int(len(small_regression_problem[0])*.2)], small_regression_problem[1][:int(len(small_regression_problem[1])*.2)])\n",
    "medium_regression_train_data = GeneratedDataset(medium_regression_problem[0][int(len(medium_regression_problem[0])*.2):], medium_regression_problem[1][int(len(medium_regression_problem[1])*.2):])\n",
    "medium_regression_test_data = GeneratedDataset(medium_regression_problem[0][:int(len(medium_regression_problem[0])*.2)], medium_regression_problem[1][:int(len(medium_regression_problem[1])*.2)])\n",
    "large_regression_train_data = GeneratedDataset(large_regression_problem[0][int(len(large_regression_problem[0])*.2):], large_regression_problem[1][int(len(large_regression_problem[1])*.2):])\n",
    "large_regression_test_data = GeneratedDataset(large_regression_problem[0][:int(len(large_regression_problem[0])*.2)], large_regression_problem[1][:int(len(large_regression_problem[1])*.2)])\n",
    "small_train_loader_regress = DataLoader(small_regression_train_data, batch_size=100, shuffle=True)\n",
    "small_test_loader_regress = DataLoader(small_regression_test_data, batch_size=100, shuffle=True)\n",
    "medium_train_loader_regress = DataLoader(medium_regression_train_data, batch_size=100, shuffle=True)\n",
    "medium_test_loader_regress = DataLoader(medium_regression_test_data, batch_size=100, shuffle=True)\n",
    "large_train_loader_regress = DataLoader(large_regression_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_regress = DataLoader(large_regression_test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class MediumClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class LargeClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class SmallRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class MediumRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class LargeRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_class = SmallClassifyNetwork().to(device)\n",
    "medium_model_class = MediumClassifyNetwork().to(device)\n",
    "large_model_class = LargeClassifyNetwork().to(device)\n",
    "small_model_regress = SmallRegressNetwork().to(device)\n",
    "medium_model_regress = MediumRegressNetwork().to(device)\n",
    "large_model_regress = LargeRegressNetwork().to(device)\n",
    "\n",
    "loss_fn_class = nn.BCEWithLogitsLoss()\n",
    "optimizer_class = torch.optim.SGD(small_model_class.parameters(), lr=1e-2)\n",
    "loss_fn_regress = nn.MSELoss()\n",
    "optimizer_regress = torch.optim.SGD(small_model_regress.parameters(), lr=1e-4)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.unsqueeze(1)).item()\n",
    "            #pred = (pred > 0.5).type(torch.float)\n",
    "            #correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    #correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 44557.512500 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 892.628577 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 743.812576 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 657.452203 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 512.072913 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 388.682809 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 263.013966 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 163.803107 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 96.340849 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 51.957639 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31.616539 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 23.664985 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 19.338184 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 17.289448 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 16.321690 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 15.583721 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 15.169406 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14.546616 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14.565438 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14.302066 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14.717782 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 13.728172 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14.150225 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 13.409641 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 13.890610 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 12.882066 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 13.103653 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 12.834488 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 13.189876 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 12.899744 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(small_train_loader_regress, small_model_regress, loss_fn_regress, optimizer_regress)\n",
    "    test(small_test_loader_regress, small_model_regress, loss_fn_regress)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallRegressNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ELU(alpha=1.0)\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_pre = copy.deepcopy(small_model_regress.cpu().state_dict())\n",
    "small_model_regress.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallRegressNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_post = copy.deepcopy(small_model_regress.cpu().state_dict())\n",
    "small_model_regress.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-7.9595e-02,  5.8272e-02,  7.2560e-02,  ..., -6.0981e-02,\n",
       "                       -1.9675e-02, -9.0082e-02],\n",
       "                      [-3.8936e-02, -2.9895e-02,  3.4903e-02,  ...,  4.5432e-02,\n",
       "                        9.9350e-02, -6.1892e-02],\n",
       "                      [-1.1030e-02,  7.1404e-02, -9.3691e-02,  ..., -4.1968e-02,\n",
       "                       -2.9137e-02, -8.4280e-02],\n",
       "                      ...,\n",
       "                      [ 3.2532e-03, -9.9735e-02,  8.2803e-02,  ...,  8.7258e-02,\n",
       "                       -8.9050e-02, -4.3630e-02],\n",
       "                      [-9.7627e-02,  5.8631e-02, -4.1444e-03,  ...,  7.0659e-02,\n",
       "                        6.5113e-02,  6.1236e-05],\n",
       "                      [-1.2081e-02, -9.3383e-02,  6.1701e-03,  ..., -3.5818e-02,\n",
       "                       -1.3414e-02,  9.7158e-02]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 0.0823,  0.0747,  0.0087,  0.0573, -0.0433, -0.0587,  0.0157,  0.0590,\n",
       "                       0.0118,  0.0674,  0.0452, -0.0149,  0.0539,  0.0263, -0.0019,  0.0472,\n",
       "                      -0.0046, -0.0930,  0.0334,  0.0583, -0.0262, -0.0221, -0.0692, -0.0118,\n",
       "                       0.0516,  0.0676, -0.0797, -0.0733,  0.0956, -0.0745,  0.0034,  0.0559,\n",
       "                      -0.0682,  0.0632,  0.0818,  0.0698, -0.0302, -0.0289, -0.0656, -0.0641,\n",
       "                       0.0260,  0.0702, -0.0091,  0.0253,  0.0912,  0.0139,  0.0068, -0.0942,\n",
       "                      -0.0873,  0.0073,  0.0717,  0.0660,  0.0273, -0.0508, -0.0570, -0.0451,\n",
       "                      -0.0601, -0.0465,  0.0712, -0.0475,  0.0377, -0.0261, -0.0991,  0.0697,\n",
       "                      -0.0111,  0.0954,  0.0222, -0.0223, -0.0468, -0.0732, -0.0805,  0.0322,\n",
       "                       0.0667, -0.0822,  0.0508, -0.0166,  0.0867,  0.0488, -0.0160,  0.0970,\n",
       "                       0.0131,  0.0784,  0.0427, -0.0389, -0.0755,  0.0304, -0.0700,  0.0074,\n",
       "                      -0.0248, -0.0799,  0.0754, -0.0314,  0.0438, -0.0164, -0.0710,  0.0085,\n",
       "                      -0.0691, -0.0674, -0.0295,  0.0706,  0.0653, -0.0337, -0.0271, -0.0767,\n",
       "                      -0.0643,  0.0241,  0.0590,  0.0926,  0.0030,  0.0027,  0.0118,  0.0723,\n",
       "                       0.0253, -0.0824,  0.0333,  0.0902,  0.0010, -0.0084,  0.0677, -0.0107,\n",
       "                       0.0434,  0.0896, -0.0824, -0.0832,  0.0610,  0.0814,  0.0337, -0.0901])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 4.5229e-02,  3.9507e-02,  7.1674e-02,  ...,  7.9664e-03,\n",
       "                       -7.5769e-02,  4.4058e-02],\n",
       "                      [ 6.9402e-02, -1.9758e-02, -4.8199e-02,  ...,  2.7098e-02,\n",
       "                        4.2962e-02,  5.6277e-02],\n",
       "                      [ 3.2634e-02, -8.5260e-02,  3.9615e-02,  ...,  1.8460e-02,\n",
       "                       -8.6565e-02, -2.3537e-02],\n",
       "                      ...,\n",
       "                      [-2.1306e-02, -2.9176e-02, -8.2049e-02,  ...,  6.3201e-02,\n",
       "                       -2.3081e-03, -5.9817e-02],\n",
       "                      [ 3.4558e-02, -3.9344e-02, -6.2141e-02,  ...,  7.3582e-02,\n",
       "                        1.0203e-02, -7.6056e-03],\n",
       "                      [-8.4326e-05,  3.6510e-02, -5.9506e-02,  ..., -4.8147e-02,\n",
       "                       -2.7557e-02, -5.5781e-03]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([-0.0036, -0.0790, -0.0367, -0.0622,  0.0339, -0.0357, -0.0006,  0.0682,\n",
       "                      -0.0569,  0.0589, -0.0152, -0.0613,  0.0086,  0.0473,  0.0194, -0.0187,\n",
       "                       0.0530, -0.0162,  0.0013,  0.0399, -0.0766,  0.0702, -0.0376,  0.0378,\n",
       "                       0.0359,  0.0212, -0.0381, -0.0192, -0.0505,  0.0627,  0.0529,  0.0018,\n",
       "                       0.0250,  0.0412,  0.0308, -0.0079, -0.0318,  0.0080,  0.0597, -0.0526,\n",
       "                      -0.0874,  0.0688, -0.0118, -0.0010,  0.0684, -0.0223,  0.0117, -0.0427,\n",
       "                      -0.0297,  0.0755, -0.0631,  0.0153, -0.0572, -0.0264,  0.0158, -0.0272,\n",
       "                      -0.0363,  0.0073, -0.0036,  0.0721,  0.0034,  0.0460, -0.0197, -0.0345,\n",
       "                      -0.0221, -0.0705, -0.0576,  0.0529, -0.0650, -0.0223, -0.0455,  0.0114,\n",
       "                      -0.0039,  0.0326,  0.0516,  0.0704,  0.0164, -0.0393, -0.0608, -0.0018,\n",
       "                       0.0098, -0.0470, -0.0240, -0.0040, -0.0015,  0.0373, -0.0099,  0.0026,\n",
       "                       0.0481, -0.0520,  0.0164, -0.0447,  0.0264, -0.0381,  0.0089,  0.0737,\n",
       "                      -0.0241,  0.0415,  0.0028,  0.0513,  0.0808,  0.0788,  0.0194,  0.0005,\n",
       "                       0.0539,  0.0656, -0.0685,  0.0536, -0.0494, -0.0591, -0.0221,  0.0498,\n",
       "                      -0.0011, -0.0181, -0.0202, -0.0483,  0.0031, -0.0502, -0.0041,  0.0505,\n",
       "                      -0.0791,  0.0819,  0.0304, -0.0163,  0.0393,  0.0431,  0.0383, -0.0220])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[-0.0272, -0.0216, -0.0015, -0.0314,  0.0538, -0.0043,  0.0766, -0.0571,\n",
       "                        0.0524, -0.0038,  0.0858,  0.0176,  0.0643,  0.0328, -0.0083,  0.0300,\n",
       "                       -0.0869,  0.0491, -0.0074, -0.0696, -0.0120, -0.0698,  0.0751,  0.0529,\n",
       "                        0.0485, -0.0261, -0.0110, -0.0062,  0.0021, -0.0687, -0.0169,  0.0182,\n",
       "                        0.0800,  0.0781,  0.0515,  0.0556, -0.0172, -0.0342, -0.0719,  0.0066,\n",
       "                        0.0077,  0.0441, -0.0672,  0.0096, -0.0595, -0.0428,  0.0064, -0.0819,\n",
       "                       -0.0252,  0.0334,  0.0645, -0.0151, -0.0551, -0.0261, -0.0452, -0.0641,\n",
       "                        0.0492, -0.0548,  0.0280, -0.0019,  0.0633, -0.0080, -0.0339,  0.0205,\n",
       "                       -0.0094,  0.0308, -0.0425, -0.0782,  0.0178,  0.0814,  0.0176,  0.0092,\n",
       "                       -0.0457,  0.0164,  0.0103,  0.0369,  0.0589, -0.0259, -0.0743, -0.0648,\n",
       "                       -0.0856,  0.0379, -0.0144,  0.0709,  0.0686, -0.0489,  0.0838, -0.0799,\n",
       "                       -0.0482, -0.0725, -0.0533,  0.0786, -0.0754, -0.0141,  0.0084,  0.0771,\n",
       "                        0.0057, -0.0481, -0.0626,  0.0403,  0.0096, -0.0004, -0.0628,  0.0815,\n",
       "                        0.0125,  0.0594,  0.0324, -0.0320, -0.0853,  0.0386,  0.0581, -0.0057,\n",
       "                       -0.0264, -0.0446, -0.0863, -0.0201,  0.0789,  0.0649,  0.0254,  0.0279,\n",
       "                        0.0429, -0.0807, -0.0804, -0.0502,  0.0122, -0.0188, -0.0667, -0.0511]])),\n",
       "             ('linear_relu_stack.4.bias', tensor([0.0330]))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0882, -0.1087, -0.0192,  ...,  0.1149,  0.0705,  0.1071],\n",
       "                      [ 0.1097,  0.0804, -0.0648,  ..., -0.1066, -0.0808, -0.0350],\n",
       "                      [-0.0583, -0.0333,  0.0060,  ..., -0.0331,  0.0008,  0.0220],\n",
       "                      ...,\n",
       "                      [ 0.0852,  0.0633, -0.0670,  ..., -0.0106,  0.0281, -0.0363],\n",
       "                      [ 0.0552,  0.0284,  0.0118,  ..., -0.0811,  0.1255, -0.0002],\n",
       "                      [ 0.1279, -0.1360, -0.0291,  ..., -0.0783, -0.0723, -0.0949]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 2.9903e-02,  1.1134e-01,  2.8027e-01, -2.9181e-02,  1.5677e-01,\n",
       "                       8.2016e-02,  1.5043e-01,  7.1861e-02, -3.1535e-02,  2.5788e-01,\n",
       "                      -3.4228e-04,  5.2460e-02,  9.2920e-03,  1.8459e-01, -1.5138e-01,\n",
       "                       2.1761e-01, -2.7553e-02, -3.3128e-02,  1.3931e-01, -1.5141e-02,\n",
       "                       2.4357e-01,  8.0106e-02,  2.9439e-02,  1.7283e-01,  9.1038e-02,\n",
       "                       5.8317e-02,  4.5428e-02,  1.2338e-01,  1.5567e-01,  2.7128e-02,\n",
       "                       7.3345e-03,  1.9422e-01,  1.0787e-01,  4.3357e-02, -7.3959e-02,\n",
       "                      -7.8112e-03,  2.1647e-01,  4.7774e-02,  1.8308e-01,  3.3135e-03,\n",
       "                       1.3840e-01,  2.5645e-02,  1.6523e-01,  8.4806e-03, -3.1283e-02,\n",
       "                       1.7821e-01,  1.4778e-01,  1.9012e-01, -6.5052e-02,  1.4943e-01,\n",
       "                       1.0906e-01,  1.6463e-01,  9.1830e-02,  4.6461e-02,  2.3650e-01,\n",
       "                      -7.0604e-02,  1.5795e-01,  1.2226e-01,  2.2979e-02,  8.0370e-02,\n",
       "                       5.3111e-02, -2.0712e-01,  8.1242e-02, -2.2175e-02, -1.2755e-02,\n",
       "                       1.0132e-01, -3.7423e-02,  9.2799e-02,  2.4074e-01,  1.8954e-01,\n",
       "                      -5.1560e-03,  6.8947e-02, -8.3852e-03, -6.9749e-03,  2.0364e-01,\n",
       "                      -2.5666e-02,  9.4925e-02,  1.6035e-02,  1.1844e-01, -4.6809e-02,\n",
       "                      -1.0123e-01,  1.0644e-01,  2.3657e-01, -3.0052e-02,  2.7087e-01,\n",
       "                       1.9518e-01,  6.7375e-02,  5.5011e-02,  1.8513e-01,  1.2563e-01,\n",
       "                       6.8813e-02, -3.3240e-02, -3.3831e-02,  2.2610e-01,  1.5677e-01,\n",
       "                       1.9037e-01,  4.9300e-02,  1.1370e-01,  1.3968e-01, -6.5031e-02,\n",
       "                       2.8458e-02, -1.4245e-01, -5.7344e-02,  6.4957e-02,  1.6826e-01,\n",
       "                       1.2351e-01,  1.9458e-02,  1.4252e-01,  9.4429e-02,  3.0781e-01,\n",
       "                       2.3485e-01,  1.2106e-01,  2.3704e-01,  1.9416e-01,  7.8004e-02,\n",
       "                       1.1651e-01,  7.0858e-02, -4.7739e-05,  2.8292e-02, -2.8189e-02,\n",
       "                       2.8152e-02,  1.4953e-01, -6.1181e-02,  5.1362e-03, -3.1462e-02,\n",
       "                       4.2325e-02,  1.5654e-02,  1.3586e-01])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0803,  0.0084, -0.0278,  ..., -0.0025,  0.0748, -0.0454],\n",
       "                      [-0.0862, -0.0646,  0.0631,  ..., -0.0799,  0.0116,  0.0130],\n",
       "                      [ 0.0796,  0.0050,  0.0639,  ...,  0.0953, -0.0542,  0.1080],\n",
       "                      ...,\n",
       "                      [ 0.3087,  0.1227,  0.0937,  ...,  0.2257,  0.0628,  0.3281],\n",
       "                      [ 0.0029,  0.0838,  0.1044,  ...,  0.0297,  0.1317, -0.0496],\n",
       "                      [ 0.0379,  0.0612, -0.0734,  ..., -0.0217,  0.1052, -0.0881]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 0.0072, -0.0626,  0.0934, -0.0182,  0.0807,  0.0526,  0.0108,  0.2458,\n",
       "                       0.1342,  0.1129,  0.1567,  0.0142,  0.1161,  0.0138,  0.0206,  0.0157,\n",
       "                       0.2002,  0.0385,  0.0015, -0.0581,  0.1007,  0.1536,  0.0058,  0.2859,\n",
       "                      -0.0421,  0.0474,  0.0588,  0.0830,  0.0843, -0.0082,  0.0842, -0.0534,\n",
       "                      -0.0106,  0.2037,  0.0718,  0.0480,  0.0383,  0.1970,  0.2108, -0.0170,\n",
       "                       0.0889,  0.2422, -0.0197,  0.2248,  0.1381, -0.0235,  0.1035, -0.0768,\n",
       "                       0.0207, -0.0358,  0.0632,  0.3517,  0.0605,  0.1186, -0.0099, -0.0729,\n",
       "                       0.0900,  0.1095, -0.0200,  0.0755,  0.0239,  0.2029,  0.1248,  0.1201,\n",
       "                       0.1570,  0.0273, -0.0211,  0.2060,  0.0759,  0.1436,  0.2515, -0.0800,\n",
       "                      -0.0495,  0.0015,  0.0621, -0.0111,  0.0211,  0.0961,  0.0019, -0.0295,\n",
       "                       0.0222,  0.0441,  0.0570, -0.0716,  0.0787,  0.0622,  0.1870, -0.0504,\n",
       "                       0.0093, -0.0209, -0.0513,  0.0997,  0.0149,  0.2005, -0.0319, -0.0542,\n",
       "                      -0.0302, -0.0563,  0.1021, -0.0789,  0.1554,  0.0230,  0.0617,  0.2483,\n",
       "                       0.0467, -0.0787, -0.0376, -0.0209,  0.2552,  0.1537, -0.0561,  0.1233,\n",
       "                       0.0439,  0.0308,  0.0386,  0.0802,  0.0879,  0.4550, -0.0563,  0.1754,\n",
       "                       0.0912,  0.0933, -0.0507,  0.0126, -0.0230,  0.2524,  0.0721,  0.0767])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 4.1663e-02,  6.3308e-03,  1.4529e-01,  2.5921e-01,  1.0152e-01,\n",
       "                        6.3535e-02, -4.8336e-03, -1.6241e+00, -1.1284e+00,  5.1387e-01,\n",
       "                       -1.1452e+00, -7.4588e-01,  1.0188e+00, -9.4064e-03,  1.3692e-03,\n",
       "                       -6.5461e-01,  6.5147e-01, -9.2494e-01, -2.3832e-03,  2.1649e-01,\n",
       "                        6.0159e-01,  6.1540e-01, -1.9074e-01, -2.4383e+00, -4.4252e-02,\n",
       "                       -1.9644e-01, -1.3477e-02, -9.8313e-02, -1.5139e-01,  3.6155e-01,\n",
       "                       -2.0523e-01,  3.3342e-02,  3.1616e-01,  8.5987e-01, -5.1159e-01,\n",
       "                        4.4921e-02, -4.9851e-01,  1.3220e+00, -1.8089e+00,  4.2755e-01,\n",
       "                       -1.2888e+00,  1.2579e+00,  1.0036e-02,  1.3871e+00,  9.8962e-01,\n",
       "                       -1.1938e-02,  4.3046e-01,  5.7046e-02, -6.3348e-01,  8.9179e-02,\n",
       "                        3.3541e-02,  1.8365e+00, -2.0085e-02,  3.2118e-01,  5.1342e-02,\n",
       "                       -4.3104e-03, -8.1367e-01,  1.0107e+00, -9.4143e-02, -3.5149e-02,\n",
       "                       -4.5715e-01,  8.6351e-01, -9.9777e-01, -4.3336e-01, -1.4253e+00,\n",
       "                        4.1275e-01, -1.9170e-01, -1.3548e+00, -4.7599e-02, -7.9617e-01,\n",
       "                        1.1380e+00,  5.9953e-02, -1.8153e-01, -9.3402e-02,  3.1946e-01,\n",
       "                       -2.9904e-01,  4.7924e-01, -7.0270e-01, -2.3813e-02,  1.7030e-03,\n",
       "                       -6.8710e-02, -3.8296e-01, -1.3972e-01,  7.4249e-02,  8.9390e-01,\n",
       "                       -2.6051e-01,  1.5070e+00,  3.8583e-01, -2.3087e-01,  8.5657e-02,\n",
       "                       -7.1095e-02, -8.3642e-01, -2.6073e-01, -1.3426e+00,  1.3412e-01,\n",
       "                       -1.4730e-01, -2.1202e-01,  1.1066e-01, -1.3167e+00, -8.9315e-02,\n",
       "                        9.1529e-01, -9.1777e-01,  1.5820e-02,  1.6113e+00,  6.8029e-02,\n",
       "                       -1.5962e-01, -1.3101e-01,  1.4611e-01,  1.4925e+00,  1.1710e+00,\n",
       "                        2.7407e-01,  4.7741e-01, -1.2805e-01,  3.3071e-01, -1.1788e+00,\n",
       "                       -1.0655e+00, -3.1054e-01,  2.4618e+00, -2.6614e-02, -8.3693e-01,\n",
       "                       -6.0668e-01, -5.2472e-01,  8.6224e-02, -1.2088e-02, -2.6338e-02,\n",
       "                        1.7610e+00, -3.7577e-01, -1.3175e-01]])),\n",
       "             ('linear_relu_stack.4.bias', tensor([0.0579]))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'large_model_post' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mean_pre_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(large_model_pre[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m----> 2\u001b[0m mean_post_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mlarge_model_post\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      3\u001b[0m std_pre_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(large_model_pre[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      4\u001b[0m std_post_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(large_model_post[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'large_model_post' is not defined"
     ]
    }
   ],
   "source": [
    "mean_pre_0 = np.mean(large_model_pre[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_post_0 = np.mean(large_model_post[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_pre_0 = np.std(large_model_pre[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_post_0 = np.std(large_model_post[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_pre_1 = np.mean(large_model_pre[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_post_1 = np.mean(large_model_post[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_pre_1 = np.std(large_model_pre[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_post_1 = np.std(large_model_post[\"linear_relu_stack.2.weight\"].numpy())\n",
    "print(\"Pre: \")\n",
    "print(\" Layer 1: \" + str(mean_pre_0) + \", \" + str(std_pre_0))\n",
    "print(\" Layer 2: \" + str(mean_pre_1) + \", \" + str(std_pre_1))\n",
    "print(\"Post: \")\n",
    "print(\" Layer 1: \" + str(mean_post_0) + \", \" + str(std_post_0))\n",
    "print(\" Layer 2: \" + str(mean_post_1) + \", \" + str(std_post_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "for item in large_model_post:\n",
    "    if item[20:] == \"weight\":\n",
    "        table = pd.DataFrame(large_model_post[item])\n",
    "        table.to_csv(\"results/elu_elu_small_regress_weight_post_\" + str(ID) + \".csv\",index=False)\n",
    "    if item[20:] == \"bias\":\n",
    "        series = pd.Series(large_model_post[item])\n",
    "        series.to_csv(\"results/elu_elu_small_regress_bias_post_\" + str(ID) + \".csv\",index=False)\n",
    "    ID += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "for item in large_model_pre:\n",
    "    if item[20:] == \"weight\":\n",
    "        table = pd.DataFrame(large_model_pre[item])\n",
    "        table.to_csv(\"results/elu_elu_small_regress_weight_pre_\" + str(ID) + \".csv\",index=False)\n",
    "    if item[20:] == \"bias\":\n",
    "        series = pd.Series(large_model_pre[item])\n",
    "        series.to_csv(\"results/elu_elu_small_regress_bias_pre_\" + str(ID) + \".csv\",index=False)\n",
    "    ID += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the newer experiments, it seems that Binary Classification partialy follows what was found with MNIST, and Regression doesn't really follow at all. For regression this makes sense, as the network is trying to predict a value along all real numbers as oposed to some set of choices. For Binary classification, it has always been close, but not really solid. There were times where it did follow MNIST, and other times where it didn't follow at all. It could be possible the data in the images in MNIST are different than the generated sets, but i have no idea at this point on how to characterize the MNIST set to be similiar with sklearn's make_classification. Have not yet tested the make_multiclass_classification, and have not yet dived into the Heatmaps of weights for the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_multiclass_problem = make_multilabel_classification(n_samples=25000, n_features=500, n_classes=10, n_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_multiclass_train_data = GeneratedDataset(large_multiclass_problem[0][int(len(large_multiclass_problem[0])*.2):], large_multiclass_problem[1][int(len(large_multiclass_problem[1])*.2):])\n",
    "large_multiclass_test_data = GeneratedDataset(large_multiclass_problem[0][:int(len(large_multiclass_problem[0])*.2)], large_multiclass_problem[1][:int(len(large_multiclass_problem[1])*.2)])\n",
    "large_train_loader_multiclass = DataLoader(large_multiclass_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_multiclass = DataLoader(large_multiclass_test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeMulticlassNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "model = LargeMulticlassNetwork().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimize = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # pred = (pred > 0.5).type(torch.float)\n",
    "            # correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.441471 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.449458 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.431501 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.441189 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.432633 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.425189 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.433316 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.418330 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.416986 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.409183 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.402614 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.394229 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.364168 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.339444 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.247414 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.172838 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.307687 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.970508 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.900927 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.853474 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.873333 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.765273 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.689242 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.681034 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.615214 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.624922 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.587897 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.611373 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.570010 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.535191 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LargeMulticlassNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=512, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_multiclass = copy.deepcopy(model.cpu().state_dict())\n",
    "model.to(device)\n",
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(large_train_loader_multiclass, model, loss, optimize)\n",
    "    test(large_test_loader_multiclass, model, loss)\n",
    "print(\"Done!\")\n",
    "post_trained_multiclass = copy.deepcopy(model.cpu().state_dict())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0442, -0.0267, -0.0142,  ..., -0.0404,  0.0069, -0.0183],\n",
       "                      [-0.0080, -0.0214,  0.0314,  ...,  0.0024,  0.0401,  0.0208],\n",
       "                      [ 0.0328,  0.0305,  0.0125,  ..., -0.0054,  0.0332,  0.0372],\n",
       "                      ...,\n",
       "                      [ 0.0295,  0.0069,  0.0092,  ..., -0.0432,  0.0014,  0.0428],\n",
       "                      [-0.0401, -0.0193, -0.0308,  ...,  0.0003, -0.0158,  0.0322],\n",
       "                      [-0.0280,  0.0170,  0.0132,  ..., -0.0377, -0.0099, -0.0394]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 4.2919e-02, -1.5841e-02, -1.6990e-02,  3.6192e-02, -2.4385e-02,\n",
       "                       9.3218e-03, -3.9001e-02,  2.3250e-02,  9.7217e-03,  4.5774e-04,\n",
       "                       3.9148e-02,  1.4828e-02,  1.2344e-02,  3.8993e-02, -4.0246e-02,\n",
       "                      -1.2096e-03, -2.7187e-02, -3.4861e-02,  2.7642e-02,  4.5555e-03,\n",
       "                      -1.9925e-03, -1.5783e-02, -1.4978e-02, -4.4663e-02,  2.4437e-02,\n",
       "                      -1.5667e-02,  1.3683e-02, -2.8462e-02,  3.6868e-02,  2.3563e-02,\n",
       "                       1.1429e-02,  1.1516e-02,  2.6937e-02,  6.4324e-03, -2.5738e-02,\n",
       "                      -1.4432e-02,  8.3003e-04, -3.0492e-02,  4.3186e-03, -2.2549e-03,\n",
       "                      -3.2131e-02, -2.4438e-02, -1.0092e-02, -1.9646e-02, -3.7702e-02,\n",
       "                       4.0567e-02,  4.3226e-02, -2.5316e-02, -2.9232e-02,  2.6409e-02,\n",
       "                      -4.2559e-02,  4.0492e-02, -2.3125e-02, -2.2076e-02,  4.2645e-02,\n",
       "                      -3.0410e-02,  3.5413e-02, -7.3846e-04, -4.1316e-02, -1.2369e-02,\n",
       "                       5.4307e-03, -2.8943e-02,  4.1284e-02, -3.7023e-02,  2.2492e-02,\n",
       "                      -3.0297e-02,  4.4070e-02,  1.8783e-02,  3.5993e-02, -2.4614e-02,\n",
       "                       3.3787e-02, -1.1708e-02,  2.0215e-02, -9.5376e-03,  3.9534e-03,\n",
       "                      -3.6681e-02,  1.0901e-02, -2.6258e-02, -1.5801e-02,  1.7827e-02,\n",
       "                      -3.4527e-02,  2.8176e-02,  2.0573e-02,  3.9996e-02, -2.9532e-02,\n",
       "                       2.2322e-02, -1.3402e-02, -4.3952e-02,  2.1299e-02, -3.7516e-02,\n",
       "                      -8.4000e-03, -2.6488e-02,  2.2765e-02,  9.2786e-03, -3.9571e-02,\n",
       "                      -4.2792e-03,  1.8525e-02,  1.0290e-02, -1.0074e-02, -2.5316e-02,\n",
       "                       4.2359e-02,  1.3514e-02, -1.9487e-02,  1.7086e-02, -2.5341e-02,\n",
       "                       3.8779e-04,  2.9961e-02, -1.8712e-02,  1.7061e-02, -3.4872e-02,\n",
       "                       3.5349e-02,  4.1470e-02,  2.5135e-02, -1.8869e-02,  2.6922e-02,\n",
       "                       2.5502e-02,  1.7437e-02, -4.3251e-02,  3.1088e-02,  3.6938e-02,\n",
       "                      -9.2920e-03, -6.3694e-03, -3.9495e-02,  6.7332e-04,  1.6638e-02,\n",
       "                      -2.8597e-02,  5.7137e-04, -3.2410e-02, -1.9607e-02, -3.0811e-02,\n",
       "                       3.1279e-03,  1.7080e-02,  1.9491e-03, -2.4371e-02, -3.4631e-02,\n",
       "                      -3.0162e-02,  3.7304e-02,  2.7698e-02, -2.9097e-02,  2.7587e-02,\n",
       "                       1.9266e-02,  3.5160e-02,  4.3132e-02,  9.7055e-03, -3.3284e-02,\n",
       "                       2.5808e-02,  4.2663e-02,  5.3936e-03,  3.0570e-02, -3.2827e-02,\n",
       "                       1.7493e-02, -1.1049e-02,  3.8063e-02,  2.1314e-02, -3.5826e-03,\n",
       "                       2.7030e-02, -4.0066e-03, -1.8182e-03, -2.3191e-02,  5.1671e-03,\n",
       "                       3.9179e-02,  1.4990e-02,  2.5045e-02, -3.2122e-02,  3.0938e-02,\n",
       "                      -1.7865e-02,  1.7510e-02, -3.0486e-02, -2.8647e-02, -3.2022e-02,\n",
       "                      -3.3439e-02, -3.5878e-02,  2.1962e-02, -3.8739e-02, -4.0597e-02,\n",
       "                       7.1588e-03,  2.0890e-02,  9.7208e-03,  7.3897e-03, -1.5755e-02,\n",
       "                       1.6791e-02, -4.0454e-02,  3.1809e-02,  4.0667e-02,  2.1450e-03,\n",
       "                      -1.6465e-02,  6.6190e-03,  6.9814e-03, -5.7527e-03, -4.2159e-02,\n",
       "                      -3.9608e-02, -2.1316e-02, -4.1696e-02,  1.1263e-02,  3.0121e-02,\n",
       "                       1.1055e-02, -5.3364e-04,  4.2215e-02,  2.7819e-02,  3.6696e-02,\n",
       "                      -3.4681e-03, -3.1591e-03,  3.9385e-02,  4.0099e-02,  1.9089e-02,\n",
       "                      -2.3736e-02, -1.9680e-03,  1.5087e-02,  3.1345e-02,  7.4309e-03,\n",
       "                      -1.2527e-02,  4.8264e-03,  3.0957e-02,  4.0005e-02, -3.4140e-02,\n",
       "                       1.3837e-02,  2.1858e-02, -6.7481e-03,  3.6432e-02,  3.6533e-03,\n",
       "                      -2.6390e-02, -1.6164e-02, -4.4794e-03,  3.4618e-02,  3.3310e-03,\n",
       "                       4.4632e-02,  1.4539e-02,  2.4907e-02, -2.0921e-02, -8.1949e-03,\n",
       "                      -2.2362e-03, -2.6006e-02, -1.0870e-02,  2.9540e-02, -3.7819e-02,\n",
       "                       2.2243e-02,  3.3581e-02, -3.1793e-02,  3.4690e-02, -5.5975e-03,\n",
       "                      -2.1055e-02,  1.4616e-02,  2.2538e-02,  7.7122e-03,  1.5132e-04,\n",
       "                       1.7453e-02, -3.1845e-03, -1.4725e-02, -7.0839e-03, -7.7325e-04,\n",
       "                      -2.0728e-02,  5.0794e-03, -3.6899e-02,  1.1463e-02, -2.1333e-03,\n",
       "                      -1.9512e-02, -2.8665e-02,  3.6995e-02,  7.1451e-03, -1.6025e-02,\n",
       "                       2.9667e-02, -9.0934e-03, -3.0711e-02,  2.6737e-02,  3.1398e-03,\n",
       "                      -1.2134e-02, -1.7778e-02, -2.9033e-02,  2.6522e-02, -1.8482e-02,\n",
       "                       6.3252e-03,  4.7878e-03, -3.9361e-02,  2.1052e-02, -1.0538e-03,\n",
       "                       4.7135e-04,  9.2799e-03,  3.3807e-03,  1.2948e-02,  2.4776e-02,\n",
       "                      -3.2196e-02, -4.4745e-03,  4.0036e-02, -3.2269e-02,  8.4117e-03,\n",
       "                       3.7415e-02,  2.6734e-02, -3.6336e-02,  5.1802e-03,  3.6500e-02,\n",
       "                       1.5014e-02, -1.1141e-02, -1.2126e-02,  4.1024e-02, -9.1174e-03,\n",
       "                       1.0423e-02,  2.6420e-02,  2.9675e-02,  3.6432e-02, -2.7257e-02,\n",
       "                       2.4060e-02, -3.7234e-02, -2.3114e-02,  2.3158e-03, -8.1141e-05,\n",
       "                      -2.2075e-02,  9.4981e-03, -3.3749e-02, -2.5561e-02,  4.2973e-02,\n",
       "                      -1.0284e-02,  2.7228e-02,  2.0740e-02, -3.8514e-02, -4.8146e-03,\n",
       "                       3.5118e-02,  3.3850e-02, -6.4603e-03,  2.2083e-02, -1.3166e-02,\n",
       "                       1.6354e-04,  4.3696e-02, -3.8667e-02,  3.9255e-02,  3.0686e-02,\n",
       "                       2.3177e-02, -2.7718e-02, -1.4331e-02,  2.5791e-02, -7.5083e-03,\n",
       "                       3.5781e-02,  1.8480e-02, -2.0134e-02, -3.8802e-02,  1.1074e-02,\n",
       "                      -3.3912e-02,  4.2291e-02,  3.2608e-03,  3.2025e-02,  5.7109e-05,\n",
       "                       2.8834e-02,  3.0357e-02,  1.2946e-02, -2.8580e-02,  5.3944e-03,\n",
       "                      -1.6753e-02,  1.2634e-02, -3.4618e-02, -2.0429e-02, -4.4215e-02,\n",
       "                       5.1954e-03,  1.1770e-02,  2.9606e-02, -1.3361e-02,  7.2780e-03,\n",
       "                      -6.4604e-03,  2.9159e-02, -1.3256e-02, -3.5466e-02,  1.2563e-02,\n",
       "                      -8.7818e-03, -1.0850e-02,  1.3771e-02, -4.3470e-02,  6.1031e-03,\n",
       "                      -4.3301e-02, -1.3212e-02,  1.0330e-02,  4.1972e-02,  3.2686e-02,\n",
       "                      -2.5274e-02,  2.5661e-02, -4.1330e-02,  3.5527e-02,  3.6354e-02,\n",
       "                      -2.7914e-02, -3.5491e-02, -3.3542e-02, -2.5462e-02, -2.2135e-02,\n",
       "                       2.5090e-02,  4.8668e-03, -4.1108e-02,  4.3589e-02, -6.7858e-03,\n",
       "                      -3.1859e-02, -3.5064e-03,  3.1647e-02,  2.2125e-03,  1.3724e-02,\n",
       "                      -2.1358e-02,  1.6079e-02, -3.8844e-02,  2.3398e-02,  1.4325e-02,\n",
       "                      -2.3102e-02, -2.3028e-02, -4.8190e-03,  2.2204e-02, -1.1203e-03,\n",
       "                      -2.0977e-02, -9.2610e-03, -5.9871e-04,  1.1788e-02, -3.0470e-02,\n",
       "                      -2.5715e-02, -4.1657e-02,  2.2117e-02,  3.4363e-02, -2.9430e-02,\n",
       "                       4.2899e-02,  2.7564e-02, -1.0463e-02,  2.7081e-02, -2.2238e-02,\n",
       "                      -1.8444e-02, -3.3858e-02,  3.8548e-02, -4.3726e-02, -4.0435e-02,\n",
       "                       2.6269e-02, -4.1063e-02,  1.1340e-02,  2.5527e-02,  2.0959e-02,\n",
       "                       1.5809e-03, -1.5378e-02,  3.4883e-02,  4.1344e-02,  3.7476e-02,\n",
       "                      -1.3932e-02,  2.4987e-02, -2.8856e-02,  2.4991e-02, -1.4385e-02,\n",
       "                      -3.1513e-02, -1.2340e-03,  2.2979e-02,  1.2414e-03,  1.8118e-02,\n",
       "                      -3.6073e-02, -5.3088e-04, -1.5426e-02,  4.2410e-02, -2.1601e-02,\n",
       "                       3.8026e-02,  4.3829e-02, -2.0694e-02,  2.4534e-02, -1.7281e-02,\n",
       "                       4.1231e-02, -2.6891e-02,  2.5363e-02,  1.7329e-02,  6.0904e-03,\n",
       "                      -1.3550e-02,  3.1440e-02, -1.8452e-02, -2.0341e-02, -2.4197e-02,\n",
       "                      -2.8669e-02,  6.9259e-03, -3.9963e-02, -3.1522e-02, -2.4140e-02,\n",
       "                       1.5685e-02, -3.2849e-02, -3.1740e-02, -3.4168e-02,  2.3480e-02,\n",
       "                      -2.0965e-02, -1.2151e-02, -3.4091e-02,  2.4980e-02,  9.8147e-03,\n",
       "                      -3.0488e-02,  9.9239e-03, -1.6309e-02, -1.5159e-02, -1.5041e-03,\n",
       "                       1.0409e-02,  2.6331e-02,  3.3297e-02,  3.7531e-02,  8.3748e-03,\n",
       "                       2.5474e-02, -1.7451e-02,  1.6494e-02, -3.7877e-02,  3.8508e-02,\n",
       "                       3.9447e-03,  4.3836e-02, -3.8693e-02, -2.6466e-02,  3.2550e-03,\n",
       "                      -2.5961e-02, -2.6318e-02, -1.2325e-02, -3.7317e-02,  2.2609e-02,\n",
       "                       6.2834e-04, -1.1728e-02,  4.0327e-02,  4.1645e-02, -4.0620e-02,\n",
       "                      -2.2461e-02, -4.0615e-02, -1.5436e-02, -2.7801e-02, -2.1813e-02,\n",
       "                       2.1569e-03, -4.0714e-02])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0256,  0.0010, -0.0174,  ...,  0.0165,  0.0378, -0.0139],\n",
       "                      [ 0.0030, -0.0202,  0.0408,  ...,  0.0286,  0.0323, -0.0223],\n",
       "                      [-0.0441, -0.0331,  0.0194,  ...,  0.0092, -0.0363, -0.0261],\n",
       "                      ...,\n",
       "                      [ 0.0014, -0.0069, -0.0060,  ...,  0.0023,  0.0175, -0.0419],\n",
       "                      [-0.0028,  0.0431, -0.0389,  ...,  0.0171,  0.0254,  0.0181],\n",
       "                      [-0.0344, -0.0100,  0.0259,  ..., -0.0105,  0.0129,  0.0189]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 6.2033e-03, -3.6454e-02,  2.4840e-02, -2.3864e-02, -3.1969e-02,\n",
       "                      -3.1136e-02,  3.5751e-02,  4.5672e-03, -4.1532e-02,  3.7931e-02,\n",
       "                      -4.1443e-02, -1.3818e-02, -3.7158e-03, -2.5977e-02,  1.9919e-02,\n",
       "                      -2.0179e-02,  1.1743e-02, -1.6106e-02, -3.7786e-02, -5.8146e-03,\n",
       "                      -2.0921e-02,  1.8075e-02,  3.3292e-02, -2.7041e-02, -1.7289e-02,\n",
       "                      -1.8372e-02,  1.0185e-02, -1.3901e-02, -3.7852e-02, -2.6518e-02,\n",
       "                       2.9819e-02, -2.2631e-02, -4.2205e-02, -2.3796e-02, -1.1658e-02,\n",
       "                      -2.7403e-03, -2.2736e-02,  3.4476e-02, -2.6470e-02, -4.1084e-02,\n",
       "                       1.9263e-02, -3.1561e-02, -3.8552e-02, -3.9993e-02,  2.5609e-02,\n",
       "                      -1.6175e-02, -3.9501e-02,  6.5688e-03,  3.2491e-02, -2.2135e-02,\n",
       "                      -1.3266e-02, -4.3471e-02,  4.2182e-02,  1.1393e-02, -9.3449e-03,\n",
       "                      -1.4078e-02,  2.8474e-02, -4.0110e-02,  1.7702e-02,  3.0147e-02,\n",
       "                      -2.1448e-03,  3.9763e-02, -5.4118e-03, -6.3866e-03,  9.9372e-03,\n",
       "                       2.2240e-02,  2.4133e-02, -1.0339e-02,  4.3277e-02,  2.7752e-02,\n",
       "                      -6.9769e-03, -2.3918e-02,  9.8319e-03, -3.5194e-02, -2.5265e-02,\n",
       "                       4.0292e-02, -2.4896e-02, -1.8491e-02,  1.5668e-02, -5.5232e-03,\n",
       "                       3.3664e-03, -2.1803e-02,  2.4222e-02,  1.5621e-03, -3.2835e-02,\n",
       "                       1.8487e-02,  3.1218e-02,  4.2126e-02,  3.0105e-03, -3.1263e-02,\n",
       "                       3.2268e-02,  2.4725e-02,  1.7561e-02,  3.5035e-02, -3.6859e-02,\n",
       "                       1.7509e-02,  3.3741e-02, -3.1527e-02, -2.4409e-02,  2.5611e-02,\n",
       "                       1.5623e-02, -4.3195e-02,  3.6609e-02,  1.9280e-02, -9.9381e-04,\n",
       "                       2.1039e-02, -3.9929e-02, -2.1669e-02,  2.1034e-02, -4.1365e-02,\n",
       "                      -1.9672e-02,  4.3977e-02, -7.2013e-03, -4.0071e-02, -2.6460e-02,\n",
       "                       4.0228e-02, -3.3410e-02, -3.0551e-02,  3.6291e-02, -1.7544e-02,\n",
       "                      -1.3964e-02,  2.5043e-02, -3.1431e-02,  4.3047e-02, -2.5323e-02,\n",
       "                      -1.7679e-02, -3.1617e-02, -9.7093e-03,  2.0165e-02,  4.3757e-02,\n",
       "                      -3.5220e-02,  1.5539e-02,  2.0077e-02, -2.4188e-02,  3.9338e-02,\n",
       "                       2.1934e-02,  2.3574e-02,  1.3812e-02, -4.0228e-02,  3.2230e-02,\n",
       "                       2.6740e-02, -2.9540e-02, -3.6990e-02,  2.0265e-02, -1.8293e-02,\n",
       "                      -3.1039e-02, -1.4023e-02, -3.2946e-03, -3.6064e-02, -7.6480e-04,\n",
       "                      -1.8280e-02, -2.3067e-02, -2.0000e-02,  3.0118e-02,  4.0151e-02,\n",
       "                      -2.4201e-02, -2.3983e-02, -3.8728e-02,  2.6660e-02, -2.1182e-02,\n",
       "                      -3.4128e-02,  1.4151e-02, -4.7820e-03,  1.3584e-02, -2.1897e-02,\n",
       "                      -4.1441e-02, -2.5371e-02, -2.3245e-03,  4.0731e-02, -5.5432e-04,\n",
       "                      -2.5350e-02, -2.5078e-02, -7.5411e-03,  9.4693e-04, -2.5524e-02,\n",
       "                       6.0427e-03, -2.5517e-02, -2.2149e-02, -7.2291e-03,  3.2129e-02,\n",
       "                       3.9876e-02,  1.1445e-02,  4.0360e-02, -2.7096e-02, -1.6440e-02,\n",
       "                      -1.7710e-02, -3.5369e-02, -2.4748e-02,  9.0148e-05,  2.6411e-02,\n",
       "                      -3.4152e-02,  2.7652e-02,  3.3392e-02, -3.7633e-02,  9.4756e-04,\n",
       "                      -1.5747e-03, -3.6267e-03,  6.7521e-03, -1.7051e-02, -2.8549e-02,\n",
       "                       4.6245e-04,  9.0299e-03,  4.2078e-02, -7.3356e-03, -1.4321e-02,\n",
       "                      -1.6048e-02, -3.9840e-02, -4.2772e-02, -1.4447e-02,  3.7584e-02,\n",
       "                      -1.1443e-03, -3.8152e-02, -1.2811e-02, -4.3828e-02, -3.9582e-02,\n",
       "                      -2.2855e-02, -6.2829e-03,  2.3375e-02, -1.1864e-02, -4.2742e-02,\n",
       "                      -3.2915e-02,  2.0263e-02,  1.5206e-02,  3.8419e-02, -4.2916e-02,\n",
       "                      -3.1959e-02, -3.2758e-02, -2.6452e-03, -1.9173e-02,  1.0084e-02,\n",
       "                       2.8580e-02,  2.9653e-02, -4.3661e-03, -1.4818e-02,  3.8734e-02,\n",
       "                      -2.1492e-02,  1.9090e-02, -1.5937e-02,  3.8854e-02,  3.3636e-02,\n",
       "                       2.6380e-02,  4.2184e-02,  2.1365e-02,  4.1200e-02,  6.9404e-04,\n",
       "                       1.7581e-02, -2.7826e-02, -2.9965e-02,  3.1629e-02,  2.6742e-02,\n",
       "                      -2.2913e-03, -1.2512e-02,  3.6464e-02, -3.6170e-02, -8.7660e-03,\n",
       "                       3.9268e-02, -4.9457e-04, -3.0544e-02, -1.0526e-02, -4.4018e-02,\n",
       "                      -1.6380e-02,  1.4597e-02,  3.6199e-02, -4.2220e-02, -4.0165e-02,\n",
       "                      -6.9954e-03,  2.0229e-02,  1.2524e-02, -1.1027e-02,  1.1747e-02,\n",
       "                       2.3912e-02,  3.9065e-02,  3.5779e-02,  2.7214e-02,  3.4514e-02,\n",
       "                      -1.5217e-02, -3.0314e-02,  1.7211e-02,  4.4081e-02, -2.7803e-02,\n",
       "                      -1.6058e-02,  3.2702e-02,  8.3409e-03, -3.0393e-03, -2.4910e-02,\n",
       "                      -3.0768e-02, -1.7979e-02,  5.1641e-03,  1.5313e-02, -5.2246e-03,\n",
       "                       3.4373e-02,  2.8719e-02, -4.2348e-03,  2.0141e-02,  1.2015e-03,\n",
       "                      -4.2326e-02, -1.0387e-02,  2.3848e-02, -2.9246e-02, -4.0365e-02,\n",
       "                      -3.9043e-02,  2.9051e-02,  1.8590e-02,  3.8999e-02, -1.0382e-02,\n",
       "                      -4.2601e-02, -4.1286e-02,  7.0312e-03, -3.0621e-03,  2.5145e-02,\n",
       "                      -3.4343e-02, -1.9705e-02, -8.0682e-03, -6.5501e-03,  2.1942e-02,\n",
       "                       7.0072e-04,  1.1047e-02, -5.4365e-03,  1.8751e-02, -2.9690e-02,\n",
       "                      -3.7400e-02,  1.7656e-02,  1.5130e-02,  8.7649e-03, -1.9781e-02,\n",
       "                       4.1289e-03,  4.0576e-02, -6.5696e-03, -3.7925e-02,  2.7019e-02,\n",
       "                       2.8550e-02,  2.4116e-02,  4.2587e-02, -1.6571e-02, -3.4307e-02,\n",
       "                       1.7989e-02,  2.8853e-02,  1.7492e-03,  2.2968e-02,  2.1538e-02,\n",
       "                      -4.2874e-02, -1.8878e-02,  8.0375e-04,  3.4803e-02,  6.7028e-03,\n",
       "                       2.2051e-02, -3.1635e-02, -1.0441e-02, -2.5631e-02,  3.4103e-02,\n",
       "                      -8.4086e-03,  2.9611e-02, -4.1284e-02,  1.6610e-02,  1.4786e-03,\n",
       "                       2.4627e-02,  1.0624e-02, -7.5199e-03,  2.7799e-02, -1.6320e-02,\n",
       "                      -4.0914e-02,  2.6218e-02,  2.8966e-02,  4.4234e-03,  3.0508e-02,\n",
       "                      -1.1787e-03, -4.0418e-02,  1.4005e-02, -2.6044e-02, -1.9852e-02,\n",
       "                      -1.2046e-02,  1.4320e-02,  1.2873e-02, -2.4248e-03, -2.7927e-02,\n",
       "                      -2.7561e-02, -1.6471e-02, -1.3622e-02, -1.3938e-03, -2.2441e-03,\n",
       "                       3.2373e-02,  3.8010e-02,  3.8571e-02, -1.5675e-03,  3.3987e-02,\n",
       "                       1.4656e-02,  7.2132e-04,  1.7321e-02,  3.3643e-03, -4.0623e-02,\n",
       "                       3.8659e-02, -3.2499e-02,  4.4068e-02,  1.7173e-02,  3.1392e-02,\n",
       "                      -3.8927e-02,  4.3004e-02,  1.3745e-02, -3.7865e-02,  2.3152e-02,\n",
       "                       2.0490e-02, -1.4676e-02, -1.3384e-02,  7.8231e-04, -1.1975e-02,\n",
       "                      -2.0788e-02,  2.3413e-02,  1.9510e-02, -1.5731e-02, -3.9309e-02,\n",
       "                      -2.5501e-04, -3.6751e-03,  4.1739e-04,  1.1103e-02, -6.5124e-03,\n",
       "                       1.8081e-02,  2.5861e-02,  1.9916e-02, -1.0695e-02,  7.0389e-03,\n",
       "                      -3.7878e-03, -9.0262e-03, -3.8327e-02,  3.9878e-02,  1.5522e-03,\n",
       "                       3.0163e-02, -4.3178e-02, -3.0371e-02,  3.1131e-02, -9.3988e-03,\n",
       "                      -3.5250e-02, -7.9107e-03,  4.3239e-02, -3.0932e-02, -7.7175e-03,\n",
       "                      -2.6006e-05, -1.8710e-02, -4.3663e-02,  3.2342e-02,  2.6736e-02,\n",
       "                      -2.2481e-02,  3.2487e-02, -2.5653e-02,  3.5485e-02,  3.5686e-02,\n",
       "                       3.8440e-02,  1.3515e-02,  2.8741e-02, -1.7213e-02, -2.3856e-02,\n",
       "                       3.6070e-02,  4.2841e-04, -2.7174e-02, -2.2725e-02, -1.4189e-02,\n",
       "                       4.1410e-02, -8.2058e-04, -6.1205e-03,  1.6040e-02,  2.5603e-02,\n",
       "                      -1.8715e-02, -1.4629e-02, -1.0982e-02, -3.6595e-02,  2.4020e-02,\n",
       "                      -2.6407e-04, -1.2782e-02,  2.6339e-02, -3.4681e-02, -1.3579e-02,\n",
       "                      -3.5085e-02,  1.5315e-02,  2.3951e-02,  3.9016e-03,  1.4659e-02,\n",
       "                      -1.1397e-02,  1.0259e-02, -1.5553e-02,  2.7056e-02, -1.6432e-02,\n",
       "                       3.3425e-02, -3.5855e-02, -2.1224e-02,  2.1337e-02,  3.7060e-03,\n",
       "                      -1.0146e-02,  2.4103e-02,  2.1835e-03,  1.3621e-02, -4.3992e-02,\n",
       "                      -3.2596e-02,  2.4500e-02,  3.7306e-03,  6.9662e-03, -3.5724e-02,\n",
       "                       3.6388e-02, -3.8721e-04, -1.0207e-02, -5.6509e-04,  2.0900e-02,\n",
       "                       3.7206e-02,  6.4011e-03,  1.7566e-02,  2.3906e-02,  2.2166e-02,\n",
       "                       2.0667e-02,  3.6967e-02,  2.8169e-02, -1.4140e-02, -2.7136e-02,\n",
       "                      -2.1473e-02, -7.8390e-03])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 0.0306,  0.0085,  0.0207,  ...,  0.0164,  0.0421,  0.0182],\n",
       "                      [ 0.0283, -0.0305, -0.0020,  ...,  0.0120,  0.0250, -0.0203],\n",
       "                      [-0.0155, -0.0098, -0.0252,  ...,  0.0438, -0.0066,  0.0102],\n",
       "                      ...,\n",
       "                      [ 0.0077,  0.0049,  0.0091,  ..., -0.0136, -0.0010,  0.0198],\n",
       "                      [ 0.0302, -0.0237, -0.0080,  ..., -0.0049,  0.0064,  0.0272],\n",
       "                      [ 0.0296, -0.0205,  0.0398,  ..., -0.0260,  0.0130, -0.0196]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.0300,  0.0015,  0.0090, -0.0390, -0.0266, -0.0429,  0.0337, -0.0287,\n",
       "                      -0.0435, -0.0272]))])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0418, -0.0115, -0.0255,  ..., -0.0429,  0.0054, -0.0207],\n",
       "                      [-0.0004, -0.0126,  0.0411,  ..., -0.0120,  0.0354,  0.0257],\n",
       "                      [ 0.0204,  0.0260,  0.0241,  ..., -0.0063,  0.0149,  0.0230],\n",
       "                      ...,\n",
       "                      [ 0.0067,  0.0153,  0.0120,  ..., -0.0384, -0.0111,  0.0280],\n",
       "                      [-0.0528, -0.0116, -0.0123,  ..., -0.0145, -0.0355,  0.0126],\n",
       "                      [-0.0372,  0.0035,  0.0203,  ..., -0.0230, -0.0341, -0.0667]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 0.0228, -0.0306, -0.0354,  0.0174, -0.0305, -0.0114, -0.0583, -0.0008,\n",
       "                      -0.0036, -0.0196,  0.0155,  0.0022,  0.0083,  0.0215, -0.0586, -0.0144,\n",
       "                      -0.0454, -0.0503,  0.0060, -0.0172, -0.0233, -0.0360, -0.0346, -0.0632,\n",
       "                       0.0055, -0.0293, -0.0087, -0.0359,  0.0125,  0.0031, -0.0110, -0.0035,\n",
       "                       0.0097, -0.0151, -0.0454, -0.0229, -0.0219, -0.0574, -0.0020, -0.0262,\n",
       "                      -0.0419, -0.0453, -0.0315, -0.0314, -0.0533,  0.0280,  0.0204, -0.0446,\n",
       "                      -0.0444,  0.0063, -0.0567,  0.0200, -0.0301, -0.0363,  0.0158, -0.0525,\n",
       "                       0.0179, -0.0216, -0.0526, -0.0317, -0.0154, -0.0430,  0.0217, -0.0529,\n",
       "                       0.0022, -0.0431,  0.0252,  0.0034,  0.0210, -0.0406,  0.0162, -0.0246,\n",
       "                       0.0009, -0.0330, -0.0205, -0.0507, -0.0068, -0.0356, -0.0376, -0.0014,\n",
       "                      -0.0525,  0.0095, -0.0031,  0.0293, -0.0422,  0.0105, -0.0348, -0.0605,\n",
       "                       0.0055, -0.0470, -0.0194, -0.0438,  0.0061, -0.0049, -0.0614, -0.0223,\n",
       "                      -0.0030, -0.0046, -0.0303, -0.0407,  0.0222, -0.0122, -0.0347, -0.0039,\n",
       "                      -0.0407, -0.0151,  0.0066, -0.0323, -0.0053, -0.0487,  0.0125,  0.0140,\n",
       "                      -0.0022, -0.0389,  0.0017,  0.0101,  0.0019, -0.0594,  0.0105,  0.0106,\n",
       "                      -0.0218, -0.0231, -0.0528, -0.0192, -0.0072, -0.0399, -0.0202, -0.0418,\n",
       "                      -0.0319, -0.0473, -0.0161,  0.0023, -0.0154, -0.0440, -0.0470, -0.0471,\n",
       "                       0.0144,  0.0168, -0.0487,  0.0118, -0.0023,  0.0230,  0.0157, -0.0097,\n",
       "                      -0.0520,  0.0061,  0.0188, -0.0059,  0.0105, -0.0534, -0.0008, -0.0283,\n",
       "                       0.0178, -0.0015, -0.0202,  0.0070, -0.0185, -0.0103, -0.0294, -0.0195,\n",
       "                       0.0267, -0.0070,  0.0059, -0.0374,  0.0164, -0.0431, -0.0072, -0.0324,\n",
       "                      -0.0417, -0.0471, -0.0534, -0.0522, -0.0041, -0.0567, -0.0604, -0.0108,\n",
       "                      -0.0051, -0.0084, -0.0125, -0.0287, -0.0037, -0.0450,  0.0140,  0.0207,\n",
       "                      -0.0101, -0.0359, -0.0113, -0.0101, -0.0227, -0.0563, -0.0614, -0.0413,\n",
       "                      -0.0581, -0.0063,  0.0157, -0.0085, -0.0191,  0.0298,  0.0107,  0.0136,\n",
       "                      -0.0204, -0.0231,  0.0204,  0.0122, -0.0019, -0.0410, -0.0241, -0.0097,\n",
       "                       0.0038, -0.0140, -0.0293, -0.0177,  0.0009,  0.0203, -0.0500, -0.0134,\n",
       "                       0.0137, -0.0227,  0.0217, -0.0137, -0.0418, -0.0384, -0.0196,  0.0087,\n",
       "                      -0.0097,  0.0274, -0.0020,  0.0084, -0.0318, -0.0251, -0.0196, -0.0469,\n",
       "                      -0.0222,  0.0107, -0.0537,  0.0002,  0.0148, -0.0441,  0.0115, -0.0157,\n",
       "                      -0.0391, -0.0017,  0.0026, -0.0122, -0.0145,  0.0045, -0.0200, -0.0303,\n",
       "                      -0.0170, -0.0188, -0.0327, -0.0106, -0.0415, -0.0175, -0.0164, -0.0365,\n",
       "                      -0.0427,  0.0210, -0.0017, -0.0251,  0.0045, -0.0234, -0.0422,  0.0113,\n",
       "                      -0.0119, -0.0222, -0.0369, -0.0444,  0.0031, -0.0371, -0.0194, -0.0131,\n",
       "                      -0.0504,  0.0014, -0.0218, -0.0198, -0.0080, -0.0114, -0.0031,  0.0004,\n",
       "                      -0.0538, -0.0217,  0.0147, -0.0471, -0.0139,  0.0200,  0.0024, -0.0510,\n",
       "                      -0.0113,  0.0144, -0.0011, -0.0329, -0.0361,  0.0241, -0.0301, -0.0098,\n",
       "                       0.0061,  0.0112,  0.0164, -0.0420,  0.0083, -0.0535, -0.0402, -0.0230,\n",
       "                      -0.0146, -0.0432, -0.0125, -0.0451, -0.0438,  0.0232, -0.0285,  0.0014,\n",
       "                      -0.0010, -0.0570, -0.0208,  0.0109,  0.0150, -0.0350, -0.0041, -0.0238,\n",
       "                      -0.0131,  0.0237, -0.0599,  0.0210,  0.0027, -0.0032, -0.0494, -0.0292,\n",
       "                       0.0023, -0.0258,  0.0170, -0.0024, -0.0275, -0.0574, -0.0057, -0.0514,\n",
       "                       0.0266, -0.0195,  0.0094, -0.0102,  0.0112,  0.0069, -0.0104, -0.0435,\n",
       "                      -0.0090, -0.0317, -0.0044, -0.0443, -0.0339, -0.0621, -0.0151,  0.0016,\n",
       "                       0.0175, -0.0364, -0.0107, -0.0212,  0.0066, -0.0311, -0.0509,  0.0068,\n",
       "                      -0.0217, -0.0244, -0.0007, -0.0564, -0.0086, -0.0607, -0.0316, -0.0072,\n",
       "                       0.0219,  0.0124, -0.0538,  0.0086, -0.0579,  0.0161,  0.0217, -0.0500,\n",
       "                      -0.0550, -0.0440, -0.0401, -0.0389,  0.0128, -0.0179, -0.0455,  0.0190,\n",
       "                      -0.0251, -0.0522, -0.0245,  0.0127, -0.0121, -0.0039, -0.0415, -0.0037,\n",
       "                      -0.0582,  0.0037,  0.0033, -0.0341, -0.0409, -0.0262,  0.0031, -0.0203,\n",
       "                      -0.0404, -0.0297, -0.0179, -0.0094, -0.0455, -0.0399, -0.0569,  0.0006,\n",
       "                       0.0143, -0.0438,  0.0265,  0.0061, -0.0316,  0.0101, -0.0405, -0.0359,\n",
       "                      -0.0512,  0.0203, -0.0557, -0.0567, -0.0033, -0.0548, -0.0118,  0.0074,\n",
       "                      -0.0014, -0.0152, -0.0259,  0.0139,  0.0174,  0.0146, -0.0328,  0.0057,\n",
       "                      -0.0411,  0.0149, -0.0280, -0.0492, -0.0225,  0.0069, -0.0223, -0.0013,\n",
       "                      -0.0559, -0.0305, -0.0360,  0.0237, -0.0356,  0.0099,  0.0275, -0.0261,\n",
       "                       0.0086, -0.0323,  0.0226, -0.0464,  0.0116,  0.0002, -0.0121, -0.0341,\n",
       "                       0.0107, -0.0342, -0.0304, -0.0462, -0.0489, -0.0098, -0.0639, -0.0302,\n",
       "                      -0.0375, -0.0067, -0.0478, -0.0404, -0.0620,  0.0087, -0.0328, -0.0356,\n",
       "                      -0.0573, -0.0008, -0.0145, -0.0497, -0.0102, -0.0345, -0.0309, -0.0177,\n",
       "                      -0.0072,  0.0083,  0.0147,  0.0145, -0.0012,  0.0043, -0.0265,  0.0028,\n",
       "                      -0.0496,  0.0190, -0.0137,  0.0274, -0.0557, -0.0444, -0.0129, -0.0461,\n",
       "                      -0.0444, -0.0281, -0.0571,  0.0086, -0.0222, -0.0333,  0.0152,  0.0207,\n",
       "                      -0.0602, -0.0390, -0.0587, -0.0317, -0.0417, -0.0439, -0.0132, -0.0560])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0155, -0.0034, -0.0303,  ...,  0.0214,  0.0218, -0.0175],\n",
       "                      [-0.0079, -0.0765,  0.0400,  ...,  0.0715,  0.0540, -0.0559],\n",
       "                      [-0.0550, -0.0462,  0.0142,  ..., -0.0014, -0.0442, -0.0347],\n",
       "                      ...,\n",
       "                      [-0.0034, -0.0060, -0.0108,  ..., -0.0104,  0.0085, -0.0423],\n",
       "                      [ 0.0086,  0.0238, -0.0490,  ...,  0.0416,  0.0470, -0.0172],\n",
       "                      [-0.0468, -0.0215,  0.0152,  ..., -0.0153,  0.0073, -0.0024]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([-0.0004, -0.0664,  0.0049, -0.0405, -0.0475, -0.0488,  0.0247, -0.0079,\n",
       "                      -0.0492,  0.0211, -0.0652, -0.0267, -0.0191, -0.0403,  0.0117, -0.0438,\n",
       "                       0.0060, -0.0346, -0.0403, -0.0127, -0.0261, -0.0013,  0.0192, -0.0252,\n",
       "                      -0.0292, -0.0241,  0.0001, -0.0322, -0.0418, -0.0388,  0.0302, -0.0357,\n",
       "                      -0.0424, -0.0360, -0.0198, -0.0199, -0.0345,  0.0283, -0.0349, -0.0509,\n",
       "                       0.0074, -0.0498, -0.0566, -0.0455,  0.0025, -0.0307, -0.0490, -0.0136,\n",
       "                       0.0273, -0.0327, -0.0270, -0.0556,  0.0400,  0.0043, -0.0269, -0.0326,\n",
       "                       0.0126, -0.0463,  0.0090,  0.0218, -0.0189,  0.0275, -0.0178, -0.0162,\n",
       "                      -0.0045,  0.0171,  0.0167, -0.0232,  0.0364,  0.0111, -0.0168, -0.0359,\n",
       "                       0.0077, -0.0483, -0.0392,  0.0322, -0.0320, -0.0317, -0.0072, -0.0184,\n",
       "                      -0.0126, -0.0305,  0.0114, -0.0189, -0.0437,  0.0054,  0.0261,  0.0358,\n",
       "                      -0.0099, -0.0502,  0.0181,  0.0183,  0.0056,  0.0263, -0.0525,  0.0022,\n",
       "                       0.0233, -0.0488, -0.0390,  0.0153, -0.0033, -0.0489,  0.0130,  0.0065,\n",
       "                      -0.0040,  0.0164, -0.0502, -0.0360,  0.0203, -0.0478, -0.0355,  0.0423,\n",
       "                      -0.0127, -0.0476, -0.0330,  0.0282, -0.0467, -0.0332,  0.0233, -0.0348,\n",
       "                      -0.0313,  0.0146, -0.0475,  0.0271, -0.0288, -0.0291, -0.0448, -0.0212,\n",
       "                       0.0276,  0.0278, -0.0479,  0.0049,  0.0129, -0.0336,  0.0250,  0.0109,\n",
       "                       0.0168,  0.0036, -0.0542,  0.0278,  0.0147, -0.0409, -0.0521,  0.0183,\n",
       "                      -0.0319, -0.0378, -0.0241, -0.0166, -0.0497, -0.0127, -0.0319, -0.0303,\n",
       "                      -0.0223,  0.0306,  0.0292, -0.0120, -0.0377, -0.0292,  0.0166, -0.0310,\n",
       "                      -0.0442,  0.0150, -0.0175,  0.0063, -0.0301, -0.0544, -0.0403, -0.0125,\n",
       "                       0.0087, -0.0076, -0.0387, -0.0388, -0.0137,  0.0059, -0.0382, -0.0038,\n",
       "                      -0.0336, -0.0327, -0.0060,  0.0201,  0.0310,  0.0109,  0.0289, -0.0397,\n",
       "                      -0.0385, -0.0325, -0.0412, -0.0429, -0.0048,  0.0084, -0.0498,  0.0159,\n",
       "                       0.0210, -0.0549, -0.0057, -0.0093, -0.0214,  0.0050, -0.0369, -0.0426,\n",
       "                      -0.0114,  0.0028,  0.0274, -0.0183, -0.0275, -0.0101, -0.0539, -0.0529,\n",
       "                      -0.0305,  0.0242,  0.0007, -0.0419, -0.0302, -0.0478, -0.0423, -0.0364,\n",
       "                      -0.0005,  0.0212, -0.0184, -0.0540, -0.0364,  0.0039,  0.0050,  0.0340,\n",
       "                      -0.0560, -0.0473, -0.0448, -0.0212, -0.0316,  0.0088,  0.0186,  0.0198,\n",
       "                      -0.0187, -0.0292,  0.0420, -0.0336,  0.0021, -0.0244,  0.0368,  0.0106,\n",
       "                       0.0093,  0.0383,  0.0123,  0.0261, -0.0042,  0.0072, -0.0327, -0.0518,\n",
       "                       0.0239,  0.0143, -0.0094, -0.0243,  0.0293, -0.0416, -0.0282,  0.0372,\n",
       "                      -0.0214, -0.0430, -0.0329, -0.0549, -0.0279,  0.0041,  0.0332, -0.0542,\n",
       "                      -0.0546, -0.0289,  0.0074, -0.0050, -0.0250, -0.0010,  0.0089,  0.0100,\n",
       "                      -0.0023,  0.0126,  0.0277, -0.0197, -0.0387,  0.0007,  0.0321, -0.0337,\n",
       "                      -0.0237,  0.0241,  0.0091, -0.0050, -0.0430, -0.0344, -0.0280, -0.0024,\n",
       "                       0.0055, -0.0100,  0.0185,  0.0215, -0.0225,  0.0071, -0.0086, -0.0501,\n",
       "                      -0.0376,  0.0089, -0.0523, -0.0503, -0.0461,  0.0136,  0.0086,  0.0342,\n",
       "                      -0.0209, -0.0449, -0.0502, -0.0062, -0.0173,  0.0086, -0.0507, -0.0453,\n",
       "                      -0.0285, -0.0191,  0.0163, -0.0063,  0.0133, -0.0041,  0.0044, -0.0478,\n",
       "                      -0.0430,  0.0050,  0.0090,  0.0008, -0.0205, -0.0046,  0.0346, -0.0232,\n",
       "                      -0.0501,  0.0177,  0.0230,  0.0131,  0.0274, -0.0279, -0.0436,  0.0071,\n",
       "                       0.0235, -0.0150,  0.0067,  0.0195, -0.0545, -0.0294, -0.0160,  0.0195,\n",
       "                       0.0061,  0.0102, -0.0402, -0.0162, -0.0405,  0.0202, -0.0218,  0.0200,\n",
       "                      -0.0547,  0.0061, -0.0165,  0.0070,  0.0088, -0.0138,  0.0201, -0.0138,\n",
       "                      -0.0520,  0.0256,  0.0220, -0.0064,  0.0137, -0.0136, -0.0507,  0.0003,\n",
       "                      -0.0290, -0.0522, -0.0258, -0.0013, -0.0036, -0.0100, -0.0415, -0.0442,\n",
       "                      -0.0283, -0.0312, -0.0120, -0.0091,  0.0217,  0.0298,  0.0299, -0.0132,\n",
       "                       0.0237, -0.0007, -0.0081,  0.0052, -0.0045, -0.0483,  0.0403, -0.0498,\n",
       "                       0.0369,  0.0174,  0.0136, -0.0513,  0.0304,  0.0021, -0.0471,  0.0197,\n",
       "                       0.0010, -0.0284, -0.0262, -0.0071, -0.0175, -0.0346,  0.0192,  0.0120,\n",
       "                      -0.0170, -0.0470, -0.0078, -0.0239, -0.0154,  0.0050, -0.0152,  0.0048,\n",
       "                       0.0122,  0.0111, -0.0204,  0.0033, -0.0217, -0.0291, -0.0406,  0.0195,\n",
       "                      -0.0053,  0.0245, -0.0569, -0.0430,  0.0235, -0.0257, -0.0446, -0.0103,\n",
       "                       0.0380, -0.0450, -0.0201,  0.0028, -0.0297, -0.0530,  0.0234,  0.0215,\n",
       "                      -0.0352,  0.0236, -0.0455,  0.0325,  0.0260,  0.0271, -0.0057,  0.0252,\n",
       "                      -0.0301, -0.0292,  0.0115, -0.0112, -0.0360, -0.0333, -0.0270,  0.0247,\n",
       "                      -0.0154, -0.0135,  0.0090,  0.0098, -0.0253, -0.0202, -0.0216, -0.0499,\n",
       "                       0.0045, -0.0167, -0.0252,  0.0212, -0.0475, -0.0165, -0.0471,  0.0063,\n",
       "                       0.0157, -0.0111, -0.0007, -0.0061,  0.0038, -0.0365,  0.0273, -0.0193,\n",
       "                       0.0231, -0.0490, -0.0290, -0.0053, -0.0140, -0.0186,  0.0209, -0.0024,\n",
       "                       0.0023, -0.0544, -0.0460,  0.0228, -0.0067, -0.0051, -0.0311,  0.0270,\n",
       "                      -0.0130, -0.0171, -0.0120,  0.0073,  0.0324, -0.0130,  0.0058,  0.0109,\n",
       "                       0.0096,  0.0090,  0.0205,  0.0168, -0.0312, -0.0427, -0.0355, -0.0224])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 0.1731,  0.4431, -0.0346,  ..., -0.0619,  0.2773,  0.0175],\n",
       "                      [ 0.1551, -0.1612, -0.0157,  ...,  0.1290,  0.0737, -0.0793],\n",
       "                      [-0.0284, -0.0527, -0.0537,  ...,  0.0162, -0.0490, -0.0064],\n",
       "                      ...,\n",
       "                      [-0.0452,  0.2208,  0.0368,  ..., -0.0740,  0.0906,  0.0635],\n",
       "                      [-0.0101, -0.0339,  0.0144,  ..., -0.0143, -0.0188,  0.0652],\n",
       "                      [ 0.0968, -0.1843,  0.0317,  ...,  0.0096,  0.0896, -0.0980]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.1103, -0.1155, -0.0244,  0.0543, -0.0127, -0.2644,  0.1078,  0.1540,\n",
       "                       0.1378, -0.1203]))])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_trained_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre: \n",
      " Layer 1: 8.3034996e-07, 0.025849843\n",
      " Layer 2: 7.102816e-05, 0.025543222\n",
      " Layer 3: 0.00018317447, 0.02547586\n",
      "Post: \n",
      " Layer 1: -0.0015452711, 0.03388202\n",
      " Layer 2: -0.005922225, 0.03329243\n",
      " Layer 3: 0.0001834683, 0.123158395\n"
     ]
    }
   ],
   "source": [
    "mean_pre_0 = np.mean(pre_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_post_0 = np.mean(post_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_pre_0 = np.std(pre_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_post_0 = np.std(post_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_pre_1 = np.mean(pre_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_post_1 = np.mean(post_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_pre_1 = np.std(pre_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_post_1 = np.std(post_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_pre_2 = np.mean(pre_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "mean_post_2 = np.mean(post_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "std_pre_2 = np.std(pre_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "std_post_2 = np.std(post_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "print(\"Pre: \")\n",
    "print(\" Layer 1: \" + str(mean_pre_0) + \", \" + str(std_pre_0))\n",
    "print(\" Layer 2: \" + str(mean_pre_1) + \", \" + str(std_pre_1))\n",
    "print(\" Layer 3: \" + str(mean_pre_2) + \", \" + str(std_pre_2))\n",
    "print(\"Post: \")\n",
    "print(\" Layer 1: \" + str(mean_post_0) + \", \" + str(std_post_0))\n",
    "print(\" Layer 2: \" + str(mean_post_1) + \", \" + str(std_post_1))\n",
    "print(\" Layer 3: \" + str(mean_post_2) + \", \" + str(std_post_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "for item in post_trained_multiclass:\n",
    "    if item[20:] == \"weight\":\n",
    "        table = pd.DataFrame(post_trained_multiclass[item])\n",
    "        table.to_csv(\"results/nonMNIST_multi_weight_post_\" + str(ID) + \".csv\",index=False)\n",
    "    if item[20:] == \"bias\":\n",
    "        series = pd.Series(post_trained_multiclass[item])\n",
    "        series.to_csv(\"results/nonMNIST_multi_bias_post_\" + str(ID) + \".csv\",index=False)\n",
    "    ID += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think these results indicate that rather than there being special characteristics of the MNIST dataset, The Neural Networks have different update patterns depending on the type of problem. It could be possible that the Loss functions impact the way a network updates, since the loss directly impacts the update pattern. However, given the results of the previous sets, this doesn't seem to be the case. I have not been testing for this, though, so this should be taken lightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9/23/2024 - There is a separation between Classification problems and regression problems in terms of network updates. Classification problems build upon patterns previously found in the network, unless it is very extreme. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
