{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmer: Jacob Maurer\n",
    "date: 9/21/2024\n",
    "Description: This file is meant to make models that are trained on data other than the MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_regression, make_multilabel_classification\n",
    "import copy\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these experiments, we are going to assume about half of the features are informative, with no redundant or repeated features. The samples are going to be (5*number of features) * 10. This is going to be constant across regression and classification. The final sets will be balanced, seperate experiments will be run with unbalanced sets. The batch size will be 10.The split between train and test sets will be 80% training and 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_classification_problem = make_classification(n_samples = 5000, n_features=100, n_informative=50)\n",
    "medium_classification_problem = make_classification(n_samples = 15000, n_features=300, n_informative=150)\n",
    "large_classification_problem = make_classification(n_samples = 25000, n_features=500, n_informative = 250)\n",
    "small_regression_problem = make_regression(n_samples = 5000, n_features=100, n_informative=10)\n",
    "medium_regression_problem = make_regression(n_samples = 15000, n_features=300, n_informative=25)\n",
    "large_regression_problem = make_regression(n_samples = 25000, n_features=500, n_informative = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_c = pd.DataFrame(small_classification_problem[0])\n",
    "small_c['100'] = small_classification_problem[1]\n",
    "medium_c = pd.DataFrame(medium_classification_problem[0])\n",
    "medium_c['300'] = medium_classification_problem[1]\n",
    "large_c = pd.DataFrame(large_classification_problem[0])\n",
    "large_c['500'] = large_classification_problem[1]\n",
    "small_r = pd.DataFrame(small_regression_problem[0])\n",
    "small_r['100'] = small_regression_problem[1]\n",
    "medium_r = pd.DataFrame(medium_regression_problem[0])\n",
    "medium_r['300'] = medium_regression_problem[1]\n",
    "large_r = pd.DataFrame(large_regression_problem[0])\n",
    "large_r['500'] = large_regression_problem[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x = copy.deepcopy(x_data)\n",
    "        self.y = copy.deepcopy(y_data)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], np.float32(self.y[index])\n",
    "\n",
    "small_classification_train_data = GeneratedDataset(small_classification_problem[0][int(len(small_classification_problem[0])*.2):], small_classification_problem[1][int(len(small_classification_problem[1])*.2):])\n",
    "small_classification_test_data = GeneratedDataset(small_classification_problem[0][:int(len(small_classification_problem[0])*.2)], small_classification_problem[1][:int(len(small_classification_problem[1])*.2)])\n",
    "medium_classification_train_data = GeneratedDataset(medium_classification_problem[0][int(len(medium_classification_problem[0])*.2):], medium_classification_problem[1][int(len(medium_classification_problem[1])*.2):])\n",
    "medium_classification_test_data = GeneratedDataset(medium_classification_problem[0][:int(len(medium_classification_problem[0])*.2)], medium_classification_problem[1][:int(len(medium_classification_problem[1])*.2)])\n",
    "large_classification_train_data = GeneratedDataset(large_classification_problem[0][int(len(large_classification_problem[0])*.2):], large_classification_problem[1][int(len(large_classification_problem[1])*.2):])\n",
    "large_classification_test_data = GeneratedDataset(large_classification_problem[0][:int(len(large_classification_problem[0])*.2)], large_classification_problem[1][:int(len(large_classification_problem[1])*.2)])\n",
    "small_train_loader_class = DataLoader(small_classification_train_data, batch_size=100, shuffle=True)\n",
    "small_test_loader_class = DataLoader(small_classification_test_data, batch_size=100, shuffle=True)\n",
    "medium_train_loader_class = DataLoader(medium_classification_train_data, batch_size=100, shuffle=True)\n",
    "medium_test_loader_class = DataLoader(medium_classification_test_data, batch_size=100, shuffle=True)\n",
    "large_train_loader_class = DataLoader(large_classification_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_class = DataLoader(large_classification_test_data, batch_size=100, shuffle=True)\n",
    "small_regression_train_data = GeneratedDataset(small_regression_problem[0][int(len(small_regression_problem[0])*.2):], small_regression_problem[1][int(len(small_regression_problem[1])*.2):])\n",
    "small_regression_test_data = GeneratedDataset(small_regression_problem[0][:int(len(small_regression_problem[0])*.2)], small_regression_problem[1][:int(len(small_regression_problem[1])*.2)])\n",
    "medium_regression_train_data = GeneratedDataset(medium_regression_problem[0][int(len(medium_regression_problem[0])*.2):], medium_regression_problem[1][int(len(medium_regression_problem[1])*.2):])\n",
    "medium_regression_test_data = GeneratedDataset(medium_regression_problem[0][:int(len(medium_regression_problem[0])*.2)], medium_regression_problem[1][:int(len(medium_regression_problem[1])*.2)])\n",
    "large_regression_train_data = GeneratedDataset(large_regression_problem[0][int(len(large_regression_problem[0])*.2):], large_regression_problem[1][int(len(large_regression_problem[1])*.2):])\n",
    "large_regression_test_data = GeneratedDataset(large_regression_problem[0][:int(len(large_regression_problem[0])*.2)], large_regression_problem[1][:int(len(large_regression_problem[1])*.2)])\n",
    "small_train_loader_regress = DataLoader(small_regression_train_data, batch_size=100, shuffle=True)\n",
    "small_test_loader_regress = DataLoader(small_regression_test_data, batch_size=100, shuffle=True)\n",
    "medium_train_loader_regress = DataLoader(medium_regression_train_data, batch_size=100, shuffle=True)\n",
    "medium_test_loader_regress = DataLoader(medium_regression_test_data, batch_size=100, shuffle=True)\n",
    "large_train_loader_regress = DataLoader(large_regression_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_regress = DataLoader(large_regression_test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class MediumClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class LargeClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class SmallRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class MediumRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "class LargeRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_class = SmallClassifyNetwork().to(device)\n",
    "medium_model_class = MediumClassifyNetwork().to(device)\n",
    "large_model_class = LargeClassifyNetwork().to(device)\n",
    "small_model_regress = SmallRegressNetwork().to(device)\n",
    "medium_model_regress = MediumRegressNetwork().to(device)\n",
    "large_model_regress = LargeRegressNetwork().to(device)\n",
    "\n",
    "loss_fn_class = nn.BCEWithLogitsLoss()\n",
    "optimizer_class = torch.optim.SGD(small_model_class.parameters(), lr=1e-2)\n",
    "loss_fn_regress = nn.MSELoss()\n",
    "optimizer_regress = torch.optim.SGD(small_model_regress.parameters(), lr=1e-4)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.unsqueeze(1)).item()\n",
    "            #pred = (pred > 0.5).type(torch.float)\n",
    "            #correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    #correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallRegressNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state = copy.deepcopy(small_model_regress.cpu().state_dict())\n",
    "old_state = copy.deepcopy(small_model_regress.cpu().state_dict())\n",
    "for key in new_state:\n",
    "    new_state[key] = old_state[key] / 1.01\n",
    "small_model_regress.load_state_dict(new_state)\n",
    "small_model_regress.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 30146.018750 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 30106.929688 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 29882.612109 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 13410.913477 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 338.706200 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 215.887773 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 179.178612 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 140.162143 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 120.542454 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 112.949309 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 96.933901 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 90.623328 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 82.986713 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 76.822636 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 71.550603 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 66.529853 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 64.703688 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 59.072357 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 57.602565 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 53.711926 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 51.793766 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 51.013853 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 48.702779 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 48.720209 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 46.462711 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 46.597377 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 44.193616 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 44.187329 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 43.228156 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 42.701597 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(small_train_loader_regress, small_model_regress, loss_fn_regress, optimizer_regress)\n",
    "    test(small_test_loader_regress, small_model_regress, loss_fn_regress)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_model = copy.deepcopy(small_model_regress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallRegressNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state = copy.deepcopy(starting_model.cpu().state_dict())\n",
    "old_state = copy.deepcopy(starting_model.cpu().state_dict())\n",
    "for key in new_state:\n",
    "    new_state[key] = old_state[key] - 0.3\n",
    "starting_model.load_state_dict(new_state)\n",
    "starting_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 33.959185 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11382033.150000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(small_test_loader_regress, small_model_regress, loss_fn_regress)\n",
    "test(small_test_loader_regress, starting_model, loss_fn_regress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallRegressNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): SiLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_pre = copy.deepcopy(small_model_regress.cpu().state_dict())\n",
    "small_model_regress.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallRegressNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): SiLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_post = copy.deepcopy(small_model_regress.cpu().state_dict())\n",
    "small_model_regress.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0269,  0.0855, -0.0366,  ..., -0.0624,  0.0195,  0.0873],\n",
       "                      [-0.0462, -0.0309,  0.0757,  ...,  0.0316,  0.0810,  0.0297],\n",
       "                      [-0.0913, -0.0127,  0.0544,  ...,  0.0541, -0.0488,  0.0258],\n",
       "                      ...,\n",
       "                      [-0.0261, -0.0509, -0.0671,  ..., -0.0918, -0.0277, -0.0343],\n",
       "                      [ 0.0474, -0.0255,  0.0784,  ...,  0.0198, -0.0280, -0.0056],\n",
       "                      [ 0.0229, -0.0514, -0.0139,  ...,  0.0608, -0.0459,  0.0043]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([-0.0315,  0.0042, -0.0359,  0.0179,  0.0234,  0.0774, -0.0863,  0.0407,\n",
       "                       0.0481,  0.0927,  0.0183, -0.0791, -0.0192, -0.0839, -0.0871,  0.0038,\n",
       "                      -0.0293,  0.0635,  0.0833,  0.0282, -0.0052,  0.0089, -0.0683, -0.0612,\n",
       "                       0.0827, -0.0714, -0.0426,  0.0230,  0.0338,  0.0857,  0.0132,  0.0294,\n",
       "                      -0.0213, -0.0261,  0.0980, -0.0352,  0.0084,  0.0689, -0.0807,  0.0222,\n",
       "                      -0.0903,  0.0291,  0.0256, -0.0156,  0.0144,  0.0612, -0.0216,  0.0685,\n",
       "                       0.0041, -0.0977, -0.0313, -0.0100,  0.0549, -0.0537, -0.0123,  0.0731,\n",
       "                       0.0457, -0.0280, -0.0140,  0.0899,  0.0896,  0.0355, -0.0391,  0.0514,\n",
       "                       0.0880, -0.0722, -0.0054,  0.0995, -0.0639,  0.0107,  0.0149,  0.0478,\n",
       "                       0.0171, -0.0354, -0.0074, -0.0717, -0.0558,  0.0248, -0.0473, -0.0602,\n",
       "                      -0.0940,  0.0965,  0.0190, -0.0596, -0.0925,  0.0301, -0.0464,  0.0271,\n",
       "                       0.0249, -0.0259,  0.0586,  0.0295,  0.0931,  0.0507,  0.0288, -0.0471,\n",
       "                      -0.0059, -0.0981, -0.0356, -0.0944, -0.0150, -0.0540, -0.0659, -0.0050,\n",
       "                       0.0955, -0.0695,  0.0010, -0.0117,  0.0590, -0.0124,  0.0949,  0.0073,\n",
       "                      -0.0936,  0.0510,  0.0499,  0.0123,  0.0977, -0.0714,  0.0576,  0.0829,\n",
       "                      -0.0393,  0.0197,  0.0655,  0.0213,  0.0365,  0.0348, -0.0966,  0.0126])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.0217, -0.0368,  0.0084,  ...,  0.0419, -0.0297, -0.0065],\n",
       "                      [-0.0216,  0.0130, -0.0695,  ..., -0.0442, -0.0131, -0.0848],\n",
       "                      [ 0.0845, -0.0658,  0.0069,  ...,  0.0631,  0.0824, -0.0575],\n",
       "                      ...,\n",
       "                      [ 0.0229, -0.0467, -0.0881,  ..., -0.0268, -0.0155, -0.0869],\n",
       "                      [ 0.0560, -0.0477,  0.0697,  ...,  0.0100, -0.0364, -0.0556],\n",
       "                      [ 0.0705, -0.0298, -0.0529,  ...,  0.0131, -0.0694, -0.0029]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 0.0666, -0.0457, -0.0742,  0.0354,  0.0329, -0.0556,  0.0235,  0.0510,\n",
       "                       0.0124,  0.0827, -0.0304,  0.0359, -0.0406,  0.0634, -0.0038, -0.0758,\n",
       "                       0.0558,  0.0748,  0.0205,  0.0578, -0.0626,  0.0808, -0.0236, -0.0491,\n",
       "                      -0.0735,  0.0198, -0.0403, -0.0143,  0.0179,  0.0366,  0.0640, -0.0623,\n",
       "                      -0.0679,  0.0794, -0.0839, -0.0275,  0.0009,  0.0607, -0.0015, -0.0754,\n",
       "                      -0.0403, -0.0181, -0.0050,  0.0520, -0.0845, -0.0594, -0.0194, -0.0607,\n",
       "                       0.0422, -0.0235, -0.0212,  0.0176, -0.0103, -0.0495,  0.0008, -0.0087,\n",
       "                       0.0015,  0.0878, -0.0496,  0.0582, -0.0086, -0.0091,  0.0676,  0.0591,\n",
       "                      -0.0793,  0.0188, -0.0163,  0.0772, -0.0863, -0.0282, -0.0619, -0.0695,\n",
       "                       0.0482,  0.0194, -0.0359,  0.0672,  0.0031, -0.0868,  0.0613, -0.0576,\n",
       "                      -0.0685,  0.0859,  0.0855,  0.0814,  0.0123,  0.0695, -0.0618,  0.0169,\n",
       "                      -0.0208,  0.0563, -0.0669, -0.0192,  0.0620, -0.0184,  0.0149, -0.0243,\n",
       "                       0.0538,  0.0728, -0.0085,  0.0037, -0.0192,  0.0362,  0.0659, -0.0812,\n",
       "                       0.0380, -0.0558, -0.0452,  0.0544, -0.0031, -0.0469, -0.0786,  0.0377,\n",
       "                       0.0541,  0.0489,  0.0031,  0.0432,  0.0705, -0.0335,  0.0468,  0.0550,\n",
       "                       0.0363,  0.0117,  0.0280,  0.0072, -0.0226,  0.0683, -0.0691, -0.0005])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[-0.0565, -0.0099,  0.0096,  0.0639,  0.0716,  0.0802, -0.0246,  0.0262,\n",
       "                        0.0247,  0.0022,  0.0212, -0.0296,  0.0040, -0.0839, -0.0335,  0.0124,\n",
       "                        0.0415,  0.0209, -0.0427, -0.0134, -0.0821, -0.0782, -0.0486,  0.0884,\n",
       "                        0.0427, -0.0701, -0.0393, -0.0578,  0.0158, -0.0236,  0.0120, -0.0796,\n",
       "                        0.0432,  0.0613, -0.0300, -0.0627,  0.0854,  0.0414,  0.0799,  0.0631,\n",
       "                        0.0387, -0.0712, -0.0629, -0.0467, -0.0177, -0.0428, -0.0703, -0.0005,\n",
       "                        0.0824, -0.0106,  0.0798, -0.0159, -0.0763, -0.0585, -0.0459, -0.0435,\n",
       "                       -0.0617,  0.0088, -0.0586, -0.0798, -0.0172,  0.0181,  0.0860, -0.0411,\n",
       "                        0.0047, -0.0038,  0.0278,  0.0290,  0.0127, -0.0789, -0.0414,  0.0317,\n",
       "                       -0.0327, -0.0533, -0.0127,  0.0824, -0.0616, -0.0865, -0.0380, -0.0880,\n",
       "                        0.0096, -0.0320,  0.0026, -0.0299,  0.0183,  0.0211,  0.0356,  0.0305,\n",
       "                       -0.0189,  0.0425, -0.0423,  0.0626, -0.0491, -0.0351, -0.0543, -0.0700,\n",
       "                        0.0134, -0.0175, -0.0481,  0.0068,  0.0118,  0.0138, -0.0778, -0.0823,\n",
       "                       -0.0246, -0.0599,  0.0759,  0.0214,  0.0023, -0.0104, -0.0159,  0.0544,\n",
       "                        0.0431, -0.0256, -0.0053, -0.0065, -0.0351, -0.0448,  0.0274,  0.0441,\n",
       "                        0.0687,  0.0125,  0.0820,  0.0159, -0.0114,  0.0160,  0.0286,  0.0118]])),\n",
       "             ('linear_relu_stack.4.bias', tensor([-0.0293]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0555,  0.0757, -0.0405,  ..., -0.1647,  0.0397,  0.0885],\n",
       "                      [-0.0461, -0.0401,  0.0779,  ...,  0.0847,  0.0869,  0.0358],\n",
       "                      [-0.1130, -0.0118,  0.0413,  ...,  0.2146, -0.0571,  0.0036],\n",
       "                      ...,\n",
       "                      [-0.0271, -0.0417, -0.0593,  ..., -0.2491, -0.0336, -0.0575],\n",
       "                      [ 0.0455, -0.0305,  0.0852,  ...,  0.0165, -0.0285, -0.0101],\n",
       "                      [ 0.0447, -0.0851,  0.0169,  ...,  0.4125, -0.0506, -0.0280]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 0.0969,  0.0384,  0.0631,  0.0758,  0.0817,  0.0890,  0.0075,  0.0869,\n",
       "                       0.1572,  0.2375,  0.1454,  0.0280,  0.0793, -0.1137, -0.0150,  0.1430,\n",
       "                      -0.0178,  0.1633,  0.1670,  0.1150, -0.0244,  0.1199,  0.0109,  0.0050,\n",
       "                       0.0960, -0.0972, -0.0230,  0.0812,  0.1358,  0.2691,  0.1016,  0.0864,\n",
       "                       0.0606,  0.0090,  0.1187, -0.1137,  0.0190,  0.1037, -0.0660,  0.0358,\n",
       "                      -0.0419,  0.1243,  0.0077,  0.1148,  0.1310,  0.1974,  0.0358,  0.0752,\n",
       "                       0.0787, -0.0047,  0.0320, -0.0180,  0.1217,  0.0276,  0.0944,  0.1576,\n",
       "                       0.1145,  0.1039, -0.0110,  0.0886,  0.0599,  0.1147,  0.0193,  0.1753,\n",
       "                       0.1785, -0.0094,  0.0268,  0.2160,  0.0155,  0.0875,  0.0884,  0.1293,\n",
       "                       0.0561,  0.0631,  0.0487, -0.0078,  0.0019,  0.1670,  0.0521,  0.0153,\n",
       "                      -0.0893,  0.2670,  0.1609,  0.0606, -0.0736,  0.1418,  0.0506,  0.1101,\n",
       "                       0.0997,  0.0194,  0.1422,  0.1157,  0.1571,  0.1468,  0.0781,  0.0422,\n",
       "                       0.0705, -0.0212,  0.0630, -0.0394,  0.0106, -0.1641, -0.0103,  0.0990,\n",
       "                       0.1348,  0.0573,  0.0699,  0.0637,  0.1413,  0.0369,  0.2185,  0.1003,\n",
       "                      -0.0483,  0.1420,  0.1352,  0.0868,  0.1321,  0.0028,  0.1576,  0.1504,\n",
       "                       0.0023,  0.0776,  0.1346,  0.0978,  0.1291,  0.1549, -0.0651,  0.0455])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.1340, -0.0173,  0.0284,  ...,  0.1634,  0.0110, -0.0110],\n",
       "                      [-0.0142,  0.0118, -0.0707,  ..., -0.0362, -0.0117, -0.0899],\n",
       "                      [ 0.0833, -0.0654,  0.0075,  ...,  0.0623,  0.0823, -0.0550],\n",
       "                      ...,\n",
       "                      [ 0.0381, -0.0511, -0.0913,  ..., -0.0087, -0.0129, -0.0931],\n",
       "                      [ 0.0505, -0.0450,  0.0736,  ...,  0.0054, -0.0365, -0.0410],\n",
       "                      [ 0.0720, -0.0298, -0.0533,  ...,  0.0152, -0.0690, -0.0040]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 0.2959, -0.0391, -0.0744,  0.0466,  0.2187, -0.0527,  0.1529,  0.0532,\n",
       "                       0.0098,  0.0929, -0.0306,  0.1952, -0.0364,  0.2083, -0.0046, -0.0467,\n",
       "                       0.0792,  0.1287,  0.0325,  0.0708,  0.2477,  0.3033, -0.0185, -0.0446,\n",
       "                      -0.0781,  0.2300, -0.0350, -0.0163,  0.0178,  0.0351,  0.0733, -0.0176,\n",
       "                       0.0112,  0.2466, -0.0844,  0.0186,  0.0469,  0.0598,  0.0559, -0.0768,\n",
       "                      -0.0327,  0.1599,  0.0148,  0.0609, -0.0769, -0.0597,  0.0145, -0.0527,\n",
       "                       0.1898, -0.0129,  0.2301,  0.0273,  0.0401, -0.0469,  0.0124, -0.0095,\n",
       "                       0.0250,  0.0886, -0.0323,  0.0793,  0.0472,  0.1441,  0.1438,  0.1785,\n",
       "                      -0.0790,  0.0622, -0.0172,  0.1700, -0.0785,  0.0494, -0.0464, -0.0680,\n",
       "                       0.0477,  0.0180, -0.0362,  0.1047,  0.0291, -0.0640,  0.0627,  0.2484,\n",
       "                      -0.0611,  0.1576,  0.1033,  0.0832,  0.0540,  0.1233,  0.2035,  0.0121,\n",
       "                      -0.0216,  0.0568, -0.0324,  0.0358,  0.0725, -0.0092,  0.0132, -0.0184,\n",
       "                       0.0563,  0.0916,  0.3702,  0.0070, -0.0160,  0.0366,  0.0629,  0.0683,\n",
       "                       0.0414,  0.0426, -0.0460,  0.0572, -0.0032, -0.0435, -0.0783,  0.0421,\n",
       "                       0.1198,  0.0935,  0.0051,  0.0458,  0.0950, -0.0234,  0.0468,  0.0597,\n",
       "                       0.3449,  0.0114,  0.1736,  0.0071,  0.0700,  0.0828, -0.0701,  0.0016])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[-1.5139e+00, -1.2766e-01,  3.5581e-02,  3.3631e-01,  1.4307e+00,\n",
       "                        1.1340e-01, -8.9073e-01,  8.6756e-02,  3.4594e-01, -3.1615e-01,\n",
       "                        2.1108e-03, -1.1078e+00, -8.4102e-02, -1.0490e+00, -2.1709e-04,\n",
       "                        6.7469e-01,  5.4349e-01,  7.4307e-01, -2.1540e-01, -2.3468e-01,\n",
       "                       -1.9805e+00, -1.4227e+00, -8.6975e-02,  3.4271e-01,  3.4701e-01,\n",
       "                       -1.3340e+00, -1.1838e-01,  2.6532e-03,  4.0219e-01,  4.6082e-02,\n",
       "                       -1.2484e-01, -5.8014e-01,  9.0099e-01,  1.3240e+00, -2.3432e-03,\n",
       "                       -6.0032e-01,  7.7005e-01,  2.3702e-02,  7.1276e-01,  6.6629e-02,\n",
       "                        3.8372e-01, -1.1657e+00, -4.2274e-01, -1.3516e-01, -1.1654e-01,\n",
       "                       -2.1762e-02, -5.8042e-01, -1.4526e-01,  1.0605e+00, -1.6519e-01,\n",
       "                        1.9480e+00, -3.4736e-01, -6.2938e-01, -1.5594e-01, -1.9579e-01,\n",
       "                       -7.6534e-03, -4.4198e-01, -2.0957e-02, -4.4144e-01, -3.2462e-01,\n",
       "                       -6.3673e-01,  1.1616e+00,  7.8467e-01,  1.1494e+00, -3.6527e-03,\n",
       "                       -5.0541e-01,  7.5033e-02,  9.8649e-01, -9.5127e-02, -7.3453e-01,\n",
       "                       -3.8581e-01, -3.4454e-02,  6.0506e-02, -1.1270e-02,  6.0183e-03,\n",
       "                        6.9704e-01, -5.3433e-01, -4.2746e-01, -4.2587e-02, -1.8349e+00,\n",
       "                       -2.0177e-01, -7.5212e-01, -3.9758e-01, -5.2928e-02,  6.4254e-01,\n",
       "                        6.9871e-01,  1.9108e+00,  2.9421e-01,  3.7303e-02, -2.2994e-02,\n",
       "                       -5.7760e-01,  8.2542e-01, -2.6099e-01,  4.8623e-01,  5.8498e-02,\n",
       "                       -2.0037e-01,  3.4953e-01, -3.5399e-01, -2.3271e+00,  3.4633e-01,\n",
       "                       -6.2702e-02,  2.6410e-01, -8.1341e-04, -1.0916e+00,  2.7246e-01,\n",
       "                       -8.4464e-01,  1.1586e-01, -2.0292e-01,  2.2314e-01,  1.1605e-01,\n",
       "                        7.0922e-02,  2.3983e-01,  8.5055e-01, -6.0362e-01, -7.2597e-02,\n",
       "                        9.1285e-02, -4.1816e-01, -1.3498e-01,  9.9890e-02,  3.2472e-01,\n",
       "                        2.5121e+00,  1.9406e-02,  1.1990e+00,  4.3621e-03, -8.6291e-01,\n",
       "                       -4.4960e-01,  1.1930e-01, -4.6031e-02]])),\n",
       "             ('linear_relu_stack.4.bias', tensor([-0.0852]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'large_model_post' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mean_pre_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(large_model_pre[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m----> 2\u001b[0m mean_post_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mlarge_model_post\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      3\u001b[0m std_pre_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(large_model_pre[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      4\u001b[0m std_post_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(large_model_post[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_relu_stack.0.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'large_model_post' is not defined"
     ]
    }
   ],
   "source": [
    "mean_pre_0 = np.mean(large_model_pre[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_post_0 = np.mean(large_model_post[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_pre_0 = np.std(large_model_pre[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_post_0 = np.std(large_model_post[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_pre_1 = np.mean(large_model_pre[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_post_1 = np.mean(large_model_post[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_pre_1 = np.std(large_model_pre[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_post_1 = np.std(large_model_post[\"linear_relu_stack.2.weight\"].numpy())\n",
    "print(\"Pre: \")\n",
    "print(\" Layer 1: \" + str(mean_pre_0) + \", \" + str(std_pre_0))\n",
    "print(\" Layer 2: \" + str(mean_pre_1) + \", \" + str(std_pre_1))\n",
    "print(\"Post: \")\n",
    "print(\" Layer 1: \" + str(mean_post_0) + \", \" + str(std_post_0))\n",
    "print(\" Layer 2: \" + str(mean_post_1) + \", \" + str(std_post_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "for item in large_model_post:\n",
    "    if item[20:] == \"weight\":\n",
    "        table = pd.DataFrame(large_model_post[item])\n",
    "        table.to_csv(\"results/relu_silu_small_regress_weight_post_\" + str(ID) + \".csv\",index=False)\n",
    "    if item[20:] == \"bias\":\n",
    "        series = pd.Series(large_model_post[item])\n",
    "        series.to_csv(\"results/relu_silu_small_regress_bias_post_\" + str(ID) + \".csv\",index=False)\n",
    "    ID += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "for item in large_model_pre:\n",
    "    if item[20:] == \"weight\":\n",
    "        table = pd.DataFrame(large_model_pre[item])\n",
    "        table.to_csv(\"results/relu_silu_small_regress_weight_pre_\" + str(ID) + \".csv\",index=False)\n",
    "    if item[20:] == \"bias\":\n",
    "        series = pd.Series(large_model_pre[item])\n",
    "        series.to_csv(\"results/relu_silu_small_regress_bias_pre_\" + str(ID) + \".csv\",index=False)\n",
    "    ID += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the newer experiments, it seems that Binary Classification partialy follows what was found with MNIST, and Regression doesn't really follow at all. For regression this makes sense, as the network is trying to predict a value along all real numbers as oposed to some set of choices. For Binary classification, it has always been close, but not really solid. There were times where it did follow MNIST, and other times where it didn't follow at all. It could be possible the data in the images in MNIST are different than the generated sets, but i have no idea at this point on how to characterize the MNIST set to be similiar with sklearn's make_classification. Have not yet tested the make_multiclass_classification, and have not yet dived into the Heatmaps of weights for the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_multiclass_problem = make_multilabel_classification(n_samples=25000, n_features=500, n_classes=10, n_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_multiclass_train_data = GeneratedDataset(large_multiclass_problem[0][int(len(large_multiclass_problem[0])*.2):], large_multiclass_problem[1][int(len(large_multiclass_problem[1])*.2):])\n",
    "large_multiclass_test_data = GeneratedDataset(large_multiclass_problem[0][:int(len(large_multiclass_problem[0])*.2)], large_multiclass_problem[1][:int(len(large_multiclass_problem[1])*.2)])\n",
    "large_train_loader_multiclass = DataLoader(large_multiclass_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_multiclass = DataLoader(large_multiclass_test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeMulticlassNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "\n",
    "model = LargeMulticlassNetwork().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimize = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # pred = (pred > 0.5).type(torch.float)\n",
    "            # correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.441471 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.449458 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.431501 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.441189 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.432633 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.425189 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.433316 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.418330 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.416986 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.409183 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.402614 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.394229 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.364168 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.339444 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.247414 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.172838 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4.307687 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.970508 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.900927 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.853474 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.873333 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.765273 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.689242 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.681034 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.615214 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.624922 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.587897 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.611373 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.570010 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3.535191 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LargeMulticlassNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=512, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_multiclass = copy.deepcopy(model.cpu().state_dict())\n",
    "model.to(device)\n",
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(large_train_loader_multiclass, model, loss, optimize)\n",
    "    test(large_test_loader_multiclass, model, loss)\n",
    "print(\"Done!\")\n",
    "post_trained_multiclass = copy.deepcopy(model.cpu().state_dict())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0442, -0.0267, -0.0142,  ..., -0.0404,  0.0069, -0.0183],\n",
       "                      [-0.0080, -0.0214,  0.0314,  ...,  0.0024,  0.0401,  0.0208],\n",
       "                      [ 0.0328,  0.0305,  0.0125,  ..., -0.0054,  0.0332,  0.0372],\n",
       "                      ...,\n",
       "                      [ 0.0295,  0.0069,  0.0092,  ..., -0.0432,  0.0014,  0.0428],\n",
       "                      [-0.0401, -0.0193, -0.0308,  ...,  0.0003, -0.0158,  0.0322],\n",
       "                      [-0.0280,  0.0170,  0.0132,  ..., -0.0377, -0.0099, -0.0394]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 4.2919e-02, -1.5841e-02, -1.6990e-02,  3.6192e-02, -2.4385e-02,\n",
       "                       9.3218e-03, -3.9001e-02,  2.3250e-02,  9.7217e-03,  4.5774e-04,\n",
       "                       3.9148e-02,  1.4828e-02,  1.2344e-02,  3.8993e-02, -4.0246e-02,\n",
       "                      -1.2096e-03, -2.7187e-02, -3.4861e-02,  2.7642e-02,  4.5555e-03,\n",
       "                      -1.9925e-03, -1.5783e-02, -1.4978e-02, -4.4663e-02,  2.4437e-02,\n",
       "                      -1.5667e-02,  1.3683e-02, -2.8462e-02,  3.6868e-02,  2.3563e-02,\n",
       "                       1.1429e-02,  1.1516e-02,  2.6937e-02,  6.4324e-03, -2.5738e-02,\n",
       "                      -1.4432e-02,  8.3003e-04, -3.0492e-02,  4.3186e-03, -2.2549e-03,\n",
       "                      -3.2131e-02, -2.4438e-02, -1.0092e-02, -1.9646e-02, -3.7702e-02,\n",
       "                       4.0567e-02,  4.3226e-02, -2.5316e-02, -2.9232e-02,  2.6409e-02,\n",
       "                      -4.2559e-02,  4.0492e-02, -2.3125e-02, -2.2076e-02,  4.2645e-02,\n",
       "                      -3.0410e-02,  3.5413e-02, -7.3846e-04, -4.1316e-02, -1.2369e-02,\n",
       "                       5.4307e-03, -2.8943e-02,  4.1284e-02, -3.7023e-02,  2.2492e-02,\n",
       "                      -3.0297e-02,  4.4070e-02,  1.8783e-02,  3.5993e-02, -2.4614e-02,\n",
       "                       3.3787e-02, -1.1708e-02,  2.0215e-02, -9.5376e-03,  3.9534e-03,\n",
       "                      -3.6681e-02,  1.0901e-02, -2.6258e-02, -1.5801e-02,  1.7827e-02,\n",
       "                      -3.4527e-02,  2.8176e-02,  2.0573e-02,  3.9996e-02, -2.9532e-02,\n",
       "                       2.2322e-02, -1.3402e-02, -4.3952e-02,  2.1299e-02, -3.7516e-02,\n",
       "                      -8.4000e-03, -2.6488e-02,  2.2765e-02,  9.2786e-03, -3.9571e-02,\n",
       "                      -4.2792e-03,  1.8525e-02,  1.0290e-02, -1.0074e-02, -2.5316e-02,\n",
       "                       4.2359e-02,  1.3514e-02, -1.9487e-02,  1.7086e-02, -2.5341e-02,\n",
       "                       3.8779e-04,  2.9961e-02, -1.8712e-02,  1.7061e-02, -3.4872e-02,\n",
       "                       3.5349e-02,  4.1470e-02,  2.5135e-02, -1.8869e-02,  2.6922e-02,\n",
       "                       2.5502e-02,  1.7437e-02, -4.3251e-02,  3.1088e-02,  3.6938e-02,\n",
       "                      -9.2920e-03, -6.3694e-03, -3.9495e-02,  6.7332e-04,  1.6638e-02,\n",
       "                      -2.8597e-02,  5.7137e-04, -3.2410e-02, -1.9607e-02, -3.0811e-02,\n",
       "                       3.1279e-03,  1.7080e-02,  1.9491e-03, -2.4371e-02, -3.4631e-02,\n",
       "                      -3.0162e-02,  3.7304e-02,  2.7698e-02, -2.9097e-02,  2.7587e-02,\n",
       "                       1.9266e-02,  3.5160e-02,  4.3132e-02,  9.7055e-03, -3.3284e-02,\n",
       "                       2.5808e-02,  4.2663e-02,  5.3936e-03,  3.0570e-02, -3.2827e-02,\n",
       "                       1.7493e-02, -1.1049e-02,  3.8063e-02,  2.1314e-02, -3.5826e-03,\n",
       "                       2.7030e-02, -4.0066e-03, -1.8182e-03, -2.3191e-02,  5.1671e-03,\n",
       "                       3.9179e-02,  1.4990e-02,  2.5045e-02, -3.2122e-02,  3.0938e-02,\n",
       "                      -1.7865e-02,  1.7510e-02, -3.0486e-02, -2.8647e-02, -3.2022e-02,\n",
       "                      -3.3439e-02, -3.5878e-02,  2.1962e-02, -3.8739e-02, -4.0597e-02,\n",
       "                       7.1588e-03,  2.0890e-02,  9.7208e-03,  7.3897e-03, -1.5755e-02,\n",
       "                       1.6791e-02, -4.0454e-02,  3.1809e-02,  4.0667e-02,  2.1450e-03,\n",
       "                      -1.6465e-02,  6.6190e-03,  6.9814e-03, -5.7527e-03, -4.2159e-02,\n",
       "                      -3.9608e-02, -2.1316e-02, -4.1696e-02,  1.1263e-02,  3.0121e-02,\n",
       "                       1.1055e-02, -5.3364e-04,  4.2215e-02,  2.7819e-02,  3.6696e-02,\n",
       "                      -3.4681e-03, -3.1591e-03,  3.9385e-02,  4.0099e-02,  1.9089e-02,\n",
       "                      -2.3736e-02, -1.9680e-03,  1.5087e-02,  3.1345e-02,  7.4309e-03,\n",
       "                      -1.2527e-02,  4.8264e-03,  3.0957e-02,  4.0005e-02, -3.4140e-02,\n",
       "                       1.3837e-02,  2.1858e-02, -6.7481e-03,  3.6432e-02,  3.6533e-03,\n",
       "                      -2.6390e-02, -1.6164e-02, -4.4794e-03,  3.4618e-02,  3.3310e-03,\n",
       "                       4.4632e-02,  1.4539e-02,  2.4907e-02, -2.0921e-02, -8.1949e-03,\n",
       "                      -2.2362e-03, -2.6006e-02, -1.0870e-02,  2.9540e-02, -3.7819e-02,\n",
       "                       2.2243e-02,  3.3581e-02, -3.1793e-02,  3.4690e-02, -5.5975e-03,\n",
       "                      -2.1055e-02,  1.4616e-02,  2.2538e-02,  7.7122e-03,  1.5132e-04,\n",
       "                       1.7453e-02, -3.1845e-03, -1.4725e-02, -7.0839e-03, -7.7325e-04,\n",
       "                      -2.0728e-02,  5.0794e-03, -3.6899e-02,  1.1463e-02, -2.1333e-03,\n",
       "                      -1.9512e-02, -2.8665e-02,  3.6995e-02,  7.1451e-03, -1.6025e-02,\n",
       "                       2.9667e-02, -9.0934e-03, -3.0711e-02,  2.6737e-02,  3.1398e-03,\n",
       "                      -1.2134e-02, -1.7778e-02, -2.9033e-02,  2.6522e-02, -1.8482e-02,\n",
       "                       6.3252e-03,  4.7878e-03, -3.9361e-02,  2.1052e-02, -1.0538e-03,\n",
       "                       4.7135e-04,  9.2799e-03,  3.3807e-03,  1.2948e-02,  2.4776e-02,\n",
       "                      -3.2196e-02, -4.4745e-03,  4.0036e-02, -3.2269e-02,  8.4117e-03,\n",
       "                       3.7415e-02,  2.6734e-02, -3.6336e-02,  5.1802e-03,  3.6500e-02,\n",
       "                       1.5014e-02, -1.1141e-02, -1.2126e-02,  4.1024e-02, -9.1174e-03,\n",
       "                       1.0423e-02,  2.6420e-02,  2.9675e-02,  3.6432e-02, -2.7257e-02,\n",
       "                       2.4060e-02, -3.7234e-02, -2.3114e-02,  2.3158e-03, -8.1141e-05,\n",
       "                      -2.2075e-02,  9.4981e-03, -3.3749e-02, -2.5561e-02,  4.2973e-02,\n",
       "                      -1.0284e-02,  2.7228e-02,  2.0740e-02, -3.8514e-02, -4.8146e-03,\n",
       "                       3.5118e-02,  3.3850e-02, -6.4603e-03,  2.2083e-02, -1.3166e-02,\n",
       "                       1.6354e-04,  4.3696e-02, -3.8667e-02,  3.9255e-02,  3.0686e-02,\n",
       "                       2.3177e-02, -2.7718e-02, -1.4331e-02,  2.5791e-02, -7.5083e-03,\n",
       "                       3.5781e-02,  1.8480e-02, -2.0134e-02, -3.8802e-02,  1.1074e-02,\n",
       "                      -3.3912e-02,  4.2291e-02,  3.2608e-03,  3.2025e-02,  5.7109e-05,\n",
       "                       2.8834e-02,  3.0357e-02,  1.2946e-02, -2.8580e-02,  5.3944e-03,\n",
       "                      -1.6753e-02,  1.2634e-02, -3.4618e-02, -2.0429e-02, -4.4215e-02,\n",
       "                       5.1954e-03,  1.1770e-02,  2.9606e-02, -1.3361e-02,  7.2780e-03,\n",
       "                      -6.4604e-03,  2.9159e-02, -1.3256e-02, -3.5466e-02,  1.2563e-02,\n",
       "                      -8.7818e-03, -1.0850e-02,  1.3771e-02, -4.3470e-02,  6.1031e-03,\n",
       "                      -4.3301e-02, -1.3212e-02,  1.0330e-02,  4.1972e-02,  3.2686e-02,\n",
       "                      -2.5274e-02,  2.5661e-02, -4.1330e-02,  3.5527e-02,  3.6354e-02,\n",
       "                      -2.7914e-02, -3.5491e-02, -3.3542e-02, -2.5462e-02, -2.2135e-02,\n",
       "                       2.5090e-02,  4.8668e-03, -4.1108e-02,  4.3589e-02, -6.7858e-03,\n",
       "                      -3.1859e-02, -3.5064e-03,  3.1647e-02,  2.2125e-03,  1.3724e-02,\n",
       "                      -2.1358e-02,  1.6079e-02, -3.8844e-02,  2.3398e-02,  1.4325e-02,\n",
       "                      -2.3102e-02, -2.3028e-02, -4.8190e-03,  2.2204e-02, -1.1203e-03,\n",
       "                      -2.0977e-02, -9.2610e-03, -5.9871e-04,  1.1788e-02, -3.0470e-02,\n",
       "                      -2.5715e-02, -4.1657e-02,  2.2117e-02,  3.4363e-02, -2.9430e-02,\n",
       "                       4.2899e-02,  2.7564e-02, -1.0463e-02,  2.7081e-02, -2.2238e-02,\n",
       "                      -1.8444e-02, -3.3858e-02,  3.8548e-02, -4.3726e-02, -4.0435e-02,\n",
       "                       2.6269e-02, -4.1063e-02,  1.1340e-02,  2.5527e-02,  2.0959e-02,\n",
       "                       1.5809e-03, -1.5378e-02,  3.4883e-02,  4.1344e-02,  3.7476e-02,\n",
       "                      -1.3932e-02,  2.4987e-02, -2.8856e-02,  2.4991e-02, -1.4385e-02,\n",
       "                      -3.1513e-02, -1.2340e-03,  2.2979e-02,  1.2414e-03,  1.8118e-02,\n",
       "                      -3.6073e-02, -5.3088e-04, -1.5426e-02,  4.2410e-02, -2.1601e-02,\n",
       "                       3.8026e-02,  4.3829e-02, -2.0694e-02,  2.4534e-02, -1.7281e-02,\n",
       "                       4.1231e-02, -2.6891e-02,  2.5363e-02,  1.7329e-02,  6.0904e-03,\n",
       "                      -1.3550e-02,  3.1440e-02, -1.8452e-02, -2.0341e-02, -2.4197e-02,\n",
       "                      -2.8669e-02,  6.9259e-03, -3.9963e-02, -3.1522e-02, -2.4140e-02,\n",
       "                       1.5685e-02, -3.2849e-02, -3.1740e-02, -3.4168e-02,  2.3480e-02,\n",
       "                      -2.0965e-02, -1.2151e-02, -3.4091e-02,  2.4980e-02,  9.8147e-03,\n",
       "                      -3.0488e-02,  9.9239e-03, -1.6309e-02, -1.5159e-02, -1.5041e-03,\n",
       "                       1.0409e-02,  2.6331e-02,  3.3297e-02,  3.7531e-02,  8.3748e-03,\n",
       "                       2.5474e-02, -1.7451e-02,  1.6494e-02, -3.7877e-02,  3.8508e-02,\n",
       "                       3.9447e-03,  4.3836e-02, -3.8693e-02, -2.6466e-02,  3.2550e-03,\n",
       "                      -2.5961e-02, -2.6318e-02, -1.2325e-02, -3.7317e-02,  2.2609e-02,\n",
       "                       6.2834e-04, -1.1728e-02,  4.0327e-02,  4.1645e-02, -4.0620e-02,\n",
       "                      -2.2461e-02, -4.0615e-02, -1.5436e-02, -2.7801e-02, -2.1813e-02,\n",
       "                       2.1569e-03, -4.0714e-02])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0256,  0.0010, -0.0174,  ...,  0.0165,  0.0378, -0.0139],\n",
       "                      [ 0.0030, -0.0202,  0.0408,  ...,  0.0286,  0.0323, -0.0223],\n",
       "                      [-0.0441, -0.0331,  0.0194,  ...,  0.0092, -0.0363, -0.0261],\n",
       "                      ...,\n",
       "                      [ 0.0014, -0.0069, -0.0060,  ...,  0.0023,  0.0175, -0.0419],\n",
       "                      [-0.0028,  0.0431, -0.0389,  ...,  0.0171,  0.0254,  0.0181],\n",
       "                      [-0.0344, -0.0100,  0.0259,  ..., -0.0105,  0.0129,  0.0189]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 6.2033e-03, -3.6454e-02,  2.4840e-02, -2.3864e-02, -3.1969e-02,\n",
       "                      -3.1136e-02,  3.5751e-02,  4.5672e-03, -4.1532e-02,  3.7931e-02,\n",
       "                      -4.1443e-02, -1.3818e-02, -3.7158e-03, -2.5977e-02,  1.9919e-02,\n",
       "                      -2.0179e-02,  1.1743e-02, -1.6106e-02, -3.7786e-02, -5.8146e-03,\n",
       "                      -2.0921e-02,  1.8075e-02,  3.3292e-02, -2.7041e-02, -1.7289e-02,\n",
       "                      -1.8372e-02,  1.0185e-02, -1.3901e-02, -3.7852e-02, -2.6518e-02,\n",
       "                       2.9819e-02, -2.2631e-02, -4.2205e-02, -2.3796e-02, -1.1658e-02,\n",
       "                      -2.7403e-03, -2.2736e-02,  3.4476e-02, -2.6470e-02, -4.1084e-02,\n",
       "                       1.9263e-02, -3.1561e-02, -3.8552e-02, -3.9993e-02,  2.5609e-02,\n",
       "                      -1.6175e-02, -3.9501e-02,  6.5688e-03,  3.2491e-02, -2.2135e-02,\n",
       "                      -1.3266e-02, -4.3471e-02,  4.2182e-02,  1.1393e-02, -9.3449e-03,\n",
       "                      -1.4078e-02,  2.8474e-02, -4.0110e-02,  1.7702e-02,  3.0147e-02,\n",
       "                      -2.1448e-03,  3.9763e-02, -5.4118e-03, -6.3866e-03,  9.9372e-03,\n",
       "                       2.2240e-02,  2.4133e-02, -1.0339e-02,  4.3277e-02,  2.7752e-02,\n",
       "                      -6.9769e-03, -2.3918e-02,  9.8319e-03, -3.5194e-02, -2.5265e-02,\n",
       "                       4.0292e-02, -2.4896e-02, -1.8491e-02,  1.5668e-02, -5.5232e-03,\n",
       "                       3.3664e-03, -2.1803e-02,  2.4222e-02,  1.5621e-03, -3.2835e-02,\n",
       "                       1.8487e-02,  3.1218e-02,  4.2126e-02,  3.0105e-03, -3.1263e-02,\n",
       "                       3.2268e-02,  2.4725e-02,  1.7561e-02,  3.5035e-02, -3.6859e-02,\n",
       "                       1.7509e-02,  3.3741e-02, -3.1527e-02, -2.4409e-02,  2.5611e-02,\n",
       "                       1.5623e-02, -4.3195e-02,  3.6609e-02,  1.9280e-02, -9.9381e-04,\n",
       "                       2.1039e-02, -3.9929e-02, -2.1669e-02,  2.1034e-02, -4.1365e-02,\n",
       "                      -1.9672e-02,  4.3977e-02, -7.2013e-03, -4.0071e-02, -2.6460e-02,\n",
       "                       4.0228e-02, -3.3410e-02, -3.0551e-02,  3.6291e-02, -1.7544e-02,\n",
       "                      -1.3964e-02,  2.5043e-02, -3.1431e-02,  4.3047e-02, -2.5323e-02,\n",
       "                      -1.7679e-02, -3.1617e-02, -9.7093e-03,  2.0165e-02,  4.3757e-02,\n",
       "                      -3.5220e-02,  1.5539e-02,  2.0077e-02, -2.4188e-02,  3.9338e-02,\n",
       "                       2.1934e-02,  2.3574e-02,  1.3812e-02, -4.0228e-02,  3.2230e-02,\n",
       "                       2.6740e-02, -2.9540e-02, -3.6990e-02,  2.0265e-02, -1.8293e-02,\n",
       "                      -3.1039e-02, -1.4023e-02, -3.2946e-03, -3.6064e-02, -7.6480e-04,\n",
       "                      -1.8280e-02, -2.3067e-02, -2.0000e-02,  3.0118e-02,  4.0151e-02,\n",
       "                      -2.4201e-02, -2.3983e-02, -3.8728e-02,  2.6660e-02, -2.1182e-02,\n",
       "                      -3.4128e-02,  1.4151e-02, -4.7820e-03,  1.3584e-02, -2.1897e-02,\n",
       "                      -4.1441e-02, -2.5371e-02, -2.3245e-03,  4.0731e-02, -5.5432e-04,\n",
       "                      -2.5350e-02, -2.5078e-02, -7.5411e-03,  9.4693e-04, -2.5524e-02,\n",
       "                       6.0427e-03, -2.5517e-02, -2.2149e-02, -7.2291e-03,  3.2129e-02,\n",
       "                       3.9876e-02,  1.1445e-02,  4.0360e-02, -2.7096e-02, -1.6440e-02,\n",
       "                      -1.7710e-02, -3.5369e-02, -2.4748e-02,  9.0148e-05,  2.6411e-02,\n",
       "                      -3.4152e-02,  2.7652e-02,  3.3392e-02, -3.7633e-02,  9.4756e-04,\n",
       "                      -1.5747e-03, -3.6267e-03,  6.7521e-03, -1.7051e-02, -2.8549e-02,\n",
       "                       4.6245e-04,  9.0299e-03,  4.2078e-02, -7.3356e-03, -1.4321e-02,\n",
       "                      -1.6048e-02, -3.9840e-02, -4.2772e-02, -1.4447e-02,  3.7584e-02,\n",
       "                      -1.1443e-03, -3.8152e-02, -1.2811e-02, -4.3828e-02, -3.9582e-02,\n",
       "                      -2.2855e-02, -6.2829e-03,  2.3375e-02, -1.1864e-02, -4.2742e-02,\n",
       "                      -3.2915e-02,  2.0263e-02,  1.5206e-02,  3.8419e-02, -4.2916e-02,\n",
       "                      -3.1959e-02, -3.2758e-02, -2.6452e-03, -1.9173e-02,  1.0084e-02,\n",
       "                       2.8580e-02,  2.9653e-02, -4.3661e-03, -1.4818e-02,  3.8734e-02,\n",
       "                      -2.1492e-02,  1.9090e-02, -1.5937e-02,  3.8854e-02,  3.3636e-02,\n",
       "                       2.6380e-02,  4.2184e-02,  2.1365e-02,  4.1200e-02,  6.9404e-04,\n",
       "                       1.7581e-02, -2.7826e-02, -2.9965e-02,  3.1629e-02,  2.6742e-02,\n",
       "                      -2.2913e-03, -1.2512e-02,  3.6464e-02, -3.6170e-02, -8.7660e-03,\n",
       "                       3.9268e-02, -4.9457e-04, -3.0544e-02, -1.0526e-02, -4.4018e-02,\n",
       "                      -1.6380e-02,  1.4597e-02,  3.6199e-02, -4.2220e-02, -4.0165e-02,\n",
       "                      -6.9954e-03,  2.0229e-02,  1.2524e-02, -1.1027e-02,  1.1747e-02,\n",
       "                       2.3912e-02,  3.9065e-02,  3.5779e-02,  2.7214e-02,  3.4514e-02,\n",
       "                      -1.5217e-02, -3.0314e-02,  1.7211e-02,  4.4081e-02, -2.7803e-02,\n",
       "                      -1.6058e-02,  3.2702e-02,  8.3409e-03, -3.0393e-03, -2.4910e-02,\n",
       "                      -3.0768e-02, -1.7979e-02,  5.1641e-03,  1.5313e-02, -5.2246e-03,\n",
       "                       3.4373e-02,  2.8719e-02, -4.2348e-03,  2.0141e-02,  1.2015e-03,\n",
       "                      -4.2326e-02, -1.0387e-02,  2.3848e-02, -2.9246e-02, -4.0365e-02,\n",
       "                      -3.9043e-02,  2.9051e-02,  1.8590e-02,  3.8999e-02, -1.0382e-02,\n",
       "                      -4.2601e-02, -4.1286e-02,  7.0312e-03, -3.0621e-03,  2.5145e-02,\n",
       "                      -3.4343e-02, -1.9705e-02, -8.0682e-03, -6.5501e-03,  2.1942e-02,\n",
       "                       7.0072e-04,  1.1047e-02, -5.4365e-03,  1.8751e-02, -2.9690e-02,\n",
       "                      -3.7400e-02,  1.7656e-02,  1.5130e-02,  8.7649e-03, -1.9781e-02,\n",
       "                       4.1289e-03,  4.0576e-02, -6.5696e-03, -3.7925e-02,  2.7019e-02,\n",
       "                       2.8550e-02,  2.4116e-02,  4.2587e-02, -1.6571e-02, -3.4307e-02,\n",
       "                       1.7989e-02,  2.8853e-02,  1.7492e-03,  2.2968e-02,  2.1538e-02,\n",
       "                      -4.2874e-02, -1.8878e-02,  8.0375e-04,  3.4803e-02,  6.7028e-03,\n",
       "                       2.2051e-02, -3.1635e-02, -1.0441e-02, -2.5631e-02,  3.4103e-02,\n",
       "                      -8.4086e-03,  2.9611e-02, -4.1284e-02,  1.6610e-02,  1.4786e-03,\n",
       "                       2.4627e-02,  1.0624e-02, -7.5199e-03,  2.7799e-02, -1.6320e-02,\n",
       "                      -4.0914e-02,  2.6218e-02,  2.8966e-02,  4.4234e-03,  3.0508e-02,\n",
       "                      -1.1787e-03, -4.0418e-02,  1.4005e-02, -2.6044e-02, -1.9852e-02,\n",
       "                      -1.2046e-02,  1.4320e-02,  1.2873e-02, -2.4248e-03, -2.7927e-02,\n",
       "                      -2.7561e-02, -1.6471e-02, -1.3622e-02, -1.3938e-03, -2.2441e-03,\n",
       "                       3.2373e-02,  3.8010e-02,  3.8571e-02, -1.5675e-03,  3.3987e-02,\n",
       "                       1.4656e-02,  7.2132e-04,  1.7321e-02,  3.3643e-03, -4.0623e-02,\n",
       "                       3.8659e-02, -3.2499e-02,  4.4068e-02,  1.7173e-02,  3.1392e-02,\n",
       "                      -3.8927e-02,  4.3004e-02,  1.3745e-02, -3.7865e-02,  2.3152e-02,\n",
       "                       2.0490e-02, -1.4676e-02, -1.3384e-02,  7.8231e-04, -1.1975e-02,\n",
       "                      -2.0788e-02,  2.3413e-02,  1.9510e-02, -1.5731e-02, -3.9309e-02,\n",
       "                      -2.5501e-04, -3.6751e-03,  4.1739e-04,  1.1103e-02, -6.5124e-03,\n",
       "                       1.8081e-02,  2.5861e-02,  1.9916e-02, -1.0695e-02,  7.0389e-03,\n",
       "                      -3.7878e-03, -9.0262e-03, -3.8327e-02,  3.9878e-02,  1.5522e-03,\n",
       "                       3.0163e-02, -4.3178e-02, -3.0371e-02,  3.1131e-02, -9.3988e-03,\n",
       "                      -3.5250e-02, -7.9107e-03,  4.3239e-02, -3.0932e-02, -7.7175e-03,\n",
       "                      -2.6006e-05, -1.8710e-02, -4.3663e-02,  3.2342e-02,  2.6736e-02,\n",
       "                      -2.2481e-02,  3.2487e-02, -2.5653e-02,  3.5485e-02,  3.5686e-02,\n",
       "                       3.8440e-02,  1.3515e-02,  2.8741e-02, -1.7213e-02, -2.3856e-02,\n",
       "                       3.6070e-02,  4.2841e-04, -2.7174e-02, -2.2725e-02, -1.4189e-02,\n",
       "                       4.1410e-02, -8.2058e-04, -6.1205e-03,  1.6040e-02,  2.5603e-02,\n",
       "                      -1.8715e-02, -1.4629e-02, -1.0982e-02, -3.6595e-02,  2.4020e-02,\n",
       "                      -2.6407e-04, -1.2782e-02,  2.6339e-02, -3.4681e-02, -1.3579e-02,\n",
       "                      -3.5085e-02,  1.5315e-02,  2.3951e-02,  3.9016e-03,  1.4659e-02,\n",
       "                      -1.1397e-02,  1.0259e-02, -1.5553e-02,  2.7056e-02, -1.6432e-02,\n",
       "                       3.3425e-02, -3.5855e-02, -2.1224e-02,  2.1337e-02,  3.7060e-03,\n",
       "                      -1.0146e-02,  2.4103e-02,  2.1835e-03,  1.3621e-02, -4.3992e-02,\n",
       "                      -3.2596e-02,  2.4500e-02,  3.7306e-03,  6.9662e-03, -3.5724e-02,\n",
       "                       3.6388e-02, -3.8721e-04, -1.0207e-02, -5.6509e-04,  2.0900e-02,\n",
       "                       3.7206e-02,  6.4011e-03,  1.7566e-02,  2.3906e-02,  2.2166e-02,\n",
       "                       2.0667e-02,  3.6967e-02,  2.8169e-02, -1.4140e-02, -2.7136e-02,\n",
       "                      -2.1473e-02, -7.8390e-03])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 0.0306,  0.0085,  0.0207,  ...,  0.0164,  0.0421,  0.0182],\n",
       "                      [ 0.0283, -0.0305, -0.0020,  ...,  0.0120,  0.0250, -0.0203],\n",
       "                      [-0.0155, -0.0098, -0.0252,  ...,  0.0438, -0.0066,  0.0102],\n",
       "                      ...,\n",
       "                      [ 0.0077,  0.0049,  0.0091,  ..., -0.0136, -0.0010,  0.0198],\n",
       "                      [ 0.0302, -0.0237, -0.0080,  ..., -0.0049,  0.0064,  0.0272],\n",
       "                      [ 0.0296, -0.0205,  0.0398,  ..., -0.0260,  0.0130, -0.0196]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.0300,  0.0015,  0.0090, -0.0390, -0.0266, -0.0429,  0.0337, -0.0287,\n",
       "                      -0.0435, -0.0272]))])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0418, -0.0115, -0.0255,  ..., -0.0429,  0.0054, -0.0207],\n",
       "                      [-0.0004, -0.0126,  0.0411,  ..., -0.0120,  0.0354,  0.0257],\n",
       "                      [ 0.0204,  0.0260,  0.0241,  ..., -0.0063,  0.0149,  0.0230],\n",
       "                      ...,\n",
       "                      [ 0.0067,  0.0153,  0.0120,  ..., -0.0384, -0.0111,  0.0280],\n",
       "                      [-0.0528, -0.0116, -0.0123,  ..., -0.0145, -0.0355,  0.0126],\n",
       "                      [-0.0372,  0.0035,  0.0203,  ..., -0.0230, -0.0341, -0.0667]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 0.0228, -0.0306, -0.0354,  0.0174, -0.0305, -0.0114, -0.0583, -0.0008,\n",
       "                      -0.0036, -0.0196,  0.0155,  0.0022,  0.0083,  0.0215, -0.0586, -0.0144,\n",
       "                      -0.0454, -0.0503,  0.0060, -0.0172, -0.0233, -0.0360, -0.0346, -0.0632,\n",
       "                       0.0055, -0.0293, -0.0087, -0.0359,  0.0125,  0.0031, -0.0110, -0.0035,\n",
       "                       0.0097, -0.0151, -0.0454, -0.0229, -0.0219, -0.0574, -0.0020, -0.0262,\n",
       "                      -0.0419, -0.0453, -0.0315, -0.0314, -0.0533,  0.0280,  0.0204, -0.0446,\n",
       "                      -0.0444,  0.0063, -0.0567,  0.0200, -0.0301, -0.0363,  0.0158, -0.0525,\n",
       "                       0.0179, -0.0216, -0.0526, -0.0317, -0.0154, -0.0430,  0.0217, -0.0529,\n",
       "                       0.0022, -0.0431,  0.0252,  0.0034,  0.0210, -0.0406,  0.0162, -0.0246,\n",
       "                       0.0009, -0.0330, -0.0205, -0.0507, -0.0068, -0.0356, -0.0376, -0.0014,\n",
       "                      -0.0525,  0.0095, -0.0031,  0.0293, -0.0422,  0.0105, -0.0348, -0.0605,\n",
       "                       0.0055, -0.0470, -0.0194, -0.0438,  0.0061, -0.0049, -0.0614, -0.0223,\n",
       "                      -0.0030, -0.0046, -0.0303, -0.0407,  0.0222, -0.0122, -0.0347, -0.0039,\n",
       "                      -0.0407, -0.0151,  0.0066, -0.0323, -0.0053, -0.0487,  0.0125,  0.0140,\n",
       "                      -0.0022, -0.0389,  0.0017,  0.0101,  0.0019, -0.0594,  0.0105,  0.0106,\n",
       "                      -0.0218, -0.0231, -0.0528, -0.0192, -0.0072, -0.0399, -0.0202, -0.0418,\n",
       "                      -0.0319, -0.0473, -0.0161,  0.0023, -0.0154, -0.0440, -0.0470, -0.0471,\n",
       "                       0.0144,  0.0168, -0.0487,  0.0118, -0.0023,  0.0230,  0.0157, -0.0097,\n",
       "                      -0.0520,  0.0061,  0.0188, -0.0059,  0.0105, -0.0534, -0.0008, -0.0283,\n",
       "                       0.0178, -0.0015, -0.0202,  0.0070, -0.0185, -0.0103, -0.0294, -0.0195,\n",
       "                       0.0267, -0.0070,  0.0059, -0.0374,  0.0164, -0.0431, -0.0072, -0.0324,\n",
       "                      -0.0417, -0.0471, -0.0534, -0.0522, -0.0041, -0.0567, -0.0604, -0.0108,\n",
       "                      -0.0051, -0.0084, -0.0125, -0.0287, -0.0037, -0.0450,  0.0140,  0.0207,\n",
       "                      -0.0101, -0.0359, -0.0113, -0.0101, -0.0227, -0.0563, -0.0614, -0.0413,\n",
       "                      -0.0581, -0.0063,  0.0157, -0.0085, -0.0191,  0.0298,  0.0107,  0.0136,\n",
       "                      -0.0204, -0.0231,  0.0204,  0.0122, -0.0019, -0.0410, -0.0241, -0.0097,\n",
       "                       0.0038, -0.0140, -0.0293, -0.0177,  0.0009,  0.0203, -0.0500, -0.0134,\n",
       "                       0.0137, -0.0227,  0.0217, -0.0137, -0.0418, -0.0384, -0.0196,  0.0087,\n",
       "                      -0.0097,  0.0274, -0.0020,  0.0084, -0.0318, -0.0251, -0.0196, -0.0469,\n",
       "                      -0.0222,  0.0107, -0.0537,  0.0002,  0.0148, -0.0441,  0.0115, -0.0157,\n",
       "                      -0.0391, -0.0017,  0.0026, -0.0122, -0.0145,  0.0045, -0.0200, -0.0303,\n",
       "                      -0.0170, -0.0188, -0.0327, -0.0106, -0.0415, -0.0175, -0.0164, -0.0365,\n",
       "                      -0.0427,  0.0210, -0.0017, -0.0251,  0.0045, -0.0234, -0.0422,  0.0113,\n",
       "                      -0.0119, -0.0222, -0.0369, -0.0444,  0.0031, -0.0371, -0.0194, -0.0131,\n",
       "                      -0.0504,  0.0014, -0.0218, -0.0198, -0.0080, -0.0114, -0.0031,  0.0004,\n",
       "                      -0.0538, -0.0217,  0.0147, -0.0471, -0.0139,  0.0200,  0.0024, -0.0510,\n",
       "                      -0.0113,  0.0144, -0.0011, -0.0329, -0.0361,  0.0241, -0.0301, -0.0098,\n",
       "                       0.0061,  0.0112,  0.0164, -0.0420,  0.0083, -0.0535, -0.0402, -0.0230,\n",
       "                      -0.0146, -0.0432, -0.0125, -0.0451, -0.0438,  0.0232, -0.0285,  0.0014,\n",
       "                      -0.0010, -0.0570, -0.0208,  0.0109,  0.0150, -0.0350, -0.0041, -0.0238,\n",
       "                      -0.0131,  0.0237, -0.0599,  0.0210,  0.0027, -0.0032, -0.0494, -0.0292,\n",
       "                       0.0023, -0.0258,  0.0170, -0.0024, -0.0275, -0.0574, -0.0057, -0.0514,\n",
       "                       0.0266, -0.0195,  0.0094, -0.0102,  0.0112,  0.0069, -0.0104, -0.0435,\n",
       "                      -0.0090, -0.0317, -0.0044, -0.0443, -0.0339, -0.0621, -0.0151,  0.0016,\n",
       "                       0.0175, -0.0364, -0.0107, -0.0212,  0.0066, -0.0311, -0.0509,  0.0068,\n",
       "                      -0.0217, -0.0244, -0.0007, -0.0564, -0.0086, -0.0607, -0.0316, -0.0072,\n",
       "                       0.0219,  0.0124, -0.0538,  0.0086, -0.0579,  0.0161,  0.0217, -0.0500,\n",
       "                      -0.0550, -0.0440, -0.0401, -0.0389,  0.0128, -0.0179, -0.0455,  0.0190,\n",
       "                      -0.0251, -0.0522, -0.0245,  0.0127, -0.0121, -0.0039, -0.0415, -0.0037,\n",
       "                      -0.0582,  0.0037,  0.0033, -0.0341, -0.0409, -0.0262,  0.0031, -0.0203,\n",
       "                      -0.0404, -0.0297, -0.0179, -0.0094, -0.0455, -0.0399, -0.0569,  0.0006,\n",
       "                       0.0143, -0.0438,  0.0265,  0.0061, -0.0316,  0.0101, -0.0405, -0.0359,\n",
       "                      -0.0512,  0.0203, -0.0557, -0.0567, -0.0033, -0.0548, -0.0118,  0.0074,\n",
       "                      -0.0014, -0.0152, -0.0259,  0.0139,  0.0174,  0.0146, -0.0328,  0.0057,\n",
       "                      -0.0411,  0.0149, -0.0280, -0.0492, -0.0225,  0.0069, -0.0223, -0.0013,\n",
       "                      -0.0559, -0.0305, -0.0360,  0.0237, -0.0356,  0.0099,  0.0275, -0.0261,\n",
       "                       0.0086, -0.0323,  0.0226, -0.0464,  0.0116,  0.0002, -0.0121, -0.0341,\n",
       "                       0.0107, -0.0342, -0.0304, -0.0462, -0.0489, -0.0098, -0.0639, -0.0302,\n",
       "                      -0.0375, -0.0067, -0.0478, -0.0404, -0.0620,  0.0087, -0.0328, -0.0356,\n",
       "                      -0.0573, -0.0008, -0.0145, -0.0497, -0.0102, -0.0345, -0.0309, -0.0177,\n",
       "                      -0.0072,  0.0083,  0.0147,  0.0145, -0.0012,  0.0043, -0.0265,  0.0028,\n",
       "                      -0.0496,  0.0190, -0.0137,  0.0274, -0.0557, -0.0444, -0.0129, -0.0461,\n",
       "                      -0.0444, -0.0281, -0.0571,  0.0086, -0.0222, -0.0333,  0.0152,  0.0207,\n",
       "                      -0.0602, -0.0390, -0.0587, -0.0317, -0.0417, -0.0439, -0.0132, -0.0560])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0155, -0.0034, -0.0303,  ...,  0.0214,  0.0218, -0.0175],\n",
       "                      [-0.0079, -0.0765,  0.0400,  ...,  0.0715,  0.0540, -0.0559],\n",
       "                      [-0.0550, -0.0462,  0.0142,  ..., -0.0014, -0.0442, -0.0347],\n",
       "                      ...,\n",
       "                      [-0.0034, -0.0060, -0.0108,  ..., -0.0104,  0.0085, -0.0423],\n",
       "                      [ 0.0086,  0.0238, -0.0490,  ...,  0.0416,  0.0470, -0.0172],\n",
       "                      [-0.0468, -0.0215,  0.0152,  ..., -0.0153,  0.0073, -0.0024]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([-0.0004, -0.0664,  0.0049, -0.0405, -0.0475, -0.0488,  0.0247, -0.0079,\n",
       "                      -0.0492,  0.0211, -0.0652, -0.0267, -0.0191, -0.0403,  0.0117, -0.0438,\n",
       "                       0.0060, -0.0346, -0.0403, -0.0127, -0.0261, -0.0013,  0.0192, -0.0252,\n",
       "                      -0.0292, -0.0241,  0.0001, -0.0322, -0.0418, -0.0388,  0.0302, -0.0357,\n",
       "                      -0.0424, -0.0360, -0.0198, -0.0199, -0.0345,  0.0283, -0.0349, -0.0509,\n",
       "                       0.0074, -0.0498, -0.0566, -0.0455,  0.0025, -0.0307, -0.0490, -0.0136,\n",
       "                       0.0273, -0.0327, -0.0270, -0.0556,  0.0400,  0.0043, -0.0269, -0.0326,\n",
       "                       0.0126, -0.0463,  0.0090,  0.0218, -0.0189,  0.0275, -0.0178, -0.0162,\n",
       "                      -0.0045,  0.0171,  0.0167, -0.0232,  0.0364,  0.0111, -0.0168, -0.0359,\n",
       "                       0.0077, -0.0483, -0.0392,  0.0322, -0.0320, -0.0317, -0.0072, -0.0184,\n",
       "                      -0.0126, -0.0305,  0.0114, -0.0189, -0.0437,  0.0054,  0.0261,  0.0358,\n",
       "                      -0.0099, -0.0502,  0.0181,  0.0183,  0.0056,  0.0263, -0.0525,  0.0022,\n",
       "                       0.0233, -0.0488, -0.0390,  0.0153, -0.0033, -0.0489,  0.0130,  0.0065,\n",
       "                      -0.0040,  0.0164, -0.0502, -0.0360,  0.0203, -0.0478, -0.0355,  0.0423,\n",
       "                      -0.0127, -0.0476, -0.0330,  0.0282, -0.0467, -0.0332,  0.0233, -0.0348,\n",
       "                      -0.0313,  0.0146, -0.0475,  0.0271, -0.0288, -0.0291, -0.0448, -0.0212,\n",
       "                       0.0276,  0.0278, -0.0479,  0.0049,  0.0129, -0.0336,  0.0250,  0.0109,\n",
       "                       0.0168,  0.0036, -0.0542,  0.0278,  0.0147, -0.0409, -0.0521,  0.0183,\n",
       "                      -0.0319, -0.0378, -0.0241, -0.0166, -0.0497, -0.0127, -0.0319, -0.0303,\n",
       "                      -0.0223,  0.0306,  0.0292, -0.0120, -0.0377, -0.0292,  0.0166, -0.0310,\n",
       "                      -0.0442,  0.0150, -0.0175,  0.0063, -0.0301, -0.0544, -0.0403, -0.0125,\n",
       "                       0.0087, -0.0076, -0.0387, -0.0388, -0.0137,  0.0059, -0.0382, -0.0038,\n",
       "                      -0.0336, -0.0327, -0.0060,  0.0201,  0.0310,  0.0109,  0.0289, -0.0397,\n",
       "                      -0.0385, -0.0325, -0.0412, -0.0429, -0.0048,  0.0084, -0.0498,  0.0159,\n",
       "                       0.0210, -0.0549, -0.0057, -0.0093, -0.0214,  0.0050, -0.0369, -0.0426,\n",
       "                      -0.0114,  0.0028,  0.0274, -0.0183, -0.0275, -0.0101, -0.0539, -0.0529,\n",
       "                      -0.0305,  0.0242,  0.0007, -0.0419, -0.0302, -0.0478, -0.0423, -0.0364,\n",
       "                      -0.0005,  0.0212, -0.0184, -0.0540, -0.0364,  0.0039,  0.0050,  0.0340,\n",
       "                      -0.0560, -0.0473, -0.0448, -0.0212, -0.0316,  0.0088,  0.0186,  0.0198,\n",
       "                      -0.0187, -0.0292,  0.0420, -0.0336,  0.0021, -0.0244,  0.0368,  0.0106,\n",
       "                       0.0093,  0.0383,  0.0123,  0.0261, -0.0042,  0.0072, -0.0327, -0.0518,\n",
       "                       0.0239,  0.0143, -0.0094, -0.0243,  0.0293, -0.0416, -0.0282,  0.0372,\n",
       "                      -0.0214, -0.0430, -0.0329, -0.0549, -0.0279,  0.0041,  0.0332, -0.0542,\n",
       "                      -0.0546, -0.0289,  0.0074, -0.0050, -0.0250, -0.0010,  0.0089,  0.0100,\n",
       "                      -0.0023,  0.0126,  0.0277, -0.0197, -0.0387,  0.0007,  0.0321, -0.0337,\n",
       "                      -0.0237,  0.0241,  0.0091, -0.0050, -0.0430, -0.0344, -0.0280, -0.0024,\n",
       "                       0.0055, -0.0100,  0.0185,  0.0215, -0.0225,  0.0071, -0.0086, -0.0501,\n",
       "                      -0.0376,  0.0089, -0.0523, -0.0503, -0.0461,  0.0136,  0.0086,  0.0342,\n",
       "                      -0.0209, -0.0449, -0.0502, -0.0062, -0.0173,  0.0086, -0.0507, -0.0453,\n",
       "                      -0.0285, -0.0191,  0.0163, -0.0063,  0.0133, -0.0041,  0.0044, -0.0478,\n",
       "                      -0.0430,  0.0050,  0.0090,  0.0008, -0.0205, -0.0046,  0.0346, -0.0232,\n",
       "                      -0.0501,  0.0177,  0.0230,  0.0131,  0.0274, -0.0279, -0.0436,  0.0071,\n",
       "                       0.0235, -0.0150,  0.0067,  0.0195, -0.0545, -0.0294, -0.0160,  0.0195,\n",
       "                       0.0061,  0.0102, -0.0402, -0.0162, -0.0405,  0.0202, -0.0218,  0.0200,\n",
       "                      -0.0547,  0.0061, -0.0165,  0.0070,  0.0088, -0.0138,  0.0201, -0.0138,\n",
       "                      -0.0520,  0.0256,  0.0220, -0.0064,  0.0137, -0.0136, -0.0507,  0.0003,\n",
       "                      -0.0290, -0.0522, -0.0258, -0.0013, -0.0036, -0.0100, -0.0415, -0.0442,\n",
       "                      -0.0283, -0.0312, -0.0120, -0.0091,  0.0217,  0.0298,  0.0299, -0.0132,\n",
       "                       0.0237, -0.0007, -0.0081,  0.0052, -0.0045, -0.0483,  0.0403, -0.0498,\n",
       "                       0.0369,  0.0174,  0.0136, -0.0513,  0.0304,  0.0021, -0.0471,  0.0197,\n",
       "                       0.0010, -0.0284, -0.0262, -0.0071, -0.0175, -0.0346,  0.0192,  0.0120,\n",
       "                      -0.0170, -0.0470, -0.0078, -0.0239, -0.0154,  0.0050, -0.0152,  0.0048,\n",
       "                       0.0122,  0.0111, -0.0204,  0.0033, -0.0217, -0.0291, -0.0406,  0.0195,\n",
       "                      -0.0053,  0.0245, -0.0569, -0.0430,  0.0235, -0.0257, -0.0446, -0.0103,\n",
       "                       0.0380, -0.0450, -0.0201,  0.0028, -0.0297, -0.0530,  0.0234,  0.0215,\n",
       "                      -0.0352,  0.0236, -0.0455,  0.0325,  0.0260,  0.0271, -0.0057,  0.0252,\n",
       "                      -0.0301, -0.0292,  0.0115, -0.0112, -0.0360, -0.0333, -0.0270,  0.0247,\n",
       "                      -0.0154, -0.0135,  0.0090,  0.0098, -0.0253, -0.0202, -0.0216, -0.0499,\n",
       "                       0.0045, -0.0167, -0.0252,  0.0212, -0.0475, -0.0165, -0.0471,  0.0063,\n",
       "                       0.0157, -0.0111, -0.0007, -0.0061,  0.0038, -0.0365,  0.0273, -0.0193,\n",
       "                       0.0231, -0.0490, -0.0290, -0.0053, -0.0140, -0.0186,  0.0209, -0.0024,\n",
       "                       0.0023, -0.0544, -0.0460,  0.0228, -0.0067, -0.0051, -0.0311,  0.0270,\n",
       "                      -0.0130, -0.0171, -0.0120,  0.0073,  0.0324, -0.0130,  0.0058,  0.0109,\n",
       "                       0.0096,  0.0090,  0.0205,  0.0168, -0.0312, -0.0427, -0.0355, -0.0224])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 0.1731,  0.4431, -0.0346,  ..., -0.0619,  0.2773,  0.0175],\n",
       "                      [ 0.1551, -0.1612, -0.0157,  ...,  0.1290,  0.0737, -0.0793],\n",
       "                      [-0.0284, -0.0527, -0.0537,  ...,  0.0162, -0.0490, -0.0064],\n",
       "                      ...,\n",
       "                      [-0.0452,  0.2208,  0.0368,  ..., -0.0740,  0.0906,  0.0635],\n",
       "                      [-0.0101, -0.0339,  0.0144,  ..., -0.0143, -0.0188,  0.0652],\n",
       "                      [ 0.0968, -0.1843,  0.0317,  ...,  0.0096,  0.0896, -0.0980]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.1103, -0.1155, -0.0244,  0.0543, -0.0127, -0.2644,  0.1078,  0.1540,\n",
       "                       0.1378, -0.1203]))])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_trained_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre: \n",
      " Layer 1: 8.3034996e-07, 0.025849843\n",
      " Layer 2: 7.102816e-05, 0.025543222\n",
      " Layer 3: 0.00018317447, 0.02547586\n",
      "Post: \n",
      " Layer 1: -0.0015452711, 0.03388202\n",
      " Layer 2: -0.005922225, 0.03329243\n",
      " Layer 3: 0.0001834683, 0.123158395\n"
     ]
    }
   ],
   "source": [
    "mean_pre_0 = np.mean(pre_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_post_0 = np.mean(post_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_pre_0 = np.std(pre_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "std_post_0 = np.std(post_trained_multiclass[\"linear_relu_stack.0.weight\"].numpy())\n",
    "mean_pre_1 = np.mean(pre_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_post_1 = np.mean(post_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_pre_1 = np.std(pre_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "std_post_1 = np.std(post_trained_multiclass[\"linear_relu_stack.2.weight\"].numpy())\n",
    "mean_pre_2 = np.mean(pre_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "mean_post_2 = np.mean(post_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "std_pre_2 = np.std(pre_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "std_post_2 = np.std(post_trained_multiclass[\"linear_relu_stack.4.weight\"].numpy())\n",
    "print(\"Pre: \")\n",
    "print(\" Layer 1: \" + str(mean_pre_0) + \", \" + str(std_pre_0))\n",
    "print(\" Layer 2: \" + str(mean_pre_1) + \", \" + str(std_pre_1))\n",
    "print(\" Layer 3: \" + str(mean_pre_2) + \", \" + str(std_pre_2))\n",
    "print(\"Post: \")\n",
    "print(\" Layer 1: \" + str(mean_post_0) + \", \" + str(std_post_0))\n",
    "print(\" Layer 2: \" + str(mean_post_1) + \", \" + str(std_post_1))\n",
    "print(\" Layer 3: \" + str(mean_post_2) + \", \" + str(std_post_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "for item in post_trained_multiclass:\n",
    "    if item[20:] == \"weight\":\n",
    "        table = pd.DataFrame(post_trained_multiclass[item])\n",
    "        table.to_csv(\"results/nonMNIST_multi_weight_post_\" + str(ID) + \".csv\",index=False)\n",
    "    if item[20:] == \"bias\":\n",
    "        series = pd.Series(post_trained_multiclass[item])\n",
    "        series.to_csv(\"results/nonMNIST_multi_bias_post_\" + str(ID) + \".csv\",index=False)\n",
    "    ID += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9/23/2024 - There is a separation between Classification problems and regression problems in terms of network updates. Classification problems build upon patterns previously found in the network, unless it is very extreme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think these results indicate that rather than there being special characteristics of the MNIST dataset, The Neural Networks have different update patterns depending on the type of problem. It could be possible that the Loss functions impact the way a network updates, since the loss directly impacts the update pattern. However, given the results of the previous sets, this doesn't seem to be the case. I have not been testing for this, though, so this should be taken lightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Heatmap Data X')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create the heatmap\n",
    "column_start = 0\n",
    "row_start = 0\n",
    "window = 1\n",
    "\n",
    "fig = plt.figure(0)\n",
    "plt.imshow([small_regression_problem[1][:50]], cmap='viridis', interpolation='nearest')\n",
    "\n",
    "# Add a colorbar for reference\n",
    "plt.colorbar()\n",
    "\n",
    "# Add labels (optional)\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Heatmap Data X')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
