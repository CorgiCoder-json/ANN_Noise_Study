{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmer: Jacob Maurer\n",
    "Date: 9/18/2024\n",
    "Description: This notebook is going to explore the idea of the data footprint, which is just the (final trained model weights) - (starting out weights). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.060906764\n",
      "-0.0401217225\n",
      "0.001037473949165444\n",
      "0.00018947100000000269\n"
     ]
    }
   ],
   "source": [
    "rr_weight_layer_one_pre = pd.read_csv(\"results/rr_s_weight_pre_1.csv\").to_numpy()\n",
    "rr_weight_layer_one_post = pd.read_csv(\"results/rr_s_weight_post_1.csv\").to_numpy()\n",
    "rr_footprint = rr_weight_layer_one_post - rr_weight_layer_one_pre\n",
    "print(np.max(rr_footprint))\n",
    "print(np.min(rr_footprint))\n",
    "print(np.mean(rr_footprint))\n",
    "print(np.median(rr_footprint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07387706499999999\n",
      "-0.06480385799999999\n",
      "-0.00012459526740174432\n",
      "-7.177000000000572e-06\n"
     ]
    }
   ],
   "source": [
    "ss_weight_layer_one_pre = pd.read_csv(\"results/ss_s_weight_pre_1.csv\").to_numpy()\n",
    "ss_weight_layer_one_post = pd.read_csv(\"results/ss_s_weight_post_1.csv\").to_numpy()\n",
    "ss_footprint = ss_weight_layer_one_post - ss_weight_layer_one_pre\n",
    "print(np.max(ss_footprint))\n",
    "print(np.min(ss_footprint))\n",
    "print(np.mean(ss_footprint))\n",
    "print(np.median(ss_footprint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Layer: \n",
      "0.09991215\n",
      "-0.09559656\n",
      "-0.00013230133303434018\n",
      "0.023558188617021514\n",
      "-0.000157678935\n",
      "New Layer: \n",
      "0.08342661\n",
      "-0.0654329772\n",
      "0.001029767883532848\n",
      "0.021198322257155098\n",
      "0.0010217178299999992\n"
     ]
    }
   ],
   "source": [
    "modified_ss_layer = ss_weight_layer_one_pre + rr_footprint\n",
    "modified_rr_layer = rr_weight_layer_one_pre + ss_footprint\n",
    "print(\"Old Layer: \")\n",
    "print(np.max(ss_weight_layer_one_post))\n",
    "print(np.min(ss_weight_layer_one_post))\n",
    "print(np.mean(ss_weight_layer_one_post))\n",
    "print(np.std(ss_weight_layer_one_post))\n",
    "print(np.median(ss_weight_layer_one_post))\n",
    "print(\"New Layer: \")\n",
    "print(np.max(modified_ss_layer))\n",
    "print(np.min(modified_ss_layer))\n",
    "print(np.mean(modified_ss_layer))\n",
    "print(np.std(modified_ss_layer))\n",
    "print(np.median(modified_ss_layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why this might be important?\n",
    "\n",
    "It means that we are able to significantly reduce training time by just adding the footprint to the layer. The only problem is, as of right now, it only works when the model has these similar parameters\n",
    "\n",
    "Hypothesis: Reduction/expansion with pooling and convolution not using neural networks. \n",
    "Evidence: Pooling/convolution scale the data while retaining most of the original pattern.\n",
    "\n",
    "Pooling2d used as a possible way to prune the network??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear_relu_stack.0.weight', tensor([[ 0.0341,  0.0272,  0.0321,  ...,  0.0197, -0.0168, -0.0107],\n",
      "        [-0.0066,  0.0117, -0.0312,  ...,  0.0284,  0.0262, -0.0167],\n",
      "        [ 0.0256, -0.0338,  0.0014,  ..., -0.0231, -0.0342,  0.0114],\n",
      "        ...,\n",
      "        [-0.0232,  0.0178,  0.0313,  ...,  0.0040,  0.0155, -0.0116],\n",
      "        [-0.0047,  0.0297, -0.0166,  ..., -0.0259,  0.0153, -0.0306],\n",
      "        [-0.0340, -0.0062,  0.0120,  ..., -0.0149, -0.0313, -0.0245]],\n",
      "       device='cuda:0')), ('linear_relu_stack.0.bias', tensor([ 0.0320, -0.0275,  0.0030, -0.0306, -0.0110,  0.0285, -0.0167, -0.0039,\n",
      "        -0.0225, -0.0339, -0.0263, -0.0272,  0.0219,  0.0334, -0.0208,  0.0185,\n",
      "         0.0325,  0.0132, -0.0124, -0.0002,  0.0082, -0.0204, -0.0028, -0.0287,\n",
      "         0.0130,  0.0340, -0.0164, -0.0280,  0.0283,  0.0283, -0.0149, -0.0108,\n",
      "         0.0045, -0.0319, -0.0247, -0.0050,  0.0145,  0.0341, -0.0151, -0.0076,\n",
      "        -0.0312, -0.0259, -0.0157, -0.0345, -0.0031,  0.0208,  0.0228,  0.0156,\n",
      "        -0.0151,  0.0308, -0.0237, -0.0133, -0.0129, -0.0216,  0.0353,  0.0021,\n",
      "         0.0283, -0.0326, -0.0306,  0.0237, -0.0295,  0.0255,  0.0018, -0.0012,\n",
      "        -0.0074,  0.0022,  0.0343, -0.0230, -0.0144,  0.0065,  0.0164, -0.0088,\n",
      "         0.0229, -0.0063, -0.0120, -0.0147, -0.0084, -0.0066,  0.0099,  0.0059,\n",
      "        -0.0279, -0.0162, -0.0096, -0.0082,  0.0238, -0.0040,  0.0272, -0.0149,\n",
      "         0.0002,  0.0115,  0.0038,  0.0075, -0.0044, -0.0185,  0.0166, -0.0262,\n",
      "         0.0179, -0.0139, -0.0178, -0.0350,  0.0130,  0.0256, -0.0067, -0.0017,\n",
      "        -0.0156, -0.0113,  0.0268,  0.0230,  0.0303,  0.0285,  0.0187,  0.0187,\n",
      "        -0.0192,  0.0246, -0.0229,  0.0126, -0.0019, -0.0349, -0.0207, -0.0018,\n",
      "         0.0089,  0.0167, -0.0152,  0.0027, -0.0316, -0.0215, -0.0344,  0.0018,\n",
      "        -0.0125, -0.0230,  0.0346, -0.0290,  0.0005,  0.0134, -0.0198,  0.0184,\n",
      "        -0.0206,  0.0024,  0.0231,  0.0313,  0.0172, -0.0027, -0.0095,  0.0343,\n",
      "        -0.0133, -0.0097, -0.0273,  0.0210, -0.0002,  0.0260, -0.0309, -0.0079,\n",
      "         0.0180,  0.0183,  0.0179,  0.0303, -0.0234,  0.0305, -0.0224, -0.0089,\n",
      "        -0.0028,  0.0277, -0.0318, -0.0124, -0.0141,  0.0284, -0.0107,  0.0044,\n",
      "         0.0094, -0.0078,  0.0116, -0.0101, -0.0088, -0.0144, -0.0044,  0.0212,\n",
      "        -0.0237, -0.0283, -0.0118, -0.0165, -0.0247, -0.0353, -0.0333,  0.0309,\n",
      "         0.0117, -0.0169,  0.0270,  0.0006,  0.0351, -0.0238,  0.0351,  0.0124,\n",
      "         0.0166, -0.0108, -0.0304, -0.0156,  0.0334, -0.0003,  0.0110,  0.0330,\n",
      "         0.0289,  0.0010, -0.0173,  0.0258, -0.0127, -0.0285,  0.0222, -0.0117,\n",
      "         0.0278, -0.0158, -0.0301,  0.0215,  0.0161,  0.0319, -0.0184,  0.0150,\n",
      "         0.0259, -0.0345,  0.0106, -0.0294,  0.0109,  0.0181,  0.0079,  0.0252,\n",
      "        -0.0325, -0.0250, -0.0060, -0.0006, -0.0189, -0.0187,  0.0141, -0.0196,\n",
      "         0.0158, -0.0278,  0.0112,  0.0178, -0.0168,  0.0194,  0.0001, -0.0241,\n",
      "         0.0271,  0.0043, -0.0042,  0.0168, -0.0195,  0.0299,  0.0163,  0.0191,\n",
      "         0.0165, -0.0051, -0.0137,  0.0027,  0.0162, -0.0310,  0.0108,  0.0046,\n",
      "        -0.0267,  0.0064,  0.0222,  0.0131, -0.0059, -0.0032, -0.0323, -0.0154,\n",
      "        -0.0265,  0.0311, -0.0246,  0.0211,  0.0092, -0.0164, -0.0030, -0.0013,\n",
      "         0.0206,  0.0253, -0.0125,  0.0176, -0.0139, -0.0123, -0.0059,  0.0200,\n",
      "         0.0077,  0.0081, -0.0043, -0.0314, -0.0169,  0.0194,  0.0230,  0.0287,\n",
      "         0.0002,  0.0242, -0.0312,  0.0339, -0.0254, -0.0005, -0.0107, -0.0062,\n",
      "         0.0329, -0.0232,  0.0162, -0.0240, -0.0111, -0.0020,  0.0300, -0.0048,\n",
      "         0.0330,  0.0240, -0.0042,  0.0346,  0.0012, -0.0242, -0.0219, -0.0043,\n",
      "         0.0188,  0.0325, -0.0087,  0.0261,  0.0175,  0.0294, -0.0078,  0.0066,\n",
      "         0.0299, -0.0321, -0.0266,  0.0216,  0.0143, -0.0299,  0.0295, -0.0149,\n",
      "         0.0326, -0.0004,  0.0340, -0.0203, -0.0278,  0.0324, -0.0107,  0.0036,\n",
      "         0.0085,  0.0009, -0.0348, -0.0286,  0.0070, -0.0138,  0.0339,  0.0206,\n",
      "        -0.0283,  0.0095, -0.0057,  0.0151,  0.0304,  0.0166,  0.0253, -0.0269,\n",
      "        -0.0233,  0.0059, -0.0227,  0.0050,  0.0010, -0.0306, -0.0113, -0.0087,\n",
      "        -0.0116,  0.0088,  0.0310, -0.0343,  0.0348, -0.0138, -0.0040, -0.0340,\n",
      "        -0.0246, -0.0226, -0.0320,  0.0088,  0.0096, -0.0007,  0.0338,  0.0183,\n",
      "         0.0303, -0.0281, -0.0152, -0.0055, -0.0240,  0.0040, -0.0301,  0.0273,\n",
      "         0.0108,  0.0295, -0.0119, -0.0244, -0.0230, -0.0356, -0.0108,  0.0015,\n",
      "        -0.0021, -0.0209, -0.0137,  0.0056, -0.0117, -0.0271,  0.0005, -0.0096,\n",
      "        -0.0314,  0.0058, -0.0114, -0.0310, -0.0153, -0.0243,  0.0084,  0.0335,\n",
      "         0.0344,  0.0248, -0.0070,  0.0280, -0.0169, -0.0042, -0.0298, -0.0015,\n",
      "         0.0311, -0.0344,  0.0128, -0.0272,  0.0121, -0.0098,  0.0313, -0.0324,\n",
      "        -0.0240, -0.0306,  0.0028,  0.0135,  0.0255,  0.0347, -0.0109, -0.0207,\n",
      "         0.0239, -0.0158, -0.0136, -0.0073,  0.0145,  0.0075, -0.0160,  0.0168,\n",
      "         0.0241,  0.0073, -0.0008, -0.0177,  0.0329,  0.0131, -0.0038, -0.0049,\n",
      "         0.0281, -0.0067,  0.0026,  0.0016,  0.0222,  0.0002,  0.0337,  0.0347,\n",
      "        -0.0202, -0.0060, -0.0277,  0.0258, -0.0307, -0.0232, -0.0140, -0.0010,\n",
      "         0.0266, -0.0336, -0.0008,  0.0156, -0.0013, -0.0148,  0.0197,  0.0126,\n",
      "        -0.0022,  0.0256,  0.0067, -0.0247,  0.0199, -0.0233, -0.0019, -0.0214,\n",
      "         0.0348,  0.0257, -0.0062,  0.0181,  0.0269,  0.0045,  0.0143, -0.0060,\n",
      "        -0.0230, -0.0186,  0.0241,  0.0216, -0.0103, -0.0295, -0.0282,  0.0164,\n",
      "        -0.0040, -0.0214,  0.0003, -0.0124, -0.0068, -0.0161,  0.0102,  0.0023,\n",
      "        -0.0081,  0.0035,  0.0119, -0.0293, -0.0176, -0.0297, -0.0104,  0.0182],\n",
      "       device='cuda:0')), ('linear_relu_stack.2.weight', tensor([[ 0.0426,  0.0171,  0.0354,  ...,  0.0323,  0.0205,  0.0095],\n",
      "        [ 0.0263,  0.0416, -0.0247,  ...,  0.0241,  0.0006, -0.0098],\n",
      "        [-0.0217, -0.0120,  0.0152,  ..., -0.0283, -0.0438,  0.0203],\n",
      "        ...,\n",
      "        [-0.0290,  0.0360,  0.0229,  ..., -0.0298, -0.0095,  0.0260],\n",
      "        [-0.0387,  0.0132,  0.0395,  ..., -0.0423,  0.0384, -0.0105],\n",
      "        [ 0.0409, -0.0409,  0.0248,  ..., -0.0150,  0.0261, -0.0295]],\n",
      "       device='cuda:0')), ('linear_relu_stack.2.bias', tensor([ 0.0031, -0.0311,  0.0409, -0.0322, -0.0292, -0.0148, -0.0345, -0.0108,\n",
      "        -0.0101, -0.0269], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests below are run using the smaller model traing first. Will move to larger model footprint after these same model tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.295743  [   64/60000]\n",
      "loss: 1.815825  [ 6464/60000]\n",
      "loss: 1.299510  [12864/60000]\n",
      "loss: 1.198148  [19264/60000]\n",
      "loss: 0.926620  [25664/60000]\n",
      "loss: 0.889095  [32064/60000]\n",
      "loss: 0.893613  [38464/60000]\n",
      "loss: 0.801828  [44864/60000]\n",
      "loss: 0.816976  [51264/60000]\n",
      "loss: 0.748970  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.739310 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.719671  [   64/60000]\n",
      "loss: 0.797256  [ 6464/60000]\n",
      "loss: 0.542338  [12864/60000]\n",
      "loss: 0.781250  [19264/60000]\n",
      "loss: 0.641137  [25664/60000]\n",
      "loss: 0.640792  [32064/60000]\n",
      "loss: 0.676180  [38464/60000]\n",
      "loss: 0.683666  [44864/60000]\n",
      "loss: 0.689323  [51264/60000]\n",
      "loss: 0.607981  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.610954 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.549364  [   64/60000]\n",
      "loss: 0.639952  [ 6464/60000]\n",
      "loss: 0.432332  [12864/60000]\n",
      "loss: 0.687585  [19264/60000]\n",
      "loss: 0.563966  [25664/60000]\n",
      "loss: 0.572663  [32064/60000]\n",
      "loss: 0.584576  [38464/60000]\n",
      "loss: 0.662095  [44864/60000]\n",
      "loss: 0.659615  [51264/60000]\n",
      "loss: 0.531365  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.556629 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#ReLU single: Trial 1: 3 epochs\n",
    "#Modified: Trial 1: 2 epochs \n",
    "#ss footprint: Trial 1: 3 epochs\n",
    "#Sigmoid single: Trial 1: 14 epochs\n",
    "#Modified: Trial 1: 13 epochs \n",
    "#rr footprint: Trial 1: 10 epochs\n",
    "t, acc = 0, 0\n",
    "while acc < 80:\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    acc = test(test_dataloader, model, loss_fn) * 100\n",
    "    t += 1\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state = model.state_dict()\n",
    "new_state[\"linear_relu_stack.0.weight\"] = new_state[\"linear_relu_stack.0.weight\"] + torch.from_numpy(rr_footprint).to(device)\n",
    "model.load_state_dict(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.350044  [   64/60000]\n",
      "loss: 1.286771  [ 6464/60000]\n",
      "loss: 0.869158  [12864/60000]\n",
      "loss: 0.912096  [19264/60000]\n",
      "loss: 0.744546  [25664/60000]\n",
      "loss: 0.721017  [32064/60000]\n",
      "loss: 0.731283  [38464/60000]\n",
      "loss: 0.711199  [44864/60000]\n",
      "loss: 0.721409  [51264/60000]\n",
      "loss: 0.614578  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.630674 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.569716  [   64/60000]\n",
      "loss: 0.671819  [ 6464/60000]\n",
      "loss: 0.440914  [12864/60000]\n",
      "loss: 0.674982  [19264/60000]\n",
      "loss: 0.572050  [25664/60000]\n",
      "loss: 0.575476  [32064/60000]\n",
      "loss: 0.593051  [38464/60000]\n",
      "loss: 0.656144  [44864/60000]\n",
      "loss: 0.651581  [51264/60000]\n",
      "loss: 0.517869  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.554890 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# modified random training sequence\n",
    "# cutoff criteria: 80%\n",
    "t, acc = 0, 0\n",
    "while acc < 80:\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    acc = test(test_dataloader, model, loss_fn) * 100\n",
    "    t += 1\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork2(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (pool): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=391, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.pool = nn.MaxPool1d(3, stride = 2)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(391, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.pool(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork2().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state = model.state_dict()\n",
    "pooling = nn.MaxPool1d(3, stride=2)\n",
    "pooled_rr_footprint = pooling(torch.from_numpy(rr_footprint))\n",
    "pooled_ss_footprint = pooling(torch.from_numpy(ss_footprint))\n",
    "new_state[\"linear_relu_stack.0.weight\"] = new_state[\"linear_relu_stack.0.weight\"] + torch.from_numpy(np.asarray(pooled_rr_footprint)).to(device)\n",
    "model.load_state_dict(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.293046  [   64/60000]\n",
      "loss: 2.224894  [ 6464/60000]\n",
      "loss: 2.132917  [12864/60000]\n",
      "loss: 2.049005  [19264/60000]\n",
      "loss: 1.869001  [25664/60000]\n",
      "loss: 1.824632  [32064/60000]\n",
      "loss: 1.706986  [38464/60000]\n",
      "loss: 1.606118  [44864/60000]\n",
      "loss: 1.562098  [51264/60000]\n",
      "loss: 1.470821  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 1.452219 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.514148  [   64/60000]\n",
      "loss: 1.450181  [ 6464/60000]\n",
      "loss: 1.306089  [12864/60000]\n",
      "loss: 1.367158  [19264/60000]\n",
      "loss: 1.176891  [25664/60000]\n",
      "loss: 1.237265  [32064/60000]\n",
      "loss: 1.181069  [38464/60000]\n",
      "loss: 1.122512  [44864/60000]\n",
      "loss: 1.162620  [51264/60000]\n",
      "loss: 1.112746  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.090834 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.155158  [   64/60000]\n",
      "loss: 1.137584  [ 6464/60000]\n",
      "loss: 0.980134  [12864/60000]\n",
      "loss: 1.134294  [19264/60000]\n",
      "loss: 0.953459  [25664/60000]\n",
      "loss: 1.025285  [32064/60000]\n",
      "loss: 0.995019  [38464/60000]\n",
      "loss: 0.938323  [44864/60000]\n",
      "loss: 1.001286  [51264/60000]\n",
      "loss: 0.976169  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.940724 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.982856  [   64/60000]\n",
      "loss: 1.000749  [ 6464/60000]\n",
      "loss: 0.829795  [12864/60000]\n",
      "loss: 1.029153  [19264/60000]\n",
      "loss: 0.856855  [25664/60000]\n",
      "loss: 0.911865  [32064/60000]\n",
      "loss: 0.901382  [38464/60000]\n",
      "loss: 0.842775  [44864/60000]\n",
      "loss: 0.912187  [51264/60000]\n",
      "loss: 0.905238  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.856719 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.873326  [   64/60000]\n",
      "loss: 0.921961  [ 6464/60000]\n",
      "loss: 0.742977  [12864/60000]\n",
      "loss: 0.967021  [19264/60000]\n",
      "loss: 0.800842  [25664/60000]\n",
      "loss: 0.841038  [32064/60000]\n",
      "loss: 0.844591  [38464/60000]\n",
      "loss: 0.785851  [44864/60000]\n",
      "loss: 0.856640  [51264/60000]\n",
      "loss: 0.861117  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.802515 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.796454  [   64/60000]\n",
      "loss: 0.869323  [ 6464/60000]\n",
      "loss: 0.686981  [12864/60000]\n",
      "loss: 0.924536  [19264/60000]\n",
      "loss: 0.761876  [25664/60000]\n",
      "loss: 0.793638  [32064/60000]\n",
      "loss: 0.804780  [38464/60000]\n",
      "loss: 0.748229  [44864/60000]\n",
      "loss: 0.818589  [51264/60000]\n",
      "loss: 0.829658  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.763994 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.738930  [   64/60000]\n",
      "loss: 0.830325  [ 6464/60000]\n",
      "loss: 0.647673  [12864/60000]\n",
      "loss: 0.892461  [19264/60000]\n",
      "loss: 0.731671  [25664/60000]\n",
      "loss: 0.759907  [32064/60000]\n",
      "loss: 0.773193  [38464/60000]\n",
      "loss: 0.720838  [44864/60000]\n",
      "loss: 0.790426  [51264/60000]\n",
      "loss: 0.804794  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.734560 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.693695  [   64/60000]\n",
      "loss: 0.799209  [ 6464/60000]\n",
      "loss: 0.618156  [12864/60000]\n",
      "loss: 0.866571  [19264/60000]\n",
      "loss: 0.706750  [25664/60000]\n",
      "loss: 0.734611  [32064/60000]\n",
      "loss: 0.745916  [38464/60000]\n",
      "loss: 0.699299  [44864/60000]\n",
      "loss: 0.768408  [51264/60000]\n",
      "loss: 0.783666  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.710873 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.656813  [   64/60000]\n",
      "loss: 0.773039  [ 6464/60000]\n",
      "loss: 0.594875  [12864/60000]\n",
      "loss: 0.844735  [19264/60000]\n",
      "loss: 0.685432  [25664/60000]\n",
      "loss: 0.714865  [32064/60000]\n",
      "loss: 0.721246  [38464/60000]\n",
      "loss: 0.681440  [44864/60000]\n",
      "loss: 0.750570  [51264/60000]\n",
      "loss: 0.764820  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.691105 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.625931  [   64/60000]\n",
      "loss: 0.750173  [ 6464/60000]\n",
      "loss: 0.575860  [12864/60000]\n",
      "loss: 0.825765  [19264/60000]\n",
      "loss: 0.666804  [25664/60000]\n",
      "loss: 0.698971  [32064/60000]\n",
      "loss: 0.698450  [38464/60000]\n",
      "loss: 0.666148  [44864/60000]\n",
      "loss: 0.735792  [51264/60000]\n",
      "loss: 0.747474  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.674184 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.599523  [   64/60000]\n",
      "loss: 0.729627  [ 6464/60000]\n",
      "loss: 0.559937  [12864/60000]\n",
      "loss: 0.808944  [19264/60000]\n",
      "loss: 0.650315  [25664/60000]\n",
      "loss: 0.685855  [32064/60000]\n",
      "loss: 0.677215  [38464/60000]\n",
      "loss: 0.652833  [44864/60000]\n",
      "loss: 0.723381  [51264/60000]\n",
      "loss: 0.731196  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.659440 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.576539  [   64/60000]\n",
      "loss: 0.710801  [ 6464/60000]\n",
      "loss: 0.546362  [12864/60000]\n",
      "loss: 0.793823  [19264/60000]\n",
      "loss: 0.635604  [25664/60000]\n",
      "loss: 0.674786  [32064/60000]\n",
      "loss: 0.657396  [38464/60000]\n",
      "loss: 0.641161  [44864/60000]\n",
      "loss: 0.712873  [51264/60000]\n",
      "loss: 0.715746  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.646428 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.556236  [   64/60000]\n",
      "loss: 0.693327  [ 6464/60000]\n",
      "loss: 0.534643  [12864/60000]\n",
      "loss: 0.780107  [19264/60000]\n",
      "loss: 0.622411  [25664/60000]\n",
      "loss: 0.665243  [32064/60000]\n",
      "loss: 0.638914  [38464/60000]\n",
      "loss: 0.630927  [44864/60000]\n",
      "loss: 0.703931  [51264/60000]\n",
      "loss: 0.700996  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.634844 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.538082  [   64/60000]\n",
      "loss: 0.676985  [ 6464/60000]\n",
      "loss: 0.524433  [12864/60000]\n",
      "loss: 0.767599  [19264/60000]\n",
      "loss: 0.610534  [25664/60000]\n",
      "loss: 0.656844  [32064/60000]\n",
      "loss: 0.621708  [38464/60000]\n",
      "loss: 0.621979  [44864/60000]\n",
      "loss: 0.696293  [51264/60000]\n",
      "loss: 0.686880  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.624465 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.521692  [   64/60000]\n",
      "loss: 0.661649  [ 6464/60000]\n",
      "loss: 0.515479  [12864/60000]\n",
      "loss: 0.756161  [19264/60000]\n",
      "loss: 0.599806  [25664/60000]\n",
      "loss: 0.649306  [32064/60000]\n",
      "loss: 0.605715  [38464/60000]\n",
      "loss: 0.614186  [44864/60000]\n",
      "loss: 0.689741  [51264/60000]\n",
      "loss: 0.673371  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.615121 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.506790  [   64/60000]\n",
      "loss: 0.647247  [ 6464/60000]\n",
      "loss: 0.507583  [12864/60000]\n",
      "loss: 0.745692  [19264/60000]\n",
      "loss: 0.590087  [25664/60000]\n",
      "loss: 0.642419  [32064/60000]\n",
      "loss: 0.590869  [38464/60000]\n",
      "loss: 0.607425  [44864/60000]\n",
      "loss: 0.684091  [51264/60000]\n",
      "loss: 0.660456  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.606674 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.493172  [   64/60000]\n",
      "loss: 0.633736  [ 6464/60000]\n",
      "loss: 0.500584  [12864/60000]\n",
      "loss: 0.736107  [19264/60000]\n",
      "loss: 0.581259  [25664/60000]\n",
      "loss: 0.636030  [32064/60000]\n",
      "loss: 0.577097  [38464/60000]\n",
      "loss: 0.601574  [44864/60000]\n",
      "loss: 0.679179  [51264/60000]\n",
      "loss: 0.648134  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.599011 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.480685  [   64/60000]\n",
      "loss: 0.621085  [ 6464/60000]\n",
      "loss: 0.494344  [12864/60000]\n",
      "loss: 0.727335  [19264/60000]\n",
      "loss: 0.573215  [25664/60000]\n",
      "loss: 0.630027  [32064/60000]\n",
      "loss: 0.564324  [38464/60000]\n",
      "loss: 0.596512  [44864/60000]\n",
      "loss: 0.674869  [51264/60000]\n",
      "loss: 0.636407  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.592034 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.469210  [   64/60000]\n",
      "loss: 0.609268  [ 6464/60000]\n",
      "loss: 0.488746  [12864/60000]\n",
      "loss: 0.719305  [19264/60000]\n",
      "loss: 0.565867  [25664/60000]\n",
      "loss: 0.624332  [32064/60000]\n",
      "loss: 0.552471  [38464/60000]\n",
      "loss: 0.592124  [44864/60000]\n",
      "loss: 0.671038  [51264/60000]\n",
      "loss: 0.625273  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.585658 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.458645  [   64/60000]\n",
      "loss: 0.598253  [ 6464/60000]\n",
      "loss: 0.483688  [12864/60000]\n",
      "loss: 0.711950  [19264/60000]\n",
      "loss: 0.559134  [25664/60000]\n",
      "loss: 0.618891  [32064/60000]\n",
      "loss: 0.541461  [38464/60000]\n",
      "loss: 0.588302  [44864/60000]\n",
      "loss: 0.667586  [51264/60000]\n",
      "loss: 0.614732  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.579810 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.448907  [   64/60000]\n",
      "loss: 0.588006  [ 6464/60000]\n",
      "loss: 0.479082  [12864/60000]\n",
      "loss: 0.705204  [19264/60000]\n",
      "loss: 0.552945  [25664/60000]\n",
      "loss: 0.613667  [32064/60000]\n",
      "loss: 0.531220  [38464/60000]\n",
      "loss: 0.584948  [44864/60000]\n",
      "loss: 0.664430  [51264/60000]\n",
      "loss: 0.604777  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.574425 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.439919  [   64/60000]\n",
      "loss: 0.578491  [ 6464/60000]\n",
      "loss: 0.474851  [12864/60000]\n",
      "loss: 0.699006  [19264/60000]\n",
      "loss: 0.547240  [25664/60000]\n",
      "loss: 0.608637  [32064/60000]\n",
      "loss: 0.521677  [38464/60000]\n",
      "loss: 0.581976  [44864/60000]\n",
      "loss: 0.661501  [51264/60000]\n",
      "loss: 0.595399  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.569446 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.431615  [   64/60000]\n",
      "loss: 0.569665  [ 6464/60000]\n",
      "loss: 0.470933  [12864/60000]\n",
      "loss: 0.693296  [19264/60000]\n",
      "loss: 0.541964  [25664/60000]\n",
      "loss: 0.603787  [32064/60000]\n",
      "loss: 0.512768  [38464/60000]\n",
      "loss: 0.579309  [44864/60000]\n",
      "loss: 0.658744  [51264/60000]\n",
      "loss: 0.586586  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.564826 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.423933  [   64/60000]\n",
      "loss: 0.561487  [ 6464/60000]\n",
      "loss: 0.467274  [12864/60000]\n",
      "loss: 0.688019  [19264/60000]\n",
      "loss: 0.537069  [25664/60000]\n",
      "loss: 0.599110  [32064/60000]\n",
      "loss: 0.504431  [38464/60000]\n",
      "loss: 0.576885  [44864/60000]\n",
      "loss: 0.656119  [51264/60000]\n",
      "loss: 0.578323  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.560521 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Norm: 5 epochs\n",
    "#rr_footprint: 5 epochs\n",
    "#ss_footprint: 6 epochs\n",
    "#Norm: 24 epochs\n",
    "#ss_footprint: 26 epochs \n",
    "t, acc = 0, 0\n",
    "while acc < 80:\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    acc = test(test_dataloader, model, loss_fn) * 100\n",
    "    t += 1\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
