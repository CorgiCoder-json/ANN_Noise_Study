{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmer: Jacob Maurer\n",
    "Date: 9/18/2024\n",
    "Description: This notebook is going to explore the idea of the data footprint, which is just the (final trained model weights) - (starting out weights). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058070543999999995\n",
      "-0.055686748999999994\n",
      "0.0010366239325176113\n",
      "0.00011877050000000042\n"
     ]
    }
   ],
   "source": [
    "rr_weight_layer_one_pre = pd.read_csv(\"results/rr_s_weight_pre_1.csv\").to_numpy()\n",
    "rr_weight_layer_one_post = pd.read_csv(\"results/rr_S_weight_post_1.csv\").to_numpy()\n",
    "rr_footprint = rr_weight_layer_one_post - rr_weight_layer_one_pre\n",
    "print(np.max(rr_footprint))\n",
    "print(np.min(rr_footprint))\n",
    "print(np.mean(rr_footprint))\n",
    "print(np.median(rr_footprint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0417123994\n",
      "-0.049363683000000005\n",
      "-0.0002584007362623034\n",
      "-3.2806499999999544e-05\n"
     ]
    }
   ],
   "source": [
    "ss_weight_layer_one_pre = pd.read_csv(\"results/ss_s_weight_pre_1.csv\").to_numpy()\n",
    "ss_weight_layer_one_post = pd.read_csv(\"results/ss_s_weight_post_1.csv\").to_numpy()\n",
    "ss_footprint = ss_weight_layer_one_post - ss_weight_layer_one_pre\n",
    "print(np.max(ss_footprint))\n",
    "print(np.min(ss_footprint))\n",
    "print(np.mean(ss_footprint))\n",
    "print(np.median(ss_footprint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Layer: \n",
      "0.07146686\n",
      "-0.07438539\n",
      "-0.00021260627435817675\n",
      "0.0223022551476396\n",
      "-0.000205464065\n",
      "New Layer: \n",
      "0.08447945300000001\n",
      "-0.086401961\n",
      "0.0010824183944217385\n",
      "0.021391103875374413\n",
      "0.0010576660500000003\n"
     ]
    }
   ],
   "source": [
    "modified_ss_layer = ss_weight_layer_one_pre + rr_footprint\n",
    "modified_rr_layer = rr_weight_layer_one_pre + ss_footprint\n",
    "print(\"Old Layer: \")\n",
    "print(np.max(ss_weight_layer_one_post))\n",
    "print(np.min(ss_weight_layer_one_post))\n",
    "print(np.mean(ss_weight_layer_one_post))\n",
    "print(np.std(ss_weight_layer_one_post))\n",
    "print(np.median(ss_weight_layer_one_post))\n",
    "print(\"New Layer: \")\n",
    "print(np.max(modified_ss_layer))\n",
    "print(np.min(modified_ss_layer))\n",
    "print(np.mean(modified_ss_layer))\n",
    "print(np.std(modified_ss_layer))\n",
    "print(np.median(modified_ss_layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why this might be important?\n",
    "\n",
    "It means that we are able to significantly reduce training time by just adding the footprint to the layer. The only problem is, as of right now, it only works when the model has these similar parameters\n",
    "\n",
    "Hypothesis: Reduction/expansion with pooling and convolution not using neural networks. \n",
    "Evidence: Pooling/convolution scale the data while retaining most of the original pattern.\n",
    "\n",
    "Pooling2d used as a possible way to prune the network??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:07<00:00, 3320078.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 327164.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:05<00:00, 881581.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Import the dataset\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear_relu_stack.0.weight', tensor([[-0.0041, -0.0227,  0.0028,  ..., -0.0200,  0.0312, -0.0309],\n",
      "        [-0.0341, -0.0111,  0.0178,  ..., -0.0121, -0.0350, -0.0059],\n",
      "        [ 0.0145, -0.0277,  0.0242,  ..., -0.0223,  0.0331, -0.0082],\n",
      "        ...,\n",
      "        [ 0.0355, -0.0140, -0.0332,  ..., -0.0105,  0.0304, -0.0010],\n",
      "        [-0.0071,  0.0342, -0.0125,  ...,  0.0202,  0.0349, -0.0034],\n",
      "        [-0.0181,  0.0039,  0.0330,  ..., -0.0007,  0.0001, -0.0082]],\n",
      "       device='cuda:0')), ('linear_relu_stack.0.bias', tensor([ 2.1006e-02, -8.1743e-03,  5.9776e-03,  2.8017e-03, -2.1296e-02,\n",
      "         1.0029e-02,  2.0166e-02, -1.7554e-02,  2.6146e-02, -2.7567e-02,\n",
      "        -2.0030e-02,  3.6295e-03,  3.0830e-02,  1.9033e-02,  1.6304e-02,\n",
      "         2.2928e-02,  3.5663e-02, -8.9360e-03,  1.5613e-02, -1.1824e-02,\n",
      "        -1.6636e-02,  2.8873e-02, -3.5365e-02, -2.2869e-02, -1.3977e-02,\n",
      "         3.1993e-02,  2.5239e-02, -3.4526e-02,  1.4082e-02, -2.3280e-02,\n",
      "         9.2048e-03, -4.0441e-03, -7.5098e-03,  4.0310e-03,  4.6408e-03,\n",
      "        -3.1119e-02, -3.5537e-03, -1.7383e-02, -4.7935e-03, -7.6844e-03,\n",
      "        -2.7141e-02, -4.7405e-03,  1.3322e-02,  8.2163e-03, -4.9020e-03,\n",
      "        -1.4208e-02,  3.2360e-02, -2.5952e-03,  3.2339e-02,  2.1655e-02,\n",
      "         2.8593e-03,  4.1680e-03, -1.1587e-02, -8.8519e-03,  7.9172e-03,\n",
      "         6.3196e-03,  1.9061e-02, -2.7340e-02,  1.5863e-03, -2.3803e-03,\n",
      "         1.7478e-02, -2.6221e-02,  1.7925e-02,  1.6355e-02,  3.4513e-02,\n",
      "         2.1560e-02, -2.6245e-02,  8.2793e-03,  9.2331e-03, -1.7564e-02,\n",
      "         2.5081e-02,  1.7803e-02,  7.5297e-03, -2.1450e-02,  1.4550e-02,\n",
      "        -3.3199e-02,  2.1987e-02,  3.1996e-03,  1.3223e-02,  1.2218e-02,\n",
      "         1.9383e-02, -2.2354e-02, -2.8426e-02, -3.5438e-02, -1.7619e-02,\n",
      "         4.1926e-03, -1.3121e-02,  1.6008e-02, -1.8234e-02,  6.6977e-05,\n",
      "        -1.1031e-02,  1.3714e-02,  3.9111e-03,  3.2299e-02,  1.8210e-02,\n",
      "        -5.1756e-03,  1.4838e-02, -3.3226e-02,  2.5541e-02, -1.8458e-02,\n",
      "         2.4385e-03, -6.1808e-03, -3.4769e-02,  3.1642e-02,  2.3311e-03,\n",
      "         3.3708e-02, -2.9902e-02, -7.6437e-03,  8.8016e-04,  6.7039e-03,\n",
      "         1.1023e-02,  2.6478e-02, -2.6646e-02, -1.5066e-02,  2.5994e-02,\n",
      "        -1.0241e-02,  3.3911e-02, -5.0530e-03, -1.6829e-03, -2.9361e-03,\n",
      "        -8.8678e-03, -2.4678e-02, -1.1904e-02, -5.9865e-03, -1.3322e-02,\n",
      "        -2.3302e-02, -1.4383e-02, -3.4267e-02,  3.5701e-02, -1.8844e-02,\n",
      "        -3.3338e-02,  8.9538e-03,  1.4391e-02, -2.8344e-04,  6.1627e-03,\n",
      "        -2.2947e-02, -2.0912e-02,  1.7710e-02, -1.2485e-02, -1.0938e-02,\n",
      "        -1.8627e-02, -9.3656e-03,  2.0124e-02, -3.5697e-02, -1.6670e-02,\n",
      "        -3.2991e-03,  3.5137e-02,  3.3714e-03, -2.1673e-02,  1.6416e-02,\n",
      "        -2.6515e-02, -2.0675e-02, -3.4551e-02,  1.8010e-02, -1.4081e-02,\n",
      "        -2.3173e-02, -2.4374e-02, -3.4587e-02, -1.1232e-02, -3.1436e-02,\n",
      "        -2.3067e-02, -1.0572e-02,  7.9421e-03, -4.2573e-03, -4.9734e-04,\n",
      "        -7.4825e-04, -2.2114e-02,  3.2420e-02,  1.0314e-02,  3.5456e-02,\n",
      "        -2.5967e-02, -2.6990e-02,  1.7272e-02,  1.9949e-02,  2.1017e-02,\n",
      "         2.3495e-02, -1.6721e-02,  3.3476e-02,  3.0259e-02,  1.1407e-02,\n",
      "         1.3146e-02,  2.3112e-02,  3.4705e-02, -1.1591e-02, -1.3252e-02,\n",
      "        -2.6787e-02, -3.5355e-02,  7.4984e-03,  3.3950e-02,  2.0995e-02,\n",
      "        -1.3043e-02, -3.4872e-02,  1.7216e-02,  1.1598e-02,  8.2590e-03,\n",
      "        -6.7995e-04,  3.5327e-02, -2.6246e-02, -1.9664e-02,  3.3747e-02,\n",
      "         2.8851e-02,  2.8038e-02,  3.0394e-02,  1.8695e-02, -1.0715e-02,\n",
      "         2.9278e-02,  2.7815e-02,  3.5338e-02, -6.8215e-03, -3.4457e-02,\n",
      "        -7.8948e-03, -1.9746e-02,  5.3305e-03,  5.3426e-03,  1.2728e-02,\n",
      "         8.1708e-03,  3.1681e-02,  2.7597e-02, -3.4764e-02,  3.9393e-03,\n",
      "         2.1061e-02, -2.0777e-02, -3.4674e-02,  2.5292e-02, -1.9523e-02,\n",
      "         8.7678e-03, -1.1654e-02, -1.9228e-02, -1.1533e-02,  3.4908e-02,\n",
      "         8.2237e-03, -9.2825e-04,  2.2464e-02, -9.1503e-03, -1.9475e-02,\n",
      "         1.2869e-02,  1.0597e-02, -3.1681e-02,  2.9840e-03, -3.1745e-02,\n",
      "         6.7999e-03,  1.7224e-02,  1.5681e-02, -5.4880e-03, -1.7118e-02,\n",
      "         1.3591e-02, -2.9976e-02,  6.1091e-03,  2.2697e-02,  3.3073e-04,\n",
      "        -3.4318e-02, -2.3562e-02,  3.3379e-02,  2.2130e-02, -1.9805e-02,\n",
      "         1.3382e-02, -3.3466e-02, -2.3083e-02, -2.0678e-02, -2.3832e-02,\n",
      "        -1.2642e-02, -2.8814e-02,  1.2823e-02, -2.0041e-03, -1.7760e-02,\n",
      "        -5.9624e-03,  1.4106e-02, -2.0019e-02, -1.0522e-02,  9.6506e-03,\n",
      "         3.2896e-02,  1.6401e-02, -3.9521e-03,  1.9422e-02,  1.6012e-02,\n",
      "        -2.0877e-02, -2.3724e-02,  1.8315e-02,  9.0342e-03,  1.4116e-02,\n",
      "        -2.8017e-02,  6.0152e-03,  1.4555e-02, -7.1638e-03,  6.4784e-03,\n",
      "        -2.9961e-03, -1.9424e-02, -3.4193e-02, -1.8408e-02, -3.1101e-02,\n",
      "         1.7851e-02,  3.5119e-02, -3.3096e-02, -3.4419e-02, -2.2273e-03,\n",
      "         1.5900e-02,  7.6470e-03, -2.2007e-02,  2.8871e-02,  2.0662e-02,\n",
      "        -2.8092e-02,  2.0053e-03,  4.8734e-03,  1.9602e-02, -1.2222e-02,\n",
      "        -2.9158e-02,  2.3161e-02, -1.1438e-02,  1.3186e-02,  2.1629e-02,\n",
      "        -2.2879e-02,  3.0407e-02, -3.0190e-02, -1.8585e-02,  2.8406e-02,\n",
      "        -3.4733e-02, -2.8883e-02,  7.6169e-03, -2.5089e-02, -6.8991e-03,\n",
      "        -2.2602e-02, -1.8272e-02, -1.1314e-02, -7.7179e-03,  3.6756e-03,\n",
      "        -1.0041e-02, -2.9726e-02,  5.8222e-03, -1.8674e-02, -3.5366e-02,\n",
      "         1.1424e-02, -3.1095e-02, -2.9652e-02, -1.4090e-02, -2.1838e-02,\n",
      "        -1.9952e-02, -2.6591e-02, -1.3481e-02,  1.7185e-02,  2.8428e-02,\n",
      "         2.6818e-02,  5.9939e-03,  3.4761e-02, -1.3874e-02,  3.0269e-02,\n",
      "         2.6296e-02,  1.9652e-02,  3.4434e-02, -7.5325e-03,  2.3866e-02,\n",
      "        -2.0556e-02, -7.5498e-03,  3.3836e-02, -3.0994e-02,  1.3781e-02,\n",
      "         2.8842e-02, -8.9105e-03, -4.6494e-03, -2.4095e-02, -7.1001e-03,\n",
      "         3.2144e-02, -2.0351e-02,  2.7567e-02, -5.2841e-03, -3.1522e-02,\n",
      "        -1.5277e-02,  2.9366e-02, -5.0584e-03, -4.7421e-04,  6.7086e-04,\n",
      "         3.4365e-02, -5.0473e-03, -1.7194e-02,  1.9959e-02,  3.1744e-02,\n",
      "         2.1613e-03, -9.4302e-03,  2.6584e-02,  2.8766e-02,  9.2048e-03,\n",
      "         5.9424e-03, -2.9007e-02, -5.9964e-03,  2.6321e-02,  2.3781e-02,\n",
      "        -2.6036e-02,  2.9680e-02, -1.7151e-02,  1.6638e-02, -8.4863e-03,\n",
      "        -3.3832e-03, -6.8909e-03, -2.0847e-02, -9.3080e-03,  2.8658e-02,\n",
      "         1.1923e-02, -1.0425e-02, -1.1008e-02, -2.7305e-02,  2.3844e-03,\n",
      "        -3.3277e-02, -1.8572e-02,  6.1447e-03,  2.0534e-02, -2.5958e-02,\n",
      "         3.5189e-02,  9.9735e-03, -2.3726e-03, -1.0484e-03, -6.9150e-03,\n",
      "         1.2652e-02, -4.4363e-03, -2.2818e-03, -8.7497e-03,  1.2523e-02,\n",
      "         2.6047e-02, -7.9000e-03,  2.4593e-02,  2.4989e-02,  1.1398e-02,\n",
      "        -3.5686e-02, -2.8490e-02,  2.3109e-02, -3.3874e-02,  5.1013e-03,\n",
      "        -2.9169e-02,  7.2921e-03, -2.7038e-02,  2.4096e-02, -3.1501e-02,\n",
      "         3.4141e-02,  2.0979e-02, -9.9289e-03, -3.0583e-02, -1.0941e-02,\n",
      "         3.3970e-02, -2.3103e-02, -3.0791e-02, -1.7097e-02,  3.1138e-03,\n",
      "        -3.5539e-03, -1.1388e-02,  1.0581e-02, -2.4376e-02, -1.9316e-02,\n",
      "        -2.8348e-02, -6.0048e-03,  1.0851e-02,  1.5398e-02,  2.4264e-02,\n",
      "        -1.8105e-02,  1.1093e-02,  3.3158e-02, -2.0608e-02, -2.9175e-04,\n",
      "         1.7394e-02,  3.3127e-02,  4.3050e-03,  7.0095e-03, -7.5443e-03,\n",
      "        -2.7805e-02, -1.0870e-02,  1.5828e-02,  3.0246e-02,  1.2928e-02,\n",
      "         1.1276e-02,  7.6830e-03, -1.0039e-02,  1.8200e-02, -1.8674e-02,\n",
      "        -2.7760e-02, -3.1894e-02, -3.2006e-02,  2.8870e-02, -5.0989e-03,\n",
      "        -1.5489e-02, -3.1709e-02, -4.2331e-03,  2.5042e-02, -1.5607e-02,\n",
      "         3.2338e-02,  1.9454e-03,  1.2135e-02, -6.1854e-03,  1.4859e-02,\n",
      "         2.6281e-02, -2.9682e-02, -2.5510e-02,  3.4075e-02,  2.3423e-02,\n",
      "         3.0314e-02, -1.4166e-02,  1.6534e-03,  8.5953e-03, -3.2958e-02,\n",
      "        -8.4947e-03, -2.4324e-02,  3.0918e-02, -1.0727e-02,  1.1643e-02,\n",
      "         7.3802e-03, -2.6351e-02,  3.1775e-02, -2.3472e-02, -2.2913e-02,\n",
      "        -8.6820e-03, -1.5937e-02, -4.6996e-03, -2.0136e-02,  2.5501e-03,\n",
      "        -3.2460e-02,  1.6571e-02], device='cuda:0')), ('linear_relu_stack.2.weight', tensor([[-0.0372, -0.0409, -0.0401,  ...,  0.0327,  0.0085, -0.0353],\n",
      "        [-0.0108,  0.0329,  0.0005,  ...,  0.0151, -0.0189,  0.0366],\n",
      "        [ 0.0294,  0.0429, -0.0294,  ..., -0.0265,  0.0184,  0.0130],\n",
      "        ...,\n",
      "        [ 0.0011, -0.0203,  0.0075,  ..., -0.0216,  0.0405,  0.0315],\n",
      "        [ 0.0086, -0.0354, -0.0005,  ..., -0.0136,  0.0379, -0.0225],\n",
      "        [-0.0083, -0.0061, -0.0295,  ...,  0.0144, -0.0120, -0.0220]],\n",
      "       device='cuda:0')), ('linear_relu_stack.2.bias', tensor([ 0.0159,  0.0269,  0.0370, -0.0340, -0.0290,  0.0340, -0.0107,  0.0190,\n",
      "        -0.0196, -0.0414], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests below are run using the smaller model traing first. Will move to larger model footprint after these same model tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.319736  [   64/60000]\n",
      "loss: 2.227729  [ 6464/60000]\n",
      "loss: 2.124299  [12864/60000]\n",
      "loss: 2.050216  [19264/60000]\n",
      "loss: 1.846248  [25664/60000]\n",
      "loss: 1.802203  [32064/60000]\n",
      "loss: 1.670375  [38464/60000]\n",
      "loss: 1.549551  [44864/60000]\n",
      "loss: 1.497507  [51264/60000]\n",
      "loss: 1.406090  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 1.369527 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.421298  [   64/60000]\n",
      "loss: 1.378088  [ 6464/60000]\n",
      "loss: 1.191825  [12864/60000]\n",
      "loss: 1.300091  [19264/60000]\n",
      "loss: 1.103312  [25664/60000]\n",
      "loss: 1.155924  [32064/60000]\n",
      "loss: 1.123164  [38464/60000]\n",
      "loss: 1.035343  [44864/60000]\n",
      "loss: 1.082098  [51264/60000]\n",
      "loss: 1.024225  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.992452 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.035349  [   64/60000]\n",
      "loss: 1.033785  [ 6464/60000]\n",
      "loss: 0.836653  [12864/60000]\n",
      "loss: 1.037098  [19264/60000]\n",
      "loss: 0.887570  [25664/60000]\n",
      "loss: 0.923294  [32064/60000]\n",
      "loss: 0.935459  [38464/60000]\n",
      "loss: 0.846806  [44864/60000]\n",
      "loss: 0.907786  [51264/60000]\n",
      "loss: 0.873388  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.837372 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.843431  [   64/60000]\n",
      "loss: 0.882595  [ 6464/60000]\n",
      "loss: 0.668571  [12864/60000]\n",
      "loss: 0.913870  [19264/60000]\n",
      "loss: 0.797524  [25664/60000]\n",
      "loss: 0.803644  [32064/60000]\n",
      "loss: 0.842719  [38464/60000]\n",
      "loss: 0.763717  [44864/60000]\n",
      "loss: 0.819225  [51264/60000]\n",
      "loss: 0.793574  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.756988 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.731381  [   64/60000]\n",
      "loss: 0.800858  [ 6464/60000]\n",
      "loss: 0.576490  [12864/60000]\n",
      "loss: 0.844763  [19264/60000]\n",
      "loss: 0.746490  [25664/60000]\n",
      "loss: 0.735293  [32064/60000]\n",
      "loss: 0.786979  [38464/60000]\n",
      "loss: 0.723380  [44864/60000]\n",
      "loss: 0.769454  [51264/60000]\n",
      "loss: 0.742190  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.708385 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.660319  [   64/60000]\n",
      "loss: 0.748612  [ 6464/60000]\n",
      "loss: 0.520459  [12864/60000]\n",
      "loss: 0.800237  [19264/60000]\n",
      "loss: 0.710535  [25664/60000]\n",
      "loss: 0.692460  [32064/60000]\n",
      "loss: 0.747594  [38464/60000]\n",
      "loss: 0.700772  [44864/60000]\n",
      "loss: 0.738309  [51264/60000]\n",
      "loss: 0.704001  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.674610 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.610951  [   64/60000]\n",
      "loss: 0.710436  [ 6464/60000]\n",
      "loss: 0.482753  [12864/60000]\n",
      "loss: 0.767895  [19264/60000]\n",
      "loss: 0.681750  [25664/60000]\n",
      "loss: 0.662961  [32064/60000]\n",
      "loss: 0.716385  [38464/60000]\n",
      "loss: 0.686100  [44864/60000]\n",
      "loss: 0.717070  [51264/60000]\n",
      "loss: 0.673058  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.648642 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.573872  [   64/60000]\n",
      "loss: 0.680132  [ 6464/60000]\n",
      "loss: 0.455240  [12864/60000]\n",
      "loss: 0.742285  [19264/60000]\n",
      "loss: 0.657258  [25664/60000]\n",
      "loss: 0.641096  [32064/60000]\n",
      "loss: 0.690013  [38464/60000]\n",
      "loss: 0.675561  [44864/60000]\n",
      "loss: 0.701765  [51264/60000]\n",
      "loss: 0.646843  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.627435 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.544363  [   64/60000]\n",
      "loss: 0.654987  [ 6464/60000]\n",
      "loss: 0.433950  [12864/60000]\n",
      "loss: 0.720892  [19264/60000]\n",
      "loss: 0.635859  [25664/60000]\n",
      "loss: 0.624058  [32064/60000]\n",
      "loss: 0.667022  [38464/60000]\n",
      "loss: 0.667654  [44864/60000]\n",
      "loss: 0.690391  [51264/60000]\n",
      "loss: 0.624147  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.609568 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.519921  [   64/60000]\n",
      "loss: 0.633654  [ 6464/60000]\n",
      "loss: 0.416781  [12864/60000]\n",
      "loss: 0.702475  [19264/60000]\n",
      "loss: 0.616933  [25664/60000]\n",
      "loss: 0.610302  [32064/60000]\n",
      "loss: 0.646711  [38464/60000]\n",
      "loss: 0.661743  [44864/60000]\n",
      "loss: 0.681789  [51264/60000]\n",
      "loss: 0.604261  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.594298 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.499135  [   64/60000]\n",
      "loss: 0.615348  [ 6464/60000]\n",
      "loss: 0.402519  [12864/60000]\n",
      "loss: 0.686363  [19264/60000]\n",
      "loss: 0.600063  [25664/60000]\n",
      "loss: 0.598863  [32064/60000]\n",
      "loss: 0.628690  [38464/60000]\n",
      "loss: 0.657484  [44864/60000]\n",
      "loss: 0.675185  [51264/60000]\n",
      "loss: 0.586664  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.581168 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.481151  [   64/60000]\n",
      "loss: 0.599540  [ 6464/60000]\n",
      "loss: 0.390411  [12864/60000]\n",
      "loss: 0.672134  [19264/60000]\n",
      "loss: 0.584917  [25664/60000]\n",
      "loss: 0.589067  [32064/60000]\n",
      "loss: 0.612688  [38464/60000]\n",
      "loss: 0.654595  [44864/60000]\n",
      "loss: 0.670004  [51264/60000]\n",
      "loss: 0.570930  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.569840 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.465403  [   64/60000]\n",
      "loss: 0.585827  [ 6464/60000]\n",
      "loss: 0.379955  [12864/60000]\n",
      "loss: 0.659484  [19264/60000]\n",
      "loss: 0.571216  [25664/60000]\n",
      "loss: 0.580422  [32064/60000]\n",
      "loss: 0.598478  [38464/60000]\n",
      "loss: 0.652791  [44864/60000]\n",
      "loss: 0.665801  [51264/60000]\n",
      "loss: 0.556711  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.560028 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.451486  [   64/60000]\n",
      "loss: 0.573885  [ 6464/60000]\n",
      "loss: 0.370803  [12864/60000]\n",
      "loss: 0.648164  [19264/60000]\n",
      "loss: 0.558733  [25664/60000]\n",
      "loss: 0.572570  [32064/60000]\n",
      "loss: 0.585850  [38464/60000]\n",
      "loss: 0.651781  [44864/60000]\n",
      "loss: 0.662237  [51264/60000]\n",
      "loss: 0.543738  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.551481 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#ReLU single: Trial 1: 3 epochs\n",
    "#Modified: Trial 1: 2 epochs \n",
    "#ss footprint: Trial 1: 3 epochs\n",
    "#Sigmoid single: Trial 1: 14 epochs\n",
    "#Modified: Trial 1: 13 epochs \n",
    "#rr footprint: Trial 1: 10 epochs\n",
    "t, acc = 0, 0\n",
    "while acc < 80:\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    acc = test(test_dataloader, model, loss_fn) * 100\n",
    "    t += 1\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state = model.state_dict()\n",
    "new_state[\"linear_relu_stack.0.weight\"] = new_state[\"linear_relu_stack.0.weight\"] + torch.from_numpy(rr_footprint).to(device)\n",
    "model.load_state_dict(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.325613  [   64/60000]\n",
      "loss: 2.112521  [ 6464/60000]\n",
      "loss: 1.908429  [12864/60000]\n",
      "loss: 1.786030  [19264/60000]\n",
      "loss: 1.508749  [25664/60000]\n",
      "loss: 1.486175  [32064/60000]\n",
      "loss: 1.381909  [38464/60000]\n",
      "loss: 1.276413  [44864/60000]\n",
      "loss: 1.256505  [51264/60000]\n",
      "loss: 1.154146  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.122879 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.163631  [   64/60000]\n",
      "loss: 1.136131  [ 6464/60000]\n",
      "loss: 0.948411  [12864/60000]\n",
      "loss: 1.085122  [19264/60000]\n",
      "loss: 0.913428  [25664/60000]\n",
      "loss: 0.957501  [32064/60000]\n",
      "loss: 0.951497  [38464/60000]\n",
      "loss: 0.885593  [44864/60000]\n",
      "loss: 0.929091  [51264/60000]\n",
      "loss: 0.867335  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.840928 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.844175  [   64/60000]\n",
      "loss: 0.878672  [ 6464/60000]\n",
      "loss: 0.674682  [12864/60000]\n",
      "loss: 0.892983  [19264/60000]\n",
      "loss: 0.773587  [25664/60000]\n",
      "loss: 0.787558  [32064/60000]\n",
      "loss: 0.812492  [38464/60000]\n",
      "loss: 0.760913  [44864/60000]\n",
      "loss: 0.808567  [51264/60000]\n",
      "loss: 0.756337  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.732938 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.703577  [   64/60000]\n",
      "loss: 0.773747  [ 6464/60000]\n",
      "loss: 0.556324  [12864/60000]\n",
      "loss: 0.807591  [19264/60000]\n",
      "loss: 0.712436  [25664/60000]\n",
      "loss: 0.705425  [32064/60000]\n",
      "loss: 0.744278  [38464/60000]\n",
      "loss: 0.709328  [44864/60000]\n",
      "loss: 0.750093  [51264/60000]\n",
      "loss: 0.694997  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.676310 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.623298  [   64/60000]\n",
      "loss: 0.715985  [ 6464/60000]\n",
      "loss: 0.492827  [12864/60000]\n",
      "loss: 0.759013  [19264/60000]\n",
      "loss: 0.674039  [25664/60000]\n",
      "loss: 0.659050  [32064/60000]\n",
      "loss: 0.701627  [38464/60000]\n",
      "loss: 0.684239  [44864/60000]\n",
      "loss: 0.716949  [51264/60000]\n",
      "loss: 0.653795  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.640478 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.570714  [   64/60000]\n",
      "loss: 0.677421  [ 6464/60000]\n",
      "loss: 0.453670  [12864/60000]\n",
      "loss: 0.726460  [19264/60000]\n",
      "loss: 0.645110  [25664/60000]\n",
      "loss: 0.629750  [32064/60000]\n",
      "loss: 0.670357  [38464/60000]\n",
      "loss: 0.670134  [44864/60000]\n",
      "loss: 0.696032  [51264/60000]\n",
      "loss: 0.622854  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.614800 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.532946  [   64/60000]\n",
      "loss: 0.648533  [ 6464/60000]\n",
      "loss: 0.426936  [12864/60000]\n",
      "loss: 0.702066  [19264/60000]\n",
      "loss: 0.621377  [25664/60000]\n",
      "loss: 0.609372  [32064/60000]\n",
      "loss: 0.645272  [38464/60000]\n",
      "loss: 0.661328  [44864/60000]\n",
      "loss: 0.681818  [51264/60000]\n",
      "loss: 0.598124  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.594980 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.504041  [   64/60000]\n",
      "loss: 0.625507  [ 6464/60000]\n",
      "loss: 0.407308  [12864/60000]\n",
      "loss: 0.682481  [19264/60000]\n",
      "loss: 0.601130  [25664/60000]\n",
      "loss: 0.594101  [32064/60000]\n",
      "loss: 0.624212  [38464/60000]\n",
      "loss: 0.655588  [44864/60000]\n",
      "loss: 0.671644  [51264/60000]\n",
      "loss: 0.577617  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.579054 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.480949  [   64/60000]\n",
      "loss: 0.606554  [ 6464/60000]\n",
      "loss: 0.392129  [12864/60000]\n",
      "loss: 0.666122  [19264/60000]\n",
      "loss: 0.583518  [25664/60000]\n",
      "loss: 0.581995  [32064/60000]\n",
      "loss: 0.606140  [38464/60000]\n",
      "loss: 0.651901  [44864/60000]\n",
      "loss: 0.664045  [51264/60000]\n",
      "loss: 0.560177  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.565973 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.461956  [   64/60000]\n",
      "loss: 0.590688  [ 6464/60000]\n",
      "loss: 0.379932  [12864/60000]\n",
      "loss: 0.652140  [19264/60000]\n",
      "loss: 0.568024  [25664/60000]\n",
      "loss: 0.571961  [32064/60000]\n",
      "loss: 0.590458  [38464/60000]\n",
      "loss: 0.649678  [44864/60000]\n",
      "loss: 0.658124  [51264/60000]\n",
      "loss: 0.545036  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.555082 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# modified random training sequence\n",
    "# cutoff criteria: 80%\n",
    "t, acc = 0, 0\n",
    "while acc < 80:\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    acc = test(test_dataloader, model, loss_fn) * 100\n",
    "    t += 1\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
