{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmer: Jacob Maurer\n",
    "Date: 9/24/2024\n",
    "Description: Testing the idea that the opposite weighted network can give insight into a better weight network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_regression, make_multilabel_classification\n",
    "import copy\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_classification_problem = make_classification(n_samples = 25000, n_features=500, n_informative = 250)\n",
    "large_regression_problem = make_regression(n_samples = 25000, n_features=500, n_informative = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x = copy.deepcopy(x_data)\n",
    "        self.y = copy.deepcopy(y_data)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], np.float32(self.y[index])\n",
    "\n",
    "large_classification_train_data = GeneratedDataset(large_classification_problem[0][int(len(large_classification_problem[0])*.2):], large_classification_problem[1][int(len(large_classification_problem[1])*.2):])\n",
    "large_classification_test_data = GeneratedDataset(large_classification_problem[0][:int(len(large_classification_problem[0])*.2)], large_classification_problem[1][:int(len(large_classification_problem[1])*.2)])\n",
    "large_train_loader_class = DataLoader(large_classification_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_class = DataLoader(large_classification_test_data, batch_size=100, shuffle=True)\n",
    "large_regression_train_data = GeneratedDataset(large_regression_problem[0][int(len(large_regression_problem[0])*.2):], large_regression_problem[1][int(len(large_regression_problem[1])*.2):])\n",
    "large_regression_test_data = GeneratedDataset(large_regression_problem[0][:int(len(large_regression_problem[0])*.2)], large_regression_problem[1][:int(len(large_regression_problem[1])*.2)])\n",
    "large_train_loader_regress = DataLoader(large_regression_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_regress = DataLoader(large_regression_test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "class LargeRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "large_model_class = LargeClassifyNetwork().to(device)\n",
    "large_model_regress = LargeRegressNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opposite_dict = copy.deepcopy(large_model_class.cpu().state_dict())\n",
    "for key in opposite_dict:\n",
    "    opposite_dict[key] *= -1 + .2\n",
    "large_model_class.to(device)\n",
    "large_model_class2 = LargeClassifyNetwork().to(device)\n",
    "large_model_class2.load_state_dict(opposite_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opposite_dict = copy.deepcopy(large_model_regress.cpu().state_dict())\n",
    "for key in opposite_dict:\n",
    "    opposite_dict[key] *= -1\n",
    "large_model_regress.to(device)\n",
    "large_model_regress2 = LargeRegressNetwork().to(device)\n",
    "large_model_regress2.load_state_dict(opposite_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_class = nn.BCELoss()\n",
    "optimizer_class = torch.optim.SGD(large_model_class.parameters(), lr=1e-2)\n",
    "loss_fn_class2 = nn.BCELoss()\n",
    "optimizer_class2 = torch.optim.SGD(large_model_class2.parameters(), lr=1e-2)\n",
    "loss_fn_regress = nn.MSELoss()\n",
    "optimizer_regress = torch.optim.SGD(large_model_regress.parameters(), lr=1e-2)\n",
    "loss_fn_regress2 = nn.MSELoss()\n",
    "optimizer_regress2 = torch.optim.SGD(large_model_regress2.parameters(), lr=1e-2)\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.unsqueeze(1)).item()\n",
    "            # pred = (pred > 0.5).type(torch.float)\n",
    "            # correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 0.736283 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 0.714588 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.587229 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.582829 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.491286 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.473744 \n",
      "\n",
      "{'net1': [-6.7121124e-05, 0.0009538211, 0.0036603732, 0.027246706, -6.880879e-05, 0.0009498017, -4.316261e-05, 0.022705873], 'net2': [5.369689e-05, -0.000763057, -0.0029282987, -0.021797365, 5.458728e-05, -0.0007665455, 0.00066451915, -0.017559199]}\n",
      "{'net1': [0.02578504, 0.026284326, 0.037934456, 0.0, 0.025800886, 0.026292682, 0.060963903, 0.0], 'net2': [0.020628033, 0.02102746, 0.030347565, 0.0, 0.020657178, 0.021030901, 0.057816267, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "means = {\"net1\": [], \"net2\": []}\n",
    "stds = {\"net1\": [], \"net2\": []}\n",
    "large_class = large_model_class.cpu().state_dict()\n",
    "opposite_class = large_model_class2.cpu().state_dict()\n",
    "for key in large_class:\n",
    "    means[\"net1\"].append(np.mean(large_class[key].numpy()))\n",
    "    means[\"net2\"].append(np.mean(opposite_class[key].numpy()))\n",
    "    stds[\"net1\"].append(np.std(large_class[key].numpy()))\n",
    "    stds[\"net2\"].append(np.std(opposite_class[key].numpy()))\n",
    "large_model_class.to(device)\n",
    "large_model_class2.to(device)\n",
    "train(large_train_loader_class, large_model_class, loss_fn_class, optimizer_class)\n",
    "train(large_train_loader_class, large_model_class2, loss_fn_class2, optimizer_class2)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "\n",
    "train(large_train_loader_class, large_model_class, loss_fn_class, optimizer_class)\n",
    "train(large_train_loader_class, large_model_class2, loss_fn_class2, optimizer_class2)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "large_class = large_model_class.cpu().state_dict()\n",
    "opposite_class = large_model_class2.cpu().state_dict()\n",
    "for key in large_class:\n",
    "    means[\"net1\"].append(np.mean(large_class[key].numpy()))\n",
    "    means[\"net2\"].append(np.mean(opposite_class[key].numpy()))\n",
    "    stds[\"net1\"].append(np.std(large_class[key].numpy()))\n",
    "    stds[\"net2\"].append(np.std(opposite_class[key].numpy()))\n",
    "large_model_class.to(device)\n",
    "large_model_class2.to(device)\n",
    "print(means)\n",
    "print(stds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 850028.702500 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 850034.068750 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 36654.905156 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31475.019961 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 36135.055469 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 33468.321484 \n",
      "\n",
      "{'net1': [3.850696e-05, 0.001461811, -0.004751329, -0.024107516, -0.54672396, -3.6230526, -3.9212875, 116.00389], 'net2': [-3.850696e-05, -0.001461811, 0.004751329, 0.024107516, 0.14124398, -0.0054369867, 0.7171431, -20.542202]}\n",
      "{'net1': [0.02578406, 0.025510116, 0.055035055, 0.0, 7.6437316, 12.305326, 48.278015, 0.0], 'net2': [0.02578406, 0.025510116, 0.055035055, 0.0, 7.3358784, 7.71518, 46.25749, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "means = {\"net1\": [], \"net2\": []}\n",
    "stds = {\"net1\": [], \"net2\": []}\n",
    "large_regress = large_model_regress.cpu().state_dict()\n",
    "opposite_regress = large_model_regress2.cpu().state_dict()\n",
    "for key in large_regress:\n",
    "    means[\"net1\"].append(np.mean(large_regress[key].numpy()))\n",
    "    means[\"net2\"].append(np.mean(opposite_regress[key].numpy()))\n",
    "    stds[\"net1\"].append(np.std(large_regress[key].numpy()))\n",
    "    stds[\"net2\"].append(np.std(opposite_regress[key].numpy()))\n",
    "large_model_regress.to(device)\n",
    "large_model_regress2.to(device)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "train(large_train_loader_regress, large_model_regress2, loss_fn_regress2, optimizer_regress2)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "train(large_train_loader_regress, large_model_regress2, loss_fn_regress2, optimizer_regress2)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "large_regress = large_model_regress.cpu().state_dict()\n",
    "opposite_regress = large_model_regress2.cpu().state_dict()\n",
    "for key in large_regress:\n",
    "    means[\"net1\"].append(np.mean(large_regress[key].numpy()))\n",
    "    means[\"net2\"].append(np.mean(opposite_regress[key].numpy()))\n",
    "    stds[\"net1\"].append(np.std(large_regress[key].numpy()))\n",
    "    stds[\"net2\"].append(np.std(opposite_regress[key].numpy()))\n",
    "large_model_regress.to(device)\n",
    "large_model_regress2.to(device)\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'net1': [-2.0919164e-05, -0.0015514004, 0.00018152373, 0.04110895], 'net2': [2.1551123e-05, 0.0015482809, 0.00029282202, -0.040358137]}\n",
      "{'net1': [0.025817605, 0.025753267, 0.05653238, 0.0], 'net2': [0.025817648, 0.025751727, 0.05649176, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "means = {\"net1\": [], \"net2\": []}\n",
    "stds = {\"net1\": [], \"net2\": []}\n",
    "large_class = large_model_class.cpu().state_dict()\n",
    "opposite_class = large_model_class2.cpu().state_dict()\n",
    "for key in large_class:\n",
    "    means[\"net1\"].append(np.mean(large_class[key].numpy()))\n",
    "    means[\"net2\"].append(np.mean(opposite_class[key].numpy()))\n",
    "    stds[\"net1\"].append(np.std(large_class[key].numpy()))\n",
    "    stds[\"net2\"].append(np.std(opposite_class[key].numpy()))\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.703579 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.699592 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.597723 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.592114 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.489344 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.466844 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.714294 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.629636 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "train(large_train_loader_class, large_model_class, loss_fn_class, optimizer_class)\n",
    "train(large_train_loader_class, large_model_class2, loss_fn_class2, optimizer_class2)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "train(large_train_loader_class, large_model_class, loss_fn_class, optimizer_class)\n",
    "train(large_train_loader_class, large_model_class2, loss_fn_class2, optimizer_class2)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "new_state = copy.deepcopy(large_model_class2.cpu().state_dict())\n",
    "old_state = copy.deepcopy(large_model_class.cpu().state_dict())\n",
    "old_state2 = copy.deepcopy(new_state)\n",
    "for key in new_state:\n",
    "    new_state[key] = (old_state[key] * old_state2[key])\n",
    "large_model_class.load_state_dict(new_state)\n",
    "large_model_class.to(device)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "train(large_train_loader_class, large_model_class, loss_fn_class, optimizer_class)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 29253.105234 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 850023.845000 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 30040.762773 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31703.303672 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 28897.320234 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 30902.173125 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 146203.059688 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 36925.466445 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 37468.502539 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "train(large_train_loader_regress, large_model_regress2, loss_fn_regress2, optimizer_regress2)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "train(large_train_loader_regress, large_model_regress2, loss_fn_regress2, optimizer_regress2)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "new_state = copy.deepcopy(large_model_regress2.cpu().state_dict())\n",
    "old_state = copy.deepcopy(large_model_regress.cpu().state_dict())\n",
    "old_state2 = copy.deepcopy(new_state)\n",
    "for key in new_state:\n",
    "    new_state[key] = (old_state[key] + old_state2[key]) / 2.0\n",
    "large_model_regress.load_state_dict(new_state)\n",
    "large_model_regress.to(device)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31693.382930 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31921.102305 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31845.476602 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 32323.142109 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31885.955430 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31332.490156 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 30379.478828 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 30295.531094 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 30584.742852 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 29253.105273 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for _ in range(epochs):\n",
    "    train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "    test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
