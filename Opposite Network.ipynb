{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmer: Jacob Maurer\n",
    "Date: 9/24/2024\n",
    "Description: Testing the idea that the opposite weighted network can give insight into a better weight network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_regression, make_multilabel_classification\n",
    "import copy\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_classification_problem = make_classification(n_samples = 25000, n_features=500, n_informative = 250)\n",
    "large_regression_problem = make_regression(n_samples = 25000, n_features=500, n_informative = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x = copy.deepcopy(x_data)\n",
    "        self.y = copy.deepcopy(y_data)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], np.float32(self.y[index])\n",
    "\n",
    "large_classification_train_data = GeneratedDataset(large_classification_problem[0][int(len(large_classification_problem[0])*.2):], large_classification_problem[1][int(len(large_classification_problem[1])*.2):])\n",
    "large_classification_test_data = GeneratedDataset(large_classification_problem[0][:int(len(large_classification_problem[0])*.2)], large_classification_problem[1][:int(len(large_classification_problem[1])*.2)])\n",
    "large_train_loader_class = DataLoader(large_classification_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_class = DataLoader(large_classification_test_data, batch_size=100, shuffle=True)\n",
    "large_regression_train_data = GeneratedDataset(large_regression_problem[0][int(len(large_regression_problem[0])*.2):], large_regression_problem[1][int(len(large_regression_problem[1])*.2):])\n",
    "large_regression_test_data = GeneratedDataset(large_regression_problem[0][:int(len(large_regression_problem[0])*.2)], large_regression_problem[1][:int(len(large_regression_problem[1])*.2)])\n",
    "large_train_loader_regress = DataLoader(large_regression_train_data, batch_size=100, shuffle=True)\n",
    "large_test_loader_regress = DataLoader(large_regression_test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "class LargeRegressNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(500, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.type(torch.float))\n",
    "        return logits\n",
    "large_model_class = LargeClassifyNetwork().to(device)\n",
    "large_model_regress = LargeRegressNetwork().to(device)\n",
    "large_model_regress2 = LargeRegressNetwork().to(device)\n",
    "large_model_regress3 = LargeRegressNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opposite_dict = copy.deepcopy(large_model_class.cpu().state_dict())\n",
    "for key in opposite_dict:\n",
    "    opposite_dict[key] *= -1 + .2\n",
    "large_model_class.to(device)\n",
    "large_model_class2 = LargeClassifyNetwork().to(device)\n",
    "large_model_class2.load_state_dict(opposite_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opposite_dict = copy.deepcopy(large_model_regress.cpu().state_dict())\n",
    "for key in opposite_dict:\n",
    "    opposite_dict[key] *= -1\n",
    "large_model_regress.to(device)\n",
    "large_model_regress2 = LargeRegressNetwork().to(device)\n",
    "large_model_regress2.load_state_dict(opposite_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn_class = nn.BCELoss()\n",
    "# optimizer_class = torch.optim.SGD(large_model_class.parameters(), lr=1e-2)\n",
    "# loss_fn_class2 = nn.BCELoss()\n",
    "# optimizer_class2 = torch.optim.SGD(large_model_class2.parameters(), lr=1e-2)\n",
    "loss_fn_regress = nn.MSELoss()\n",
    "optimizer_regress = torch.optim.SGD(large_model_regress.parameters(), lr=1e-2)\n",
    "loss_fn_regress2 = nn.MSELoss()\n",
    "optimizer_regress2 = torch.optim.SGD(large_model_regress2.parameters(), lr=1e-2)\n",
    "loss_fn_regress3 = nn.MSELoss()\n",
    "optimizer_regress3 = torch.optim.SGD(large_model_regress3.parameters(), lr=1e-2)\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.unsqueeze(1)).item()\n",
    "            # pred = (pred > 0.5).type(torch.float)\n",
    "            # correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['linear_relu_stack', '0', 'weight']\n",
      "['linear_relu_stack', '0', 'bias']\n",
      "['linear_relu_stack', '2', 'weight']\n",
      "['linear_relu_stack', '2', 'bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LargeClassifyNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=256, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = {\"net1\": [], \"net2\": []}\n",
    "stds = {\"net1\": [], \"net2\": []}\n",
    "large_class = large_model_class.cpu().state_dict()\n",
    "opposite_class = large_model_class2.cpu().state_dict()\n",
    "for key in large_class:\n",
    "    split_key = key.split('.')\n",
    "    print(split_key)\n",
    "large_model_class.to(device)\n",
    "large_model_class2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 0.736283 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 0.714588 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.587229 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.582829 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.491286 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.473744 \n",
      "\n",
      "{'net1': [-6.7121124e-05, 0.0009538211, 0.0036603732, 0.027246706, -6.880879e-05, 0.0009498017, -4.316261e-05, 0.022705873], 'net2': [5.369689e-05, -0.000763057, -0.0029282987, -0.021797365, 5.458728e-05, -0.0007665455, 0.00066451915, -0.017559199]}\n",
      "{'net1': [0.02578504, 0.026284326, 0.037934456, 0.0, 0.025800886, 0.026292682, 0.060963903, 0.0], 'net2': [0.020628033, 0.02102746, 0.030347565, 0.0, 0.020657178, 0.021030901, 0.057816267, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "means = {\"net1\": [], \"net2\": []}\n",
    "stds = {\"net1\": [], \"net2\": []}\n",
    "large_class = large_model_class.cpu().state_dict()\n",
    "opposite_class = large_model_class2.cpu().state_dict()\n",
    "for key in large_class[:-2]:\n",
    "    means[\"net1\"].append(np.mean(large_class[key].numpy()))\n",
    "    means[\"net2\"].append(np.mean(opposite_class[key].numpy()))\n",
    "    stds[\"net1\"].append(np.std(large_class[key].numpy()))\n",
    "    stds[\"net2\"].append(np.std(opposite_class[key].numpy()))\n",
    "large_model_class.to(device)\n",
    "large_model_class2.to(device)\n",
    "train(large_train_loader_class, large_model_class, loss_fn_class, optimizer_class)\n",
    "train(large_train_loader_class, large_model_class2, loss_fn_class2, optimizer_class2)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "\n",
    "train(large_train_loader_class, large_model_class, loss_fn_class, optimizer_class)\n",
    "train(large_train_loader_class, large_model_class2, loss_fn_class2, optimizer_class2)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "large_class = large_model_class.cpu().state_dict()\n",
    "opposite_class = large_model_class2.cpu().state_dict()\n",
    "for key in large_class:\n",
    "    means[\"net1\"].append(np.mean(large_class[key].numpy()))\n",
    "    means[\"net2\"].append(np.mean(opposite_class[key].numpy()))\n",
    "    stds[\"net1\"].append(np.std(large_class[key].numpy()))\n",
    "    stds[\"net2\"].append(np.std(opposite_class[key].numpy()))\n",
    "large_model_class.to(device)\n",
    "large_model_class2.to(device)\n",
    "print(means)\n",
    "print(stds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 869663.440000 \n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlarge_test_loader_regress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlarge_model_regress2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_regress2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m means \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet1\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet2\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n\u001b[0;32m      4\u001b[0m stds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet1\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet2\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n",
      "Cell \u001b[1;32mIn[12], line 38\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     37\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 38\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(pred, y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# pred = (pred > 0.5).type(torch.float)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# correct += (pred == y.unsqueeze(1)).type(torch.float).sum().item()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\Dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\Dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 24\u001b[0m, in \u001b[0;36mLargeRegressNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 24\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_relu_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\Dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\Dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\Dev\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\Dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\Dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\Dev\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "means = {\"net1\": [], \"net2\": []}\n",
    "stds = {\"net1\": [], \"net2\": []}\n",
    "large_regress = large_model_regress.cpu().state_dict()\n",
    "opposite_regress = large_model_regress2.cpu().state_dict()\n",
    "for key in large_regress:\n",
    "    means[\"net1\"].append(np.mean(large_regress[key].numpy()))\n",
    "    means[\"net2\"].append(np.mean(opposite_regress[key].numpy()))\n",
    "    stds[\"net1\"].append(np.std(large_regress[key].numpy()))\n",
    "    stds[\"net2\"].append(np.std(opposite_regress[key].numpy()))\n",
    "large_model_regress.to(device)\n",
    "large_model_regress2.to(device)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "train(large_train_loader_regress, large_model_regress2, loss_fn_regress2, optimizer_regress2)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "train(large_train_loader_regress, large_model_regress2, loss_fn_regress2, optimizer_regress2)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "large_regress = large_model_regress.cpu().state_dict()\n",
    "opposite_regress = large_model_regress2.cpu().state_dict()\n",
    "for key in large_regress:\n",
    "    means[\"net1\"].append(np.mean(large_regress[key].numpy()))\n",
    "    means[\"net2\"].append(np.mean(opposite_regress[key].numpy()))\n",
    "    stds[\"net1\"].append(np.std(large_regress[key].numpy()))\n",
    "    stds[\"net2\"].append(np.std(opposite_regress[key].numpy()))\n",
    "large_model_regress.to(device)\n",
    "large_model_regress2.to(device)\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'net1': [-2.0919164e-05, -0.0015514004, 0.00018152373, 0.04110895], 'net2': [2.1551123e-05, 0.0015482809, 0.00029282202, -0.040358137]}\n",
      "{'net1': [0.025817605, 0.025753267, 0.05653238, 0.0], 'net2': [0.025817648, 0.025751727, 0.05649176, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "means = {\"net1\": [], \"net2\": []}\n",
    "stds = {\"net1\": [], \"net2\": []}\n",
    "large_class = large_model_class.cpu().state_dict()\n",
    "opposite_class = large_model_class2.cpu().state_dict()\n",
    "for key in large_class:\n",
    "    means[\"net1\"].append(np.mean(large_class[key].numpy()))\n",
    "    means[\"net2\"].append(np.mean(opposite_class[key].numpy()))\n",
    "    stds[\"net1\"].append(np.std(large_class[key].numpy()))\n",
    "    stds[\"net2\"].append(np.std(opposite_class[key].numpy()))\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.703579 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.699592 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.597723 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.592114 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.489344 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.466844 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.714294 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.629636 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "train(large_train_loader_class, large_model_class, loss_fn_class, optimizer_class)\n",
    "train(large_train_loader_class, large_model_class2, loss_fn_class2, optimizer_class2)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "train(large_train_loader_class, large_model_class, loss_fn_class, optimizer_class)\n",
    "train(large_train_loader_class, large_model_class2, loss_fn_class2, optimizer_class2)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "test(large_test_loader_class, large_model_class2, loss_fn_class2)\n",
    "new_state = copy.deepcopy(large_model_class2.cpu().state_dict())\n",
    "old_state = copy.deepcopy(large_model_class.cpu().state_dict())\n",
    "old_state2 = copy.deepcopy(new_state)\n",
    "for key in new_state:\n",
    "    new_state[key] = (old_state[key] * old_state2[key])\n",
    "large_model_class.load_state_dict(new_state)\n",
    "large_model_class.to(device)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n",
    "train(large_train_loader_class, large_model_class, loss_fn_class, optimizer_class)\n",
    "test(large_test_loader_class, large_model_class, loss_fn_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 869690.325000 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 869686.517500 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 47522.478906 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 52418.655703 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 49462.052266 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 34582.257617 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 572628.103750 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 32759.122969 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 33299.041992 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 32414.055234 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "train(large_train_loader_regress, large_model_regress2, loss_fn_regress2, optimizer_regress2)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "train(large_train_loader_regress, large_model_regress2, loss_fn_regress2, optimizer_regress2)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "new_state = copy.deepcopy(large_model_regress2.cpu().state_dict())\n",
    "old_state = copy.deepcopy(large_model_regress.cpu().state_dict())\n",
    "old_state2 = copy.deepcopy(new_state)\n",
    "for key in new_state:\n",
    "    new_state[key] = (.2*old_state[key] + .8*old_state2[key]) / 2.0\n",
    "large_model_regress.load_state_dict(new_state)\n",
    "large_model_regress.to(device)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 34129.928984 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 32527.510117 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 32106.747500 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 32490.091914 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31111.944766 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31304.640039 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31607.434688 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31273.644023 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 31166.953125 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 30080.981914 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for _ in range(epochs):\n",
    "    train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "    test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 890309.682500 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 890323.281250 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 890355.686250 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 49727.075625 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 34999.237891 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 48966.797969 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 46348.094414 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 34014.563867 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 48091.018828 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 259402.471875 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 38036.571289 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 37397.703672 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 35875.797930 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "test(large_test_loader_regress, large_model_regress3, loss_fn_regress3)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "train(large_train_loader_regress, large_model_regress2, loss_fn_regress2, optimizer_regress2)\n",
    "train(large_train_loader_regress, large_model_regress3, loss_fn_regress3, optimizer_regress3)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "test(large_test_loader_regress, large_model_regress3, loss_fn_regress3)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "train(large_train_loader_regress, large_model_regress2, loss_fn_regress2, optimizer_regress2)\n",
    "train(large_train_loader_regress, large_model_regress3, loss_fn_regress3, optimizer_regress3)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "test(large_test_loader_regress, large_model_regress2, loss_fn_regress2)\n",
    "test(large_test_loader_regress, large_model_regress3, loss_fn_regress3)\n",
    "new_state = copy.deepcopy(large_model_regress2.cpu().state_dict())\n",
    "old_state = copy.deepcopy(large_model_regress.cpu().state_dict())\n",
    "old_state2 = copy.deepcopy(new_state)\n",
    "old_state3 = copy.deepcopy(large_model_regress3.cpu().state_dict())\n",
    "for key in new_state:\n",
    "    new_state[key] = (old_state[key] + old_state2[key]) / 3.0\n",
    "large_model_regress.load_state_dict(new_state)\n",
    "large_model_regress.to(device)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)\n",
    "train(large_train_loader_regress, large_model_regress, loss_fn_regress, optimizer_regress)\n",
    "test(large_test_loader_regress, large_model_regress, loss_fn_regress)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
